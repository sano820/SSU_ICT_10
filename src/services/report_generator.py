# -*- coding: utf-8 -*-
"""report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yZRIyuIv9qEXQwDH-MH-H9opGAjir0nF
"""

!pip install -q openai langchain langchain-core langchain-openai langchain-community pytube tavily-python youtube-search langgraph arxiv pymupdf

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')

# %cd /content/drive/MyDrive/ICT_í”„ë¡œì íŠ¸

import time   # ì„ì‹œ íŒŒì¼ ì´ë¦„ ìƒì„±ì„ ìœ„í•´ ì¶”ê°€
import glob   # íŒŒì¼ ì‚­ì œë¥¼ ìœ„í•´ ì¶”ê°€
import requests
import json
from datetime import datetime, timedelta  # ë‚ ì§œ ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€

# OpenAI & LangChain
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain import hub
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate

# LangChain Community Tools & Loaders
from langchain_community.tools.youtube.search import YouTubeSearchTool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader
from langchain_community.document_loaders import ArxivLoader
from langchain_community.retrievers import TavilySearchAPIRetriever

# External libs
from pytube import YouTube  # pytube ì§ì ‘ ì‚¬ìš©

# LangGraph
from langgraph.graph import StateGraph, END

# Typing
from typing import TypedDict, List, Dict, Optional

# Tools
from tools import find_youtube_videos, analyze_video_content, search_naver_news, search_global_news, search_arxiv_papers, tavily_web_search

import os
from google.colab import userdata

os.environ['OPENAI_API_KEY'] = userdata.get('ssu')
os.environ['TAVILY_API_KEY'] = userdata.get('tavily')
os.environ['YOUTUBE_API_KEY'] = userdata.get('youtube')
os.environ['NAVER_CLIENT_ID'] = userdata.get('naver_client_id')
os.environ['NAVER_CLIENT_SECRET'] = userdata.get('naver_client_secret')
os.environ['NEWS_API_KEY'] = userdata.get('news_api_key')

class DomesticAnalysis(TypedDict):
    final_summary: str
    distilled_summary: str

class GlobalTrends(TypedDict):
    generated_keywords: str
    prediction: str
    distilled_prediction: str

class AcademicResearch(TypedDict):
    summary: str
    distilled_summary: str

class AgentState(TypedDict):
    # í•„ìˆ˜ ì…ë ¥ê°’
    target_job: str
    target_company: List[str]

    # ë…¸ë“œë¥¼ ê±°ì¹˜ë©° ì±„ì›Œì§€ëŠ” ê°’ë“¤
    final_report: Optional[str]
    domestic_analysis: Optional[DomesticAnalysis]
    global_trends: GlobalTrends
    academic_research: AcademicResearch

# --- ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
def domestic_job_analysis_node(state: AgentState) -> Dict[str, DomesticAnalysis]:
    """
    êµ­ë‚´ ì±„ìš© ì‹œì¥ì„ 3ê°€ì§€ ê´€ì (ì±„ìš© ê³µê³ , í•©ê²© í›„ê¸°, í˜„ì§ì ì¸í„°ë·°)ì—ì„œ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    print("--- êµ­ë‚´ ì±„ìš© ì‹œì¥ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")
    target_job = state["target_job"]
    target_companies = state["target_company"]
    company_query = " OR ".join(target_companies) # ì—¬ëŸ¬ ê¸°ì—…ì„ ORë¡œ ë¬¶ì–´ ê²€ìƒ‰

    # 1. ì±„ìš© ê³µê³  ë¶„ì„
    print("1. ì±„ìš© ê³µê³  ë¶„ì„ ì¤‘...")
    postings_results = tavily_web_search.invoke(
        f'"{company_query}" {target_job} ì‹ ì… ì±„ìš© ê³µê³  ìê²©ìš”ê±´ ìš°ëŒ€ì‚¬í•­'
    )
    postings_summary = analyzer_chain.invoke({
        "target_job": target_job,
        "search_results": postings_results,
        "request": f"""
                  '{company_query}'ì˜ '{target_job}' ì‹ ì… ì±„ìš© ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬, ì•„ë˜ êµ¬ì¡°ì— ë§ì¶° **'ì¸¡ì • ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸'** ì •ë³´ë§Œ ì¶”ì¶œí•´ì¤˜.

                  - **[ì§ë¬´ ëª©í‘œ(Role Goal)]**: ì´ ì§ë¬´ê°€ ë‹¬ì„±í•´ì•¼ í•  **ì •ëŸ‰ì /ì •ì„±ì  ëª©í‘œ**ëŠ” ë¬´ì—‡ì¸ê°€? (ì˜ˆ: MAU 10ë§Œ ë‹¬ì„±, ë¦¬í…ì…˜ 5% ê°œì„ , ì‹ ê·œ í”¼ì²˜ ì„±ê³µì  ëŸ°ì¹­)
                  - **[ì£¼ìš” ì±…ì„(Key Responsibilities)]**: ìœ„ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì•¡ì…˜ì€ ë¬´ì—‡ì¸ê°€?
                  - **[í•µì‹¬ ì—­ëŸ‰(Core Competencies)]**:
                      - **(Hard Skills)**: í•„ìˆ˜ì ì¸ **íˆ´ê³¼ ê¸°ìˆ ëª…**ì„ ëª¨ë‘ ë‚˜ì—´í•´ì¤˜. (ì˜ˆ: Python, AWS, Figma, SQL, GA)
                      - **(í˜‘ì—… ë°©ì‹)**: ì–´ë–¤ ë™ë£Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í˜‘ì—…í•˜ëŠ”ì§€ **êµ¬ì²´ì ì¸ í”„ë¡œì„¸ìŠ¤**ë¥¼ ëª…ì‹œí•´ì¤˜. (ì˜ˆ: "PO, ë””ìì´ë„ˆì™€ ìŠ¬ë™ìœ¼ë¡œ ì†Œí†µ", "Git-flow ê¸°ë°˜ ì½”ë“œ ë¦¬ë·°", "Zeplinìœ¼ë¡œ ë””ìì¸ ê°€ì´ë“œ ê³µìœ ")
                  - **[ìš°ëŒ€ ì‚¬í•­(Preferred)]**: ì–´ë–¤ **êµ¬ì²´ì ì¸ ê²½í—˜**ì„ ê°€ì§„ ì§€ì›ìë¥¼ ì„ í˜¸í•˜ëŠ”ê°€? (ì˜ˆ: "ì‹¤ì œ ì„œë¹„ìŠ¤ ì¶œì‹œ ê²½í—˜", "ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ ê²½í—˜", "ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ê²½í—˜")

                  **â€» 'ë„ì „ ì •ì‹ ', 'ì—´ì •' ë“± ì¶”ìƒì ì¸ í‚¤ì›Œë“œëŠ” ë¶„ì„ì—ì„œ ì œì™¸í•  ê²ƒ.**
                  """
    }).content

    # 2. í•©ê²© í›„ê¸° ë¶„ì„
    print("2. í•©ê²© í›„ê¸° ë¶„ì„ ì¤‘ (ìµœê·¼ 3ë…„ ì •ë³´)...")

    # ìµœê·¼ 3ë…„ ì—°ë„ ì¿¼ë¦¬ ìƒì„±
    current_year = datetime.now().year
    years_query = " OR ".join(str(y) for y in range(current_year, current_year - 3, -1))

    # Tavily ì›¹ ê²€ìƒ‰ (ìµœê·¼ 3ë…„)
    reviews_web_results = tavily_web_search.invoke(
        f'"{company_query}" {target_job} ì‹ ì… í•©ê²© í›„ê¸° ({years_query}) site:velog.io OR site:tistory.com OR site:brunch.co.kr'
    )

    # ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰ (ìµœê·¼ 3ë…„)
    youtube_summary = ""
    try:
        # ì—¬ê¸°ì— ì‚¬ìš©ìì˜ ìœ íŠœë¸Œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ì½”ë“œë¥¼ ë„£ìŠµë‹ˆë‹¤.

        video_urls_str = find_youtube_videos.invoke({
            "topic": f'"{company_query}" {target_job} ì‹ ì… í•©ê²© í›„ê¸°',
            "language": "korean",
            "time_filter": "3 years"
        })
        if video_urls_str and "Error" not in video_urls_str:
            first_video_url = video_urls_str.splitlines()[0].strip()
            print(f"í•©ê²© í›„ê¸° ì˜ìƒ ë¶„ì„ ì¤‘: {first_video_url}")

            youtube_summary = analyze_video_content({
                "video_url" : first_video_url,
                "question" : f"""
                            '{company_query}'ì˜ '{target_job}' ì§ë¬´ ì‹ ì… í•©ê²© í›„ê¸° ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬, **'ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤'** ê¸°ë°˜ì˜ í•©ê²© ì „ëµì„ ì•„ë˜ êµ¬ì¡°ë¡œ ì •ë¦¬í•´ì¤˜.

                            - **[í•©ê²©ì í”„ë¡œí•„(Profile)]**: í•©ê²©ìë“¤ì˜ ê³µí†µì ì¸ **êµ¬ì²´ì  ìŠ¤í™**ì€ ë¬´ì—‡ì¸ê°€? (ì˜ˆ: ì „ê³µ, í•™ì , ì¸í„´ ê²½í—˜ íšŸìˆ˜, í”„ë¡œì íŠ¸ ê°œìˆ˜)
                            - **[ì±„ìš© í”„ë¡œì„¸ìŠ¤ë³„ ì¤€ë¹„ ì‚¬í•­(Process Prep)]**: ê° ë‹¨ê³„ë³„ë¡œ **ë¬´ì—‡ì„, ì–´ë–»ê²Œ** ì¤€ë¹„í–ˆëŠ”ê°€?
                                - **(ì„œë¥˜/ê³¼ì œ)**: ì–´ë–¤ **í”„ë¡œì íŠ¸**ë¥¼ ê°•ì¡°í–ˆê³ , ê³¼ì œëŠ” **ì–´ë–¤ ê¸°ìˆ /ë°©ë²•ë¡ **ì„ ì‚¬ìš©í•´ í•´ê²°í–ˆëŠ”ê°€?
                                - **(ë©´ì ‘)**: ì–´ë–¤ **í”„ë¡œì íŠ¸ ê²½í—˜**ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ì•˜ê³ , ì–´ë–»ê²Œ ë‹µë³€í–ˆëŠ”ê°€?
                            - **[ê²°ì •ì  í•©ê²© ì¦ê±°(Actionable Evidence)]**: í•©ê²©ìë“¤ì´ ì œì‹œí•˜ëŠ” ìì‹ ì˜ **ê°€ì¥ ê°•ë ¥í•œ ê²½ìŸë ¥(ì •ëŸ‰ì  ì„±ê³¼ ë˜ëŠ” êµ¬ì²´ì  ê²½í—˜)**ì€ ë¬´ì—‡ì¸ê°€?
                                (ì˜ˆ: "MSA êµ¬ì¡°ë¡œ ì „í™˜í•˜ì—¬ ì„œë²„ ë¹„ìš© 20% ì ˆê° í”„ë¡œì íŠ¸", "ê³µëª¨ì „ 2íšŒ ìˆ˜ìƒ", "Kaggle ìƒìœ„ 5% ë‹¬ì„±", "GAU 1ë§Œëª… ì•± ì¶œì‹œ ê²½í—˜")

                            **â€» 'ë…¸ë ¥', 'ëˆê¸°' ê°™ì€ ì¶”ìƒì ì¸ ë‹µë³€ì€ ì œì™¸í•˜ê³ , ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.**
                            """
            })
    except Exception as e:
        print(f"ìœ íŠœë¸Œ í•©ê²© í›„ê¸° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        youtube_summary = "ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì™€ ìœ íŠœë¸Œ ë¶„ì„ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ë¶„ì„
    combined_results = f"--- ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ìµœê·¼ 3ë…„) ---\n{reviews_web_results}\n\n--- ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ (ìµœê·¼ 3ë…„) ---\n{youtube_summary}"

    reviews_summary = analyzer_chain.invoke({
        "target_job": target_job,
        "search_results": combined_results,
        "request": f"""
                  '{company_query}'ì˜ '{target_job}' í˜„ì§ì ì¸í„°ë·° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, **'ì‹¤ì²œ ê°€ëŠ¥í•œ(Actionable)'** ì •ë³´ë§Œ ì•„ë˜ êµ¬ì¡°ë¡œ ë¶„ì„í•´ì¤˜.

                  - **[ì£¼ë‹ˆì–´ì˜ ì²« 1ë…„(First Year)]**: ì…ì‚¬ 1ë…„ì°¨ì—ê²Œ ê¸°ëŒ€í•˜ëŠ” **êµ¬ì²´ì ì¸ ì—­í• ê³¼ ì„±ê³¼ ìˆ˜ì¤€**ì€ ë¬´ì—‡ì¸ê°€?
                  - **[ì„±ê³¼ ì¸¡ì • ë°©ì‹(Performance Metric)]**: ì´ ì§ë¬´ì˜ ì„±ê³¼ëŠ” **ì–´ë–¤ ì§€í‘œ(KPI)**ë¡œ ì¸¡ì •ë˜ëŠ”ê°€? (ì˜ˆ: ê°œë°œ ì†ë„, ë²„ê·¸ ë°œìƒë¥ , ìœ ì € ë¦¬í…ì…˜ ê¸°ì—¬ë„)
                  - **[êµ¬ì²´ì ì¸ ì„±ì¥ ì¡°ì–¸(Actionable Advice)]**: í˜„ì§ìë“¤ì´ ì·¨ì¤€ìƒì—ê²Œ 'ëœ¬êµ¬ë¦„ ì¡ëŠ” ì†Œë¦¬ ë§ê³ ' ì‹¤ì§ˆì ìœ¼ë¡œ ì¤€ë¹„í•´ì•¼ í•œë‹¤ê³  ê°•ì¡°í•˜ëŠ” **êµ¬ì²´ì ì¸ ê¸°ìˆ , ê²½í—˜, ê³µë¶€ ë°©ë²•**ì€ ë¬´ì—‡ì¸ê°€?
                      (ì˜ˆ: "CS ê¸°ì´ˆ, íŠ¹íˆ ë„¤íŠ¸ì›Œí¬ì™€ OSë¥¼ ê¹Šê²Œ íŒŒë¼", "ë‹¨ìˆœ í´ë¡  ì½”ë”© ë§ê³ , íŠ¸ë˜í”½ì„ ê³ ë ¤í•œ ì„¤ê³„ë¥¼ í•´ë´ë¼", "ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•´ë³¸ ê²½í—˜ì´ ì¤‘ìš”í•˜ë‹¤")

                  **â€» 'ì„±ì‹¤í•¨', 'ì—´ì •'ê³¼ ê°™ì€ ì¶”ìƒì ì¸ ì¡°ì–¸ì€ ë°°ì œí•˜ê³ , í–‰ë™ìœ¼ë¡œ ì˜®ê¸¸ ìˆ˜ ìˆëŠ” ì¡°ì–¸ë§Œ ìš”ì•½í•´ì¤˜.**
                  """
    }).content

    # 3. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ (ê¸°ì¡´ê³¼ ë™ì¼)
    print("3. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ì¤‘...")
    interviews_results = tavily_web_search.invoke(
        f'"{company_query}" {target_job} í˜„ì§ì ì¸í„°ë·° "ì¼í•˜ëŠ” ë°©ì‹" "ì¡°ì§ ë¬¸í™”"'
    )
    interview_summary = analyzer_chain.invoke({
        "target_job": target_job,
        "search_results": interviews_results,
        "request": f"'{company_query}'ì˜ '{target_job}' í˜„ì§ì ì¸í„°ë·° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹ ì…ì´ ë¯¸ë¦¬ ì•Œê³  ìˆìœ¼ë©´ ì¢‹ì€ ì ê³¼ ë©´ì ‘ê´€ì—ê²Œ ì–´í•„í•  ìˆ˜ ìˆì„ë§Œí•œ ê²½í—˜ì´ë‚˜ ìŠ¤í™ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”"
    }).content

    # ìµœì¢… ê²°ê³¼ ì¢…í•© (ê¸°ì¡´ê³¼ ë™ì¼)
    print("ì¢…í•© ë¶„ì„ ê²°ê³¼ ìƒì„± ì¤‘...")
    final_summary = f"""
                    ### **{', '.join(target_companies)} {target_job} ì§ë¬´ êµ­ë‚´ ì‹œì¥ ë¶„ì„**

                    #### **1. ì±„ìš© ê³µê³ ì—ì„œ ë“œëŸ¬ë‚œ ê³µì‹ ìš”êµ¬ì‚¬í•­**
                    {postings_summary}

                    #### **2. í•©ê²©ìë“¤ì´ ë§í•˜ëŠ” ì‹¤ì œ ì·¨ì—… ì¤€ë¹„ ê³¼ì • (ìµœê·¼ 3ë…„)**
                    {reviews_summary}

                    #### **3. í˜„ì§ìê°€ ë§í•˜ëŠ” ì‹¤ì œ ì—…ë¬´ í™˜ê²½ê³¼ ì„±ì¥ í¬ì¸íŠ¸**
                    {interview_summary}
                    """

    # ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  í•µì‹¬ ìš”ì•½ ìƒì„±
    print("-> ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  í•µì‹¬ ìš”ì•½ ìƒì„± ì¤‘...")
    distillation_prompt = f"""
    ì•„ë˜ì˜ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œì—ì„œ, '{target_job}' ì§ë¬´ì˜ êµ­ë‚´ ì‹œì¥ í˜„í™©ì— ëŒ€í•œ ê°€ì¥ í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.
    ---
    {final_summary}
    """
    distilled_summary = llm.invoke(distillation_prompt).content

    # ì´ ìš”ì•½ì´ ë¹„ì–´ìˆê±°ë‚˜ ì´ìƒí•˜ê²Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
    print(f"-> [domestic] Distilled Summary (ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬ë  ê°’): '{distilled_summary}'")


    # AgentStateì˜ DomesticAnalysis êµ¬ì¡°ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {
            "domestic_analysis": {
            "final_summary": final_summary,
            "distilled_summary": distilled_summary
            }
    }

def global_tech_trend_node(state: AgentState) -> Dict:
    """
    ëª¨ë“  ì§ë¬´ì— ëŒ€í•´ ê¸€ë¡œë²Œ íŠ¸ë Œë“œë¥¼ ë‹¤ê°ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ê° ì •ë³´ë¥¼ ì—°ê²°í•˜ì—¬ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    Tavily(ì‹¤ë¬´)ì™€ News API(ì‹œì¥)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print("\n--- ê¸€ë¡œë²Œ ë™ì  íŠ¸ë Œë“œ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")
    target_job = state["target_job"]
    target_company = state["target_company"][0]
    domestic_summary = state["domestic_analysis"]["distilled_summary"]

    # --- 1. ì§ë¬´ ë§ì¶¤í˜• ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± ---
    print("1. ì§ë¬´ ë§ì¶¤í˜• ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
    keyword_generation_prompt = f"'{target_job}' ì§ë¬´ì˜ ë¯¸ë˜ ë™í–¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ì˜ë¬¸ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 5ê°œ ìƒì„±í•´ì£¼ì„¸ìš”."
    search_keywords = llm.invoke(keyword_generation_prompt).content
    print(f"-> ìƒì„±ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")

    # --- 2. [â­ìˆ˜ì •â­] 'News API'ë¡œ ê±°ì‹œ í™˜ê²½ íŠ¸ë Œë“œ ë¶„ì„ ---
    print("\n2. News APIë¡œ ê±°ì‹œ í™˜ê²½(ì‹œì¥, ê¸°ì—…) íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
    industry_name_prompt = f"'{target_company}' íšŒì‚¬ê°€ ì†í•œ í•µì‹¬ ì‚°ì—… ë¶„ì•¼ë¥¼ í•œ ë‹¨ì–´ì˜ ì˜ë¬¸ìœ¼ë¡œ ì•Œë ¤ì¤˜."
    industry_name = llm.invoke(industry_name_prompt).content.strip()

    # News APIì— ë” ì í•©í•œ ì¿¼ë¦¬ë¡œ ìˆ˜ì •
    news_query = f'"{target_company}" AND ("{industry_name}" OR business strategy OR market trend OR investment)'

    # [â­ìˆ˜ì •â­] tavily_web_search ëŒ€ì‹  search_global_news ë„êµ¬ ì‚¬ìš©
    macro_results = search_global_news.invoke(news_query)

    macro_trends_summary = analyzer_chain.invoke({
        "target_job": target_job,
        "search_results": macro_results,
        "request": f"""
        '{industry_name}' ì‚°ì—…ê³¼ '{target_company}'ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ì‚°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ **ì‹œì¥ ë™í–¥ì´ë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë³€í™”**ë¥¼ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.
        """
    }).content

    # --- 3. 'Tavily'ë¡œ ì‹¤ë¬´ì íŠ¸ë Œë“œ ë¶„ì„ ---
    print("\n3. Tavilyë¡œ ê¸€ë¡œë²Œ ì‹¤ë¬´ì íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
    practitioner_query = f'"{target_job}" AND ({search_keywords}) case study OR best practices OR industry report'
    practitioner_results = tavily_web_search.invoke(practitioner_query) # ì—¬ê¸°ì„œëŠ” Tavily ì‚¬ìš©
    practitioner_trends_summary = analyzer_chain.invoke({
        "target_job": target_job,
        "search_results": practitioner_results,
        "request": "ìµœì‹  ê¸€ë¡œë²Œ ì„±ê³µ ì‚¬ë¡€ë‚˜ ì „ë¬¸ê°€ ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬, í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ëŠ” ìƒˆë¡œìš´ ì—…ë¬´ ë°©ì‹ì´ë‚˜ ì„±ê³µ ë°©ì •ì‹ì„ ìš”ì•½í•´ì¤˜."
    }).content

    # --- 4. ë¯¸ë˜ ì‹ í˜¸ ë¶„ì„ (YouTube ë“±) ---
    print("\n4. ë¯¸ë˜ ì‹ í˜¸(ì»¨í¼ëŸ°ìŠ¤ ë“±) ë¶„ì„ ì¤‘...")
    future_signals_summary = "ê´€ë ¨ ë¯¸ë˜ ì‹ í˜¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." # (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    # --- 5. ìµœì¢… ì¢…í•© ë° 'ì¶”ë¡ ì ' ë¯¸ë˜ ì˜ˆì¸¡ ---
    print("\n5. ì¢…í•© ë¶„ì„ ë° 'ì¶”ë¡ ì ' ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...")
    combined_trends = f"""
    --- [ì‹œì¥ ê´€ì ] ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„ (News API ê¸°ë°˜) ---
    {macro_trends_summary}

    --- [ì‹¤ë¬´ì ê´€ì ] ì„±ê³µ ë°©ì •ì‹ ë¶„ì„ (Tavily ê¸°ë°˜) ---
    {practitioner_trends_summary}

    --- [ë¯¸ë˜ ì‹ í˜¸ ê´€ì ] ë‹¤ê°€ì˜¬ íŒ¨ëŸ¬ë‹¤ì„ (ì»¨í¼ëŸ°ìŠ¤ ë“±) ---
    {future_signals_summary}
    """

    # (prediction_prompt ë° ë‚˜ë¨¸ì§€ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
    prediction_prompt = f"""
    ë‹¹ì‹ ì€ ìµœê³ ì˜ ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë“¤ì„ 'ì—°ê²°'í•˜ê³  'ì¶”ë¡ 'í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    [êµ­ë‚´ í˜„í™©]: {domestic_summary}
    [ê¸€ë¡œë²Œ ë™í–¥]: {combined_trends}
    [ë³´ê³ ì„œ ì‘ì„± ì§€ì‹œ]
    1. [ë¯¸ë˜ ê²©ì°¨ ë¶„ì„]: ...
    2. [ê¸°íšŒì˜ ì°½]: ...
    3. [ì•¡ì…˜ í”Œëœ]: ...
    """
    prediction = llm.invoke(prediction_prompt).content
    distilled_prediction = llm.invoke(f"ë‹¤ìŒ ë³´ê³ ì„œì˜ í•µì‹¬ ê²°ë¡ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n---\n{prediction}").content

    # [â­ìˆ˜ì •â­] AgentState êµ¬ì¡°ì— ë§ì¶° generated_keywordsë„ í•¨ê»˜ ë°˜í™˜
    return {
        "global_trends": {
            "generated_keywords": search_keywords,
            "prediction": prediction,
            "distilled_prediction": distilled_prediction
        }
    }

# ì§ì—…ì˜ íŠ¹ì„± íŒŒì•…í•´ ë…¼ë¬¸ ì„œì¹­ ì—¬ë¶€ ê²°ì •

def should_research_papers(state: AgentState) -> str:
    """
    target_jobì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë…¼ë¬¸ ë¶„ì„ì´ í•„ìš”í•œì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.
    """
    print("\n--- [Router] ë…¼ë¬¸ ë¶„ì„ í•„ìš” ì—¬ë¶€ íŒë‹¨ ì¤‘... ---")
    target_job = state["target_job"]

    prompt = f"""
    ì‚¬ìš©ìì˜ ëª©í‘œ ì§ë¬´ëŠ” '{target_job}'ì…ë‹ˆë‹¤.
    ì´ ì§ë¬´ëŠ” ìµœì‹  í•™ìˆ  ì—°êµ¬ ë…¼ë¬¸ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ëŠ” ê²ƒì´ ì·¨ì—… ì¤€ë¹„ì— ê²°ì •ì ìœ¼ë¡œ ì¤‘ìš”í•œ, R&D ë˜ëŠ” ë”¥í…Œí¬ ë¶„ì•¼ì— í•´ë‹¹í•©ë‹ˆê¹Œ?
    ì˜¤ì§ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke(prompt).content.strip().lower()
    print(f"-> LLMì˜ íŒë‹¨: {response}")

    if "yes" in response:
        return "research_papers" # 'yes'ì¼ ê²½ìš°, academic_research_nodeë¡œ ê°€ëŠ” ê²½ë¡œ ì´ë¦„
    else:
        return "skip_research"   # 'no'ì¼ ê²½ìš°, ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ê°€ëŠ” ê²½ë¡œ ì´ë¦„

def academic_research_node(state: AgentState) -> Dict[str, Dict[str, str]]:
    """
    ì´ì „ ë…¸ë“œì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'search_arxiv_papers' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬
    ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ì„ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ , ì‹¬ì¸µ í•™ìŠµ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.
    """
    print("\n--- í•™ìˆ  ì—°êµ¬ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ (arXiv) ---")
    target_job = state["target_job"]
    global_trends = state.get("global_trends", {})

    # 1. ì´ì „ ë…¸ë“œì—ì„œ ìƒì„±ëœ ë™ì  í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì–´ ìƒì„±
    search_keywords = global_trends.get("generated_keywords", target_job)
    query = f'"{target_job}" OR ({search_keywords})'
    print(f"arXiv ë™ì  ê²€ìƒ‰ ì¿¼ë¦¬: {query}")

    # 2. ArxivLoader ë¡œì§ ëŒ€ì‹  ìƒˆë¡œ ë§Œë“  ë„êµ¬ í˜¸ì¶œ
    paper_abstracts = search_arxiv_papers.invoke({
        "query": query,
        "load_max_docs": 2
    })

    # 3. ë¶„ì„ ë° ì¡°ì–¸ ìƒì„±
    global_prediction = global_trends.get("prediction", "ì•Œ ìˆ˜ ì—†ìŒ")

    prompt_template = """
    ë‹¹ì‹ ì€ IT ê¸°ìˆ  ë¶„ì•¼ì˜ ìˆ˜ì„ ì—°êµ¬ì›ì´ì ì¹œì ˆí•œ ë©˜í† ì…ë‹ˆë‹¤.
    í˜„ì¬ ì‚°ì—…ê³„ì—ì„œëŠ” "{global_prediction}"ì™€ ê°™ì€ ë¯¸ë˜ê°€ ì˜ˆì¸¡ë˜ê³  ìˆìŠµë‹ˆë‹¤.
    ì´ëŸ¬í•œ ì‚°ì—…ê³„ì˜ ì˜ˆì¸¡ì„ ì—¼ë‘ì— ë‘ê³ , ì•„ë˜ ìµœì‹  í•™ìˆ  ì—°êµ¬ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ í•™ìƒì—ê²Œ ì¡°ì–¸í•´ì£¼ì„¸ìš”.

    --- ë…¼ë¬¸ ë‚´ìš© ---
    {abstracts}

    ---
    [ë¶„ì„ ìš”ì²­]
    í•™ìƒì˜ ëˆˆë†’ì´ì—ì„œ, ì‚°ì—…ê³„ ì˜ˆì¸¡ê³¼ í•™ê³„ ì—°êµ¬ë¥¼ ì—°ê²°í•˜ì—¬ ë‹¤ìŒ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.
    1. **í•µì‹¬ ê¸°ë°˜ ì§€ì‹**: ì´ ì—°êµ¬ë“¤ì„ ì´í•´í•˜ê¸° ìœ„í•´ í•™ìƒì´ ê³µë¶€í•´ì•¼ í•  í•µì‹¬ ì´ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?
    2. **ë¯¸ë˜ ì—­ëŸ‰**: ì´ ë…¼ë¬¸ë“¤ì´ ì•”ì‹œí•˜ëŠ” ë¯¸ë˜ì˜ {target_job}ì—ê²Œ ì¤‘ìš”í•´ì§ˆ ìƒˆë¡œìš´ ê¸°ìˆ  ì—­ëŸ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?
    3. **í•™ìŠµ ë°©í–¥**: ì´ ê°œë…ë“¤ì„ ê²½í—˜í•´ë³¼ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ í† ì´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)
    research_analyzer_chain = prompt | llm

    summary = research_analyzer_chain.invoke({
        "target_job": target_job,
        "global_prediction": global_prediction,
        "abstracts": paper_abstracts
    }).content

    print("í•™ìˆ  ì—°êµ¬ ë¶„ì„ ì™„ë£Œ.")

    # ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  í•µì‹¬ ìš”ì•½ ìƒì„±
    distillation_prompt = f"ë‹¤ìŒ í•™ìˆ  ë¶„ì„ ë³´ê³ ì„œì˜ í•µì‹¬ ê²°ë¡ ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n---\n{summary}"
    distilled_summary = llm.invoke(distillation_prompt).content
    print(f"-> í•™ìˆ  ì—°êµ¬ í•µì‹¬ ìš”ì•½ ìƒì„± ì™„ë£Œ: '{distilled_summary}'")

    # AgentState êµ¬ì¡°ì— ë§ì¶° ë‘ ê°€ì§€ ìš”ì•½ë³¸ì„ ëª¨ë‘ ë°˜í™˜
    return {
        "academic_research": {
            "summary": summary,
            "distilled_summary": distilled_summary
        }
    }

def generate_final_report(state: AgentState) -> Dict:
    print("\n--- [Final Step] ìµœì¢… ë³´ê³ ì„œ ìƒì„± ---")

    domestic_summary = state["domestic_analysis"]["distilled_summary"]
    global_prediction = state["global_trends"]["distilled_prediction"]


    academic_summary = ""
    if state.get("academic_research"):
        academic_summary = state["academic_research"]["summary"]

    academic_section = f"\n[í•™ìˆ  ì—°êµ¬ ë™í–¥]: {academic_summary}" if academic_summary else ""

    final_report_prompt = f"""
    ì•„ë˜ì˜ í•µì‹¬ ìš”ì•½ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, '{state['target_job']}' ì·¨ì—… ì¤€ë¹„ìƒì„ ìœ„í•œ ìµœì¢… ì»¤ë¦¬ì–´ ë¡œë“œë§µì„ ì™„ì„±ë„ ë†’ê²Œ ì‘ì„±í•´ì¤˜.

    [êµ­ë‚´ í˜„í™©]: {domestic_summary}
    [ê¸€ë¡œë²Œ ì˜ˆì¸¡]: {global_prediction}{academic_section}
    """
    final_report = llm.invoke(final_report_prompt).content

    return {"final_report": final_report}

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

ANALYSIS_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ {target_job} ë¶„ì•¼ ì „ë¬¸ ì»¤ë¦¬ì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì²­ì‚¬í•­ì„ ë¶„ì„í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
--- ê²€ìƒ‰ ê²°ê³¼ ---
{search_results}
--- ë¶„ì„ ìš”ì²­ ---
{request}
"""
prompt = ChatPromptTemplate.from_template(ANALYSIS_PROMPT_TEMPLATE)
analyzer_chain = prompt | llm

workflow = StateGraph(AgentState)
workflow.add_node("domestic_analysis", domestic_job_analysis_node)
workflow.add_node("global_trends", global_tech_trend_node)
workflow.add_node("academic_research", academic_research_node)
workflow.add_node("generate_final_report", generate_final_report)

workflow.set_entry_point("domestic_analysis")
workflow.add_edge("domestic_analysis", "global_trends")
workflow.add_conditional_edges(
    "global_trends",
    should_research_papers,
    {
        "research_papers": "academic_research",
        "skip_research": "generate_final_report"
    }
)
workflow.add_edge("academic_research", "generate_final_report")
workflow.add_edge("generate_final_report", END)

app = workflow.compile()


def run_graph_analysis(target_job: str, target_company: list) -> str:
    """LangGraphë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸš€ LangGraph ë¶„ì„ ì‹œì‘...")
    initial_state = {
        "target_job": target_job,
        "target_company": target_company
    }
    # stream ëŒ€ì‹  invokeë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ê²°ê³¼ë§Œ ë°›ìŒ
    final_state = app.invoke(initial_state)
    final_report = final_state.get("final_report", "ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    print(" LangGraph ë¶„ì„ ì™„ë£Œ.")
    return final_report

