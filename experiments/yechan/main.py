import pprint
import json
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.tools import ArxivQueryRun, TavilySearchResults
from googleapiclient.discovery import build
from IPython.display import display, Markdown

# (AgentState, ëª¨ë“  Pydantic ëª¨ë¸, ëª¨ë“  ë…¸ë“œ ë° í—¬í¼ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

if __name__ == '__main__':
    # --- 1. ì¤€ë¹„ ë‹¨ê³„ ---
    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("âœ… API ë° ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸ”´ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
        exit()

    llm_final_analyzer = ChatOpenAI(
        model="gpt-4o", # <-- ëª¨ë¸ ì´ë¦„ ë³€ê²½
        temperature=0.2, # ì•½ê°„ì˜ ìœ ì—°ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ë‘ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        model_kwargs={"response_format": {"type": "json_object"}}
    )

    # 2. ì¼ë°˜ì ì¸ êµ¬ì¡°í™” ë°ì´í„° ë¶„ì„ìš© (ëŒ€í•™ì›ìƒê¸‰)
    llm_structured_analyzer = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}}
    )

    # 3. ì°½ì˜ì ì¸ ë¬¸ì¥ ìƒì„±ìš©
    llm_creative = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=1
    )

    # 4. ë¹ ë¥´ê³  ì €ë ´í•œ ë‹¨ìˆœ ë¶„ë¥˜/ë³€í™˜ìš©
    llm_fast_classifier = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0,
        max_tokens=100
    )

    tavily_tool = TavilySearchResults(max_results=3)
    arxiv_tool = ArxivQueryRun()

    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    workflow = StateGraph(AgentState)

    # --- 2a. ëª¨ë“  ë…¸ë“œ ì •ì˜ ---
    workflow.add_node("intent_classifier", intent_classifier_node)
    workflow.add_node("user_profiling", user_profiling_node)
    # êµ­ë‚´ ë¶„ì„ (ë³‘ë ¬)
    workflow.add_node("analyze_postings", analyze_postings_node)
    workflow.add_node("analyze_reviews", analyze_reviews_node)
    workflow.add_node("analyze_interviews", analyze_interviews_node)
    workflow.add_node("combine_domestic", combine_domestic_analysis_node)
    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ (ë³‘ë ¬)
    workflow.add_node("analyze_tech_trends", analyze_tech_trends_node)
    workflow.add_node("analyze_market_trends", analyze_market_trends_node)
    workflow.add_node("analyze_leaders_vision", analyze_leaders_vision_node)
    workflow.add_node("combine_global", combine_global_trends_node)
    # ê°­ ë¶„ì„ ë° ë¼ìš°í„°
    workflow.add_node("gap_analysis", gap_analysis_node)
    workflow.add_node("router", llm_router_node)
    # ìµœì¢… ì¶”ì²œ ë…¸ë“œ
    workflow.add_node("recommend_learning", recommend_learning_node)
    workflow.add_node("recommend_storytelling", recommend_storytelling_node) # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œ ì¶”ê°€


    # --- 2b. ì—£ì§€(ì—°ê²°ì„ ) ì •ì˜ ---
    workflow.set_entry_point("intent_classifier")

    # ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°
    workflow.add_conditional_edges(
        "intent_classifier",
        lambda state: state["intent_classification"],
        {
            "portfolio_analysis": "user_profiling",
            "irrelevant": END
        }
    )

    # êµ­ë‚´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë³‘ë ¬)
    workflow.add_edge("user_profiling", "analyze_postings")
    workflow.add_edge("user_profiling", "analyze_reviews")
    workflow.add_edge("user_profiling", "analyze_interviews")
    workflow.add_edge("analyze_postings", "combine_domestic")
    workflow.add_edge("analyze_reviews", "combine_domestic")
    workflow.add_edge("analyze_interviews", "combine_domestic")

    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë³‘ë ¬)
    workflow.add_edge("combine_domestic", "analyze_tech_trends")
    workflow.add_edge("combine_domestic", "analyze_market_trends")
    workflow.add_edge("combine_domestic", "analyze_leaders_vision")
    workflow.add_edge("analyze_tech_trends", "combine_global")
    workflow.add_edge("analyze_market_trends", "combine_global")
    workflow.add_edge("analyze_leaders_vision", "combine_global")

    # ê°­ ë¶„ì„ ë° ë¼ìš°í„° ì—°ê²°
    workflow.add_edge("combine_global", "gap_analysis")
    workflow.add_edge("gap_analysis", "router")

    # [ìˆ˜ì •] ë¼ìš°í„°ì˜ ê²°ì •ì— ë”°ë¼ ì¶”ì²œ ë…¸ë“œë¡œ ë¶„ê¸°
    workflow.add_conditional_edges(
        "router",
        lambda state: state["next_action"],
        {
            "recommend_learning": "recommend_learning",
            "recommend_storytelling": "recommend_storytelling" # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œë¡œ ì—°ê²°
        }
    )

    # [ìˆ˜ì •] ê° ì¶”ì²œ ë…¸ë“œê°€ ì‹¤í–‰ëœ í›„ ê·¸ë˜í”„ ì¢…ë£Œ
    workflow.add_edge("recommend_learning", END)
    workflow.add_edge("recommend_storytelling", END) # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œë„ ì¢…ë£Œë¡œ ì—°ê²°

    app = workflow.compile()
    print("âœ… Workflow compiled successfully!")

    # --- 3. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ì •ì˜ ---
    # """
    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ì—”ì§€ë‹ˆì–´",
        "í¬ë§ ê¸°ì—…": ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤"],
        "í•™ë…„/í•™ê¸°": "4í•™ë…„ 1í•™ê¸°",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì»´í“¨í„°ê³µí•™ê³¼",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "Python, SQL, PyTorch, AWS S3/EC2 ê¸°ë³¸ ì‚¬ìš© ê²½í—˜, ì •ë³´ì²˜ë¦¬ê¸°ì‚¬",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ìº¡ìŠ¤í†¤ ë””ìì¸ í”„ë¡œì íŠ¸ (PyTorch ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ ë° ë°°í¬ ì‹œë„)",
        # "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "MLOps ë¶„ì•¼ë¡œ ì „ë¬¸ì„±ì„ í‚¤ìš°ê³  ì‹¶ì€ë°, ì–´ë–¤ ê¸°ìˆ ì„ ë” ê³µë¶€í•´ì•¼ í• ê¹Œìš”?", 
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"
    }

    """

    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ëª¨ë¸ ìµœì í™” ì—”ì§€ë‹ˆì–´ ë˜ëŠ” ê²½ëŸ‰í™” ì—°êµ¬ì›",
        "í¬ë§ ê¸°ì—…": ["ì‚¼ì„±ì „ì", "SKT", "Lunit"],
        "í•™ë…„/í•™ê¸°": "ì„ì‚¬ ì¡¸ì—… í›„ ì·¨ì—…ì¤€ë¹„ìƒ",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ìê³µí•™ê³¼ ì„ì‚¬ ì¡¸ì—…",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "Python, C++, Linux, PyTorch, TensorFlow, ONNX, ëª¨ë¸ ê²½ëŸ‰í™” (Pruning, Quantization), CUDA í”„ë¡œê·¸ë˜ë° ê¸°ë³¸",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ì„ì‚¬ ì¡¸ì—… ë…¼ë¬¸: 'Transformer ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ Knowledge Distillationì„ í†µí•œ ê²½ëŸ‰í™” ì—°êµ¬', ììœ¨ì£¼í–‰ ê´€ë ¨ í•™íšŒì—ì„œ ë…¼ë¬¸ í¬ìŠ¤í„° ë°œí‘œ ê²½í—˜",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ì œ ì„ì‚¬ ì—°êµ¬ ê²½í—˜ì´ ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œ ì–´ë–»ê²Œ ì–´í•„ë  ìˆ˜ ìˆì„ì§€, ë©´ì ‘ì—ì„œ ì–´ë–»ê²Œ ì„¤ëª…í•´ì•¼ ë‹¤ë¥¸ ì§€ì›ìë“¤ê³¼ ì°¨ë³„í™”ë  ìˆ˜ ìˆì„ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤."
    }
    """
    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm_final_analyzer": llm_final_analyzer,
        "llm_structured_analyzer": llm_structured_analyzer,
        "llm_creative": llm_creative,
        "llm_fast_classifier": llm_fast_classifier,
        "tools": {"tavily": tavily_tool, "arxiv": arxiv_tool}
    }

# --- 4. ê·¸ë˜í”„ ì‹¤í–‰ (ë°±ì—”ë“œ ì—­í• ) ---
print("\nğŸš€ ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ (ë°±ì—”ë“œ ì—­í• )")
print("="*80)

execution_log = {}
final_state = None  # <-- [ì¶”ê°€] ìµœì¢… ìƒíƒœë¥¼ ì €ì¥í•  ë³€ìˆ˜

# streamì„ í†µí•´ ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³  ë¡œê·¸ì— ê¸°ë¡
for state_update in app.stream(initial_state):
    node_name = list(state_update.keys())[0]
    node_output = state_update[node_name]

    # ê°œë°œì í™•ì¸ìš© ë‚´ë¶€ ë¡œê·¸ ì¶œë ¥
    print(f"\n--- ğŸ“Œ [ë…¸ë“œ: {node_name}] ì‹¤í–‰ ì™„ë£Œ (ë‚´ë¶€ ë°ì´í„°) ---")
    pprint.pprint(node_output)

    # ì‹¤í–‰ ë¡œê·¸ì— í˜„ì¬ ë…¸ë“œì˜ ëª¨ë“  ì¶œë ¥ê°’ì„ ì—…ë°ì´íŠ¸
    if node_output:
        execution_log.update(node_output)

    final_state = state_update # <-- [ì¶”ê°€] ë§¤ë²ˆ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ë®ì–´ì“°ê¸°

print("\n\nâœ… ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ê²°ê³¼ê°€ execution_logì™€ final_stateì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("ë‹¤ìŒ ì…€ì—ì„œ ìµœì¢… ê²°ê³¼ë¬¼ì„ í™•ì¸í•˜ì„¸ìš”.")

from IPython.display import display, Markdown
from typing import Dict, Any

def display_section(log: Dict[str, Any], key: str, title: str):
    """
    ì‹¤í–‰ ë¡œê·¸(log)ì—ì„œ íŠ¹ì • í‚¤(key)ì˜ ë©”ì‹œì§€ë¥¼ ì°¾ì•„
    ì§€ì •ëœ ì œëª©(title)ê³¼ í•¨ê»˜ Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    message = log.get(key)
    # ë©”ì‹œì§€ê°€ ì¡´ì¬í•˜ê³ , ê³µë°± ë¬¸ìê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    if message and str(message).strip():
        display(Markdown(f"\n{title}"))
        display(Markdown(message))

# --- ìµœì¢… ê²°ê³¼ë¬¼ ì¶œë ¥ (ì‚¬ìš©ì í™”ë©´ ë Œë”ë§) ---
print("="*80)
print("âœ¨ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ì—ì´ì „íŠ¸ ìµœì¢… ë³´ê³ ì„œ âœ¨")
print("="*80)

# í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¶„ì„ ë‹¨ê³„ë³„ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
display_section(execution_log, "streaming_intent", "### ğŸ” ë¶„ì„ ì‹œì‘")
display_section(execution_log, "streaming_user_profile", "### ğŸ‘¤ í”„ë¡œí•„ ìš”ì•½")
display_section(execution_log, "streaming_domestic_analysis", "### ğŸ‡°ğŸ‡· êµ­ë‚´ ì±„ìš© ì‹œì¥ ë™í–¥")
display_section(execution_log, "streaming_global_analysis", "### ğŸŒ ê¸€ë¡œë²Œ ê¸°ìˆ  íŠ¸ë Œë“œ")
display_section(execution_log, "streaming_gap_analysis", "### ğŸ“Š ì¢…í•© ì—­ëŸ‰ ì§„ë‹¨")
display_section(execution_log, "streaming_route", "### ğŸ§­ ì¶”ì²œ ë°©í–¥ ì„¤ì •")

# ìµœì¢… ì¶”ì²œ ë³´ê³ ì„œëŠ” 'í•™ìŠµ' ë˜ëŠ” 'ìŠ¤í† ë¦¬í…”ë§' ì¤‘ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ,
# ë‘˜ ì¤‘ ë‚´ìš©ì´ ìˆëŠ” í•˜ë‚˜ë¥¼ ì°¾ì•„ì„œ í•´ë‹¹í•˜ëŠ” ì œëª©ê³¼ í•¨ê»˜ ì¶œë ¥í•©ë‹ˆë‹¤.
if execution_log.get("streaming_study_recommend"):
    display_section(execution_log, "streaming_study_recommend", "### ğŸ“š ë§ì¶¤í˜• í•™ìŠµ ë¡œë“œë§µ")
elif execution_log.get("streaming_story_recommend"):
    display_section(execution_log, "streaming_story_recommend", "### ğŸ™ï¸ ë§ì¶¤í˜• ìŠ¤í† ë¦¬í…”ë§ ê°€ì´ë“œ")

print("="*80)
