import pprint
import json
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_community.tools import ArxivQueryRun, TavilySearchResults
from googleapiclient.discovery import build
from IPython.display import display, Markdown

# (AgentState, ëª¨ë“  Pydantic ëª¨ë¸, ëª¨ë“  ë…¸ë“œ ë° í—¬í¼ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

if __name__ == '__main__':
    # --- 1. ì¤€ë¹„ ë‹¨ê³„ ---
    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("âœ… API ë° ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸ”´ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
        exit()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    tavily_tool = TavilySearchResults(max_results=3)
    arxiv_tool = ArxivQueryRun()

    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    workflow = StateGraph(AgentState)

    # --- 2a. ëª¨ë“  ë…¸ë“œ ì •ì˜ ---
    workflow.add_node("intent_classifier", intent_classifier_node)
    workflow.add_node("user_profiling", user_profiling_node)
    # êµ­ë‚´ ë¶„ì„ (ë³‘ë ¬)
    workflow.add_node("analyze_postings", analyze_postings_node)
    workflow.add_node("analyze_reviews", analyze_reviews_node)
    workflow.add_node("analyze_interviews", analyze_interviews_node)
    workflow.add_node("combine_domestic", combine_domestic_analysis_node)
    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ (ë³‘ë ¬)
    workflow.add_node("analyze_tech_trends", analyze_tech_trends_node)
    workflow.add_node("analyze_market_trends", analyze_market_trends_node)
    workflow.add_node("analyze_leaders_vision", analyze_leaders_vision_node)
    workflow.add_node("combine_global", combine_global_trends_node)
    # ê°­ ë¶„ì„ ë° ë¼ìš°í„°
    workflow.add_node("gap_analysis", gap_analysis_node)
    workflow.add_node("router", llm_router_node)
    # ìµœì¢… ì¶”ì²œ ë…¸ë“œ
    workflow.add_node("recommend_learning", recommend_learning_node)
    workflow.add_node("recommend_storytelling", recommend_storytelling_node) # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œ ì¶”ê°€


    # --- 2b. ì—£ì§€(ì—°ê²°ì„ ) ì •ì˜ ---
    workflow.set_entry_point("intent_classifier")

    # ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°
    workflow.add_conditional_edges(
        "intent_classifier",
        lambda state: state["intent_classification"],
        {
            "portfolio_analysis": "user_profiling",
            "irrelevant": END
        }
    )

    # êµ­ë‚´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë³‘ë ¬)
    workflow.add_edge("user_profiling", "analyze_postings")
    workflow.add_edge("user_profiling", "analyze_reviews")
    workflow.add_edge("user_profiling", "analyze_interviews")
    workflow.add_edge("analyze_postings", "combine_domestic")
    workflow.add_edge("analyze_reviews", "combine_domestic")
    workflow.add_edge("analyze_interviews", "combine_domestic")

    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë³‘ë ¬)
    workflow.add_edge("combine_domestic", "analyze_tech_trends")
    workflow.add_edge("combine_domestic", "analyze_market_trends")
    workflow.add_edge("combine_domestic", "analyze_leaders_vision")
    workflow.add_edge("analyze_tech_trends", "combine_global")
    workflow.add_edge("analyze_market_trends", "combine_global")
    workflow.add_edge("analyze_leaders_vision", "combine_global")
    
    # ê°­ ë¶„ì„ ë° ë¼ìš°í„° ì—°ê²°
    workflow.add_edge("combine_global", "gap_analysis")
    workflow.add_edge("gap_analysis", "router")

    # [ìˆ˜ì •] ë¼ìš°í„°ì˜ ê²°ì •ì— ë”°ë¼ ì¶”ì²œ ë…¸ë“œë¡œ ë¶„ê¸°
    workflow.add_conditional_edges(
        "router",
        lambda state: state["next_action"],
        {
            "recommend_learning": "recommend_learning",
            "recommend_storytelling": "recommend_storytelling" # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œë¡œ ì—°ê²°
        }
    )

    # [ìˆ˜ì •] ê° ì¶”ì²œ ë…¸ë“œê°€ ì‹¤í–‰ëœ í›„ ê·¸ë˜í”„ ì¢…ë£Œ
    workflow.add_edge("recommend_learning", END)
    workflow.add_edge("recommend_storytelling", END) # <-- ìŠ¤í† ë¦¬í…”ë§ ë…¸ë“œë„ ì¢…ë£Œë¡œ ì—°ê²°
    
    app = workflow.compile()
    print("âœ… Workflow compiled successfully!")

    # --- 3. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ì •ì˜ ---
    """
    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ì—”ì§€ë‹ˆì–´",
        "í¬ë§ ê¸°ì—…": ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤"],
        "í•™ë…„/í•™ê¸°": "4í•™ë…„ 1í•™ê¸°",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì»´í“¨í„°ê³µí•™ê³¼",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "Python, SQL, PyTorch, AWS S3/EC2 ê¸°ë³¸ ì‚¬ìš© ê²½í—˜, ì •ë³´ì²˜ë¦¬ê¸°ì‚¬",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ìº¡ìŠ¤í†¤ ë””ìì¸ í”„ë¡œì íŠ¸ (PyTorch ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ ë° ë°°í¬ ì‹œë„)",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "MLOps ë¶„ì•¼ë¡œ ì „ë¬¸ì„±ì„ í‚¤ìš°ê³  ì‹¶ì€ë°, ì–´ë–¤ ê¸°ìˆ ì„ ë” ê³µë¶€í•´ì•¼ í• ê¹Œìš”?"
    }

    """

    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ëª¨ë¸ ìµœì í™” ì—”ì§€ë‹ˆì–´ ë˜ëŠ” ê²½ëŸ‰í™” ì—°êµ¬ì›",
        "í¬ë§ ê¸°ì—…": ["ì‚¼ì„±ì „ì", "SKT", "Lunit"],
        "í•™ë…„/í•™ê¸°": "ì„ì‚¬ ì¡¸ì—… í›„ ì·¨ì—…ì¤€ë¹„ìƒ",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ìê³µí•™ê³¼ ì„ì‚¬ ì¡¸ì—…",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "Python, C++, Linux, PyTorch, TensorFlow, ONNX, ëª¨ë¸ ê²½ëŸ‰í™” (Pruning, Quantization), CUDA í”„ë¡œê·¸ë˜ë° ê¸°ë³¸",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ì„ì‚¬ ì¡¸ì—… ë…¼ë¬¸: 'Transformer ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ Knowledge Distillationì„ í†µí•œ ê²½ëŸ‰í™” ì—°êµ¬', ììœ¨ì£¼í–‰ ê´€ë ¨ í•™íšŒì—ì„œ ë…¼ë¬¸ í¬ìŠ¤í„° ë°œí‘œ ê²½í—˜",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ì œ ì„ì‚¬ ì—°êµ¬ ê²½í—˜ì´ ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œ ì–´ë–»ê²Œ ì–´í•„ë  ìˆ˜ ìˆì„ì§€, ë©´ì ‘ì—ì„œ ì–´ë–»ê²Œ ì„¤ëª…í•´ì•¼ ë‹¤ë¥¸ ì§€ì›ìë“¤ê³¼ ì°¨ë³„í™”ë  ìˆ˜ ìˆì„ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤."
    }

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm,
        "tools": {"tavily": tavily_tool, "arxiv": arxiv_tool}
    }

# --- 4. ê·¸ë˜í”„ ì‹¤í–‰ (ë°±ì—”ë“œ ì—­í• ) ---
print("\nğŸš€ ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ (ë°±ì—”ë“œ ì—­í• )")
print("="*80)

execution_log = {}
final_state = None  # <-- [ì¶”ê°€] ìµœì¢… ìƒíƒœë¥¼ ì €ì¥í•  ë³€ìˆ˜

# streamì„ í†µí•´ ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³  ë¡œê·¸ì— ê¸°ë¡
for state_update in app.stream(initial_state):
    node_name = list(state_update.keys())[0]
    node_output = state_update[node_name]

    # ê°œë°œì í™•ì¸ìš© ë‚´ë¶€ ë¡œê·¸ ì¶œë ¥
    print(f"\n--- ğŸ“Œ [ë…¸ë“œ: {node_name}] ì‹¤í–‰ ì™„ë£Œ (ë‚´ë¶€ ë°ì´í„°) ---")
    pprint.pprint(node_output)

    # ì‹¤í–‰ ë¡œê·¸ì— í˜„ì¬ ë…¸ë“œì˜ ëª¨ë“  ì¶œë ¥ê°’ì„ ì—…ë°ì´íŠ¸
    if node_output:
        execution_log.update(node_output)
    
    final_state = state_update # <-- [ì¶”ê°€] ë§¤ë²ˆ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ë®ì–´ì“°ê¸°

print("\n\nâœ… ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ê²°ê³¼ê°€ execution_logì™€ final_stateì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("ë‹¤ìŒ ì…€ì—ì„œ ìµœì¢… ê²°ê³¼ë¬¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# --- 5. ìµœì¢… ê²°ê³¼ë¬¼ ì¶œë ¥ (í”„ë¡ íŠ¸ì—”ë“œ ì—­í• ) ---

from IPython.display import display, Markdown

print("="*80)
print("âœ¨ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼ (ì‚¬ìš©ì í™”ë©´) âœ¨")
print("="*80)

# 1. ì˜ë„ ë¶„ì„ ë©”ì‹œì§€ ì¶œë ¥
intent_message = execution_log.get("streaming_intent", "")
if intent_message:
    display(Markdown(f"### ğŸ” ì´ˆê¸° ë¶„ì„"))
    display(Markdown(intent_message))

# 2. í”„ë¡œí•„ ë¶„ì„ ë©”ì‹œì§€ ì¶œë ¥
profile_message = execution_log.get("streaming_user_profile", "")
if profile_message:
    display(Markdown(f"\n### ğŸ‘¤ í”„ë¡œí•„ ìš”ì•½"))
    display(Markdown(profile_message))

# 3. ê°­ ë¶„ì„ ë©”ì‹œì§€ ì¶œë ¥
gap_message = execution_log.get("streaming_gap_analysis", "")
if gap_message:
    display(Markdown(f"\n### ğŸ“Š ì—­ëŸ‰ ì§„ë‹¨"))
    display(Markdown(gap_message))

# 4. ë¼ìš°í„°ì˜ ì¶”ì²œ ë°©í–¥ ë¯¸ë¦¬ë³´ê¸° ë©”ì‹œì§€ ì¶œë ¥
router_message = execution_log.get("streaming_route", "") # 'streaming_route' -> 'streaming_message'
if router_message:
    display(Markdown(f"\n### ğŸ§­ ì¶”ì²œ ë°©í–¥ ë¯¸ë¦¬ë³´ê¸°"))
    display(Markdown(router_message))

# 5. ìµœì¢… ì¶”ì²œ ë³´ê³ ì„œ ì¶œë ¥ (í•™ìŠµ ë˜ëŠ” ìŠ¤í† ë¦¬í…”ë§)
learning_report_text = execution_log.get("streaming_study_recommend", "")
story_report_text = execution_log.get("streaming_story_recommend", "")

if learning_report_text:
    display(Markdown(f"\n### ğŸ“š ë§ì¶¤í˜• í•™ìŠµ ë¡œë“œë§µ"))
    display(Markdown(learning_report_text))
elif story_report_text:
    display(Markdown(f"\n### ğŸ™ï¸ ë§ì¶¤í˜• ìŠ¤í† ë¦¬í…”ë§ ê°€ì´ë“œ"))
    display(Markdown(story_report_text))

print("="*80)
