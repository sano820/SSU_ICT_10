# -*- coding: utf-8 -*-
"""report_version3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwuR9GuCQeEe54IwrQILgvdmc92BikwR
"""

!pip install -q openai==1.101.0 langchain==0.3.27 langchain-core==0.3.74 langchain-openai==0.3.30 langchain-community==0.3.27 pytube==15.0.0 tavily-python==0.7.11 youtube-search-python==1.6.6 langgraph==0.6.4 arxiv==2.2.0 pymupdf==1.26.3 youtube_transcript_api==1.2.2

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')

# %cd /content/drive/MyDrive/ICT_í”„ë¡œì íŠ¸

import json
from datetime import datetime, timedelta

# OpenAI & LangChain
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

# LangChain Community Tools & Loaders
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.tools import ArxivQueryRun, TavilySearchResults
from langchain_community.document_loaders import ArxivLoader
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# LangGraph
from langgraph.graph import StateGraph, END

# Typing
from typing import TypedDict, List, Dict, Any, Optional, Annotated, Type, Literal
import operator

# Tools
from tools import analyze_video_content, search_naver_news, search_global_news, search_arxiv_papers, tavily_web_search, find_videos_with_transcripts, analyze_youtube_topic

# Prompt
from prompts import USER_PROFILING_PROMPT, DOMESTIC_JOB_ANALYSIS_PROMPT, GLOBAL_TREND_ANALYSIS_PROMPT, GAP_ANALYSIS_PROMPT, LLM_ROUTER_PROMPT, RECOMMEND_LEARNING, RECOMMEND_STORYTELLING_PROMPT, FINAL_REPORT_PROMPT

import os
from google.colab import userdata

os.environ['OPENAI_API_KEY'] = userdata.get('ssu')
os.environ['TAVILY_API_KEY'] = userdata.get('tavily')
os.environ['YOUTUBE_API_KEY'] = userdata.get('youtube')
os.environ['NAVER_CLIENT_ID'] = userdata.get('naver_client_id')
os.environ['NAVER_CLIENT_SECRET'] = userdata.get('naver_client_secret')
os.environ['NEWS_API_KEY'] = userdata.get('news_api_key')

class AgentAPIs:
    """
    ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  API í‚¤ë¥¼ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        print("-> API í‚¤ ë¡œë”© ì‹œì‘...")

        # userdataì—ì„œ ê° API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        self.openai_api_key = userdata.get('ssu')
        self.tavily_api_key = userdata.get('tavily')
        self.youtube_api_key = userdata.get('youtube')
        self.naver_client_id = userdata.get('naver_client_id')
        self.naver_client_secret = userdata.get('naver_client_secret')
        self.news_api_key = userdata.get('news_api_key')

        # í•„ìˆ˜ í‚¤ë“¤ì´ ëª¨ë‘ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if not all([self.openai_api_key, self.tavily_api_key, self.youtube_api_key]):
            raise ValueError("í•„ìˆ˜ API í‚¤(OpenAI, Tavily, YouTube)ê°€ Colab Secretsì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print("âœ… ëª¨ë“  API í‚¤ ë¡œë”© ì™„ë£Œ.")

    def set_env_variables(self):
        """
        ë¡œë“œëœ í‚¤ë“¤ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´)
        """
        print("-> í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì¤‘...")
        os.environ['OPENAI_API_KEY'] = self.openai_api_key
        os.environ['TAVILY_API_KEY'] = self.tavily_api_key
        os.environ['YOUTUBE_API_KEY'] = self.youtube_api_key
        os.environ['NAVER_CLIENT_ID'] = self.naver_client_id
        os.environ['NAVER_CLIENT_SECRET'] = self.naver_client_secret
        os.environ['NEWS_API_KEY'] = self.news_api_key
        print("âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ.")

class AgentState(TypedDict):
    user_profile_raw: Dict[str, Any]
    api_keys: Any
    youtube_service: Any
    llm: Any
    user_profile_structured: Dict[str, Any]
    target_job: List[str]
    target_company: List[str]
    user_questions: str
    domestic_analysis_components: Dict[str, Any]
    domestic_keywords: Dict[str, Any]
    global_trends: Dict[str, Any]
    gap_analysis: Dict[str, Any]
    next_action: str
    routing_reason_narrative: str
    routing_reason_structured: Dict[str, Any]
    learning_recommendations: Dict[str, Any]
    storytelling_recommendations: Dict[str, Any]
    final_report: str

def extract_structured_data_flexible(
    prompt_inputs: Dict[str, Any],
    extraction_prompt_template: str,
    pydantic_model: Type[BaseModel],
    llm: Any,
    log_message: str
) -> Dict[str, Any]:
    """
    ì£¼ì–´ì§„ 'ì–´ë–¤' ì…ë ¥ ë³€ìˆ˜(prompt_inputs)ì™€ Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë” ìœ ì—°í•œ ë²”ìš© í—¬í¼ í•¨ìˆ˜.
    """
    print(log_message)

    try:
        parser = PydanticOutputParser(pydantic_object=pydantic_model)

        prompt = ChatPromptTemplate.from_template(
            template=extraction_prompt_template,
            partial_variables={"format_instructions": parser.get_format_instructions()},
        )

        extractor_chain = prompt | llm | parser
        # [ìˆ˜ì •] ì¸ìë¡œ ë°›ì€ prompt_inputs ë”•ì…”ë„ˆë¦¬ë¥¼ ê·¸ëŒ€ë¡œ invokeì— ì „ë‹¬
        extracted_data = extractor_chain.invoke(prompt_inputs)
        return extracted_data.model_dump()

    except Exception as e:
        print(f"-> êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return {}

class UserProfile(TypedDict):
    academic_year: Optional[str]
    major: Optional[str]
    skills_and_certs: Optional[str]
    experience_specs: Optional[str]
    goals: Optional[str]
    narrative_summary: str

class RefinedCompanies(BaseModel):
    companies: List[str] = Field(description="ì •ì œëœ ê°œë³„ ê³µì‹ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸")

def user_profiling_node(state: AgentState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì •ë³´ë¥¼ ì •ì œ/êµ¬ì¡°í™”í•˜ê³ ,
    'ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì 'ì„ ë³„ë„ì˜ í‚¤ë¡œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 0] ì‚¬ìš©ì ì •ë³´ êµ¬ì¡°í™” ë…¸ë“œ ì‹¤í–‰ ---")

    user_info = state["user_profile_raw"]

    key_to_label = {
        "ëª©í‘œ ì§ë¬´": "ëª©í‘œ ì§ë¬´",
        "í¬ë§ ê¸°ì—…": "í¬ë§ ê¸°ì—…",
        "í•™ë…„/í•™ê¸°": "í•™ë…„/í•™ê¸°",
        "ì¬í•™ ì—¬ë¶€": "ì¬í•™ ì—¬ë¶€",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ê³µ",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™",
        "ê´€ì‹¬ ë¶„ì•¼ ë° ëª©í‘œ": "ê´€ì‹¬ ë¶„ì•¼ ë° ëª©í‘œ",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì "
    }
    target_job = user_info.get("ëª©í‘œ ì§ë¬´", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
    target_company = user_info.get("í¬ë§ ê¸°ì—…", [])
    profile_parts = [f"- {label}: {user_info.get(key)}" for key, label in key_to_label.items() if user_info.get(key) and str(user_info.get(key)).strip()]
    profile_text = "\n".join(profile_parts) if profile_parts else "ì…ë ¥ëœ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

    user_info_extract_prompt = USER_PROFILING_PROMPT["user_info_extract"]

    parser = JsonOutputParser(pydantic_object=UserProfile)
    prompt = ChatPromptTemplate.from_template(template=user_info_extract_prompt)
    profiling_chain = prompt | llm | parser

    try:
        structured_profile = profiling_chain.invoke({
            "profile_text": profile_text,
            "target_job": target_job
        })
        print("-> êµ¬ì¡°í™”ëœ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"-> í”„ë¡œí•„ ìƒì„± ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}\n-> ëŒ€ì²´ í”„ë¡œí•„ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        structured_profile = {"narrative_summary": "ì‚¬ìš©ì ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

    # ì‚¬ìš©ìì˜ í•µì‹¬ ì§ˆë¬¸ì„ ë³„ë„ë¡œ ì¶”ì¶œ
    user_questions = user_info.get("ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ", "")
    print(f"-> ì¶”ì¶œëœ ì‚¬ìš©ì ì§ˆë¬¸: '{user_questions}'")

    original_target_job = user_info.get("ëª©í‘œ ì§ë¬´", "")
    refined_jobs = [original_target_job] if original_target_job else [] # ê¸°ë³¸ê°’ì€ ì›ë³¸ê°’ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸

    # LLMì„ ì´ìš©í•œ ì§ë¬´ëª… êµ¬ì²´í™” ë¡œì§ ---
    if original_target_job:
        print(f"-> ëª©í‘œ ì§ë¬´ êµ¬ì²´í™” ì‹œì‘ (ì…ë ¥: '{original_target_job}')")

        # 1. ì§ë¬´ëª… êµ¬ì²´í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ ì •ì˜
        job_refinement_prompt = USER_PROFILING_PROMPT["job_refinement"]

        try:
            # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
            parser = JsonOutputParser()
            prompt = ChatPromptTemplate.from_template(job_refinement_prompt)
            refinement_chain = prompt | llm | parser

            refined_jobs = refinement_chain.invoke({"original_job": original_target_job})

            print(f"-> êµ¬ì²´í™”ëœ ì§ë¬´ ëª©ë¡: {refined_jobs}")

        except Exception as e:
            # LLM íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬
            print(f"-> ì§ë¬´ëª… êµ¬ì²´í™” ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ì›ë³¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            refined_jobs = [original_target_job]
    else:
        # ëª©í‘œ ì§ë¬´ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        print("-> ëª©í‘œ ì§ë¬´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        refined_jobs = []

    original_target_companies = user_info.get("í¬ë§ ê¸°ì—…", [])
    refined_companies = []
    # LLMì„ ì´ìš©í•œ ê¸°ì—…ëª… ì •ì œ ë¡œì§ ---
    if original_target_companies:
        print(f"-> í¬ë§ ê¸°ì—… ëª©ë¡ ì •ì œ ì‹œì‘ (ì…ë ¥: {original_target_companies})")

        # 1. í•¨ìˆ˜ ë‚´ì—ì„œ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ì„ ì§ì ‘ ì •ì˜
        company_refinement_prompt = USER_PROFILING_PROMPT["company_refinement"]

        try:
            # 2. LLMì´ Pydantic ëª¨ë¸ì— ë§ì¶° ì¶œë ¥í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
            structured_llm = llm.with_structured_output(RefinedCompanies)
            prompt = ChatPromptTemplate.from_template(company_refinement_prompt)
            refinement_chain = prompt | structured_llm

            company_list_str = ", ".join(original_target_companies)

            # 3. ì²´ì¸ì„ í˜¸ì¶œí•˜ë©´ Pydantic ëª¨ë¸ ê°ì²´ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.
            result_model = refinement_chain.invoke({"company_list_str": company_list_str})
            refined_companies = result_model.companies # .companies ì†ì„±ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì ‘ê·¼

            print(f"-> ì •ì œëœ ê¸°ì—… ëª©ë¡: {refined_companies}")

        except Exception as e:
            print(f"-> ê¸°ì—…ëª… ì •ì œ ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ì›ë³¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            refined_companies = original_target_companies
    else:
        # 4. í¬ë§ ê¸°ì—…ì„ ì…ë ¥í•˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        print("-> í¬ë§ ê¸°ì—…ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return {
        "user_profile_structured": structured_profile,
        "target_job": refined_jobs,
        "target_company": refined_companies,
        "user_questions": user_questions
    }

class PostingAnalysisOutput(BaseModel):
    """ì±„ìš© ê³µê³  ë¶„ì„ ê²°ê³¼ ëª¨ë¸"""
    role_goal: str = Field(description="ì´ ì§ë¬´ê°€ ë‹¬ì„±í•´ì•¼ í•  ì •ëŸ‰ì /ì •ì„±ì  ëª©í‘œ")
    key_responsibilities: List[str] = Field(description="ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì£¼ìš” ì±…ì„ ë¦¬ìŠ¤íŠ¸")
    hard_skills: List[str] = Field(description="í•„ìˆ˜ì ì¸ íˆ´ê³¼ ê¸°ìˆ ëª… ë¦¬ìŠ¤íŠ¸")
    collaboration_process: str = Field(description="ì–´ë–¤ ë™ë£Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í˜‘ì—…í•˜ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”„ë¡œì„¸ìŠ¤")
    preferred_experiences: List[str] = Field(description="ì„ í˜¸í•˜ëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ ë¦¬ìŠ¤íŠ¸")

class ReviewAnalysisOutput(BaseModel):
    """
    ëª¨ë“  ì§êµ°ì˜ 'í•©ê²© í›„ê¸°'ì—ì„œ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì¶”ì¶œ ëª¨ë¸.
    'ì–´ë–»ê²Œ í•©ê²©í–ˆëŠ”ê°€?'ë¼ëŠ” ì±„ìš© í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ë³„ í•µì‹¬ ì „ëµì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.
    """
    document_strategy: List[str] = Field(
        description="ì„œë¥˜ ì „í˜•(ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ) í†µê³¼ë¥¼ ìœ„í•´ í•©ê²©ìê°€ ê°•ì¡°í–ˆë˜ í•µì‹¬ ê²½í—˜, ì—­ëŸ‰, ë˜ëŠ” ì‘ì„± ì „ëµ"
    )
    test_strategy: List[str] = Field(
        description="ì¸ì ì„±, NCS, ë…¼ìˆ , ì½”ë”© í…ŒìŠ¤íŠ¸, ê³¼ì œ ë“± í•„ê¸°/ê³¼ì œ ì „í˜•ì˜ ì¢…ë¥˜ì™€ êµ¬ì²´ì ì¸ ì¤€ë¹„ ë°©ë²• ë˜ëŠ” íŒ"
    )
    job_interview_strategy: List[str] = Field(
        description="1ì°¨ ë©´ì ‘(ì‹¤ë¬´ì§„ ë©´ì ‘)ì—ì„œ ì§ë¬´ ì—­ëŸ‰ì„ ì–´í•„í•˜ê¸° ìœ„í•´ í•©ê²©ìê°€ ì‚¬ìš©í•œ ì „ëµì´ë‚˜ ë°›ì•˜ë˜ í•µì‹¬ ì§ˆë¬¸"
    )
    final_interview_strategy: List[str] = Field(
        description="ìµœì¢… ë©´ì ‘(ì„ì› ë©´ì ‘)ì—ì„œ ì¸ì„±ì´ë‚˜ ì¡°ì§ ì í•©ì„±ì„ ì–´í•„í•˜ê¸° ìœ„í•´ í•©ê²©ìê°€ ì‚¬ìš©í•œ ì „ëµì´ë‚˜ ë°›ì•˜ë˜ í•µì‹¬ ì§ˆë¬¸"
    )
    critical_success_factor: str = Field(
        description="í•©ê²©ìê°€ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ëŠ”, í•©ê²©ì— ê°€ì¥ ê²°ì •ì ì´ì—ˆë˜ ìì‹ ë§Œì˜ ì°¨ë³„í™” í¬ì¸íŠ¸ (ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ í•œ ê°€ì§€)"
    )

class InterviewAnalysisOutput(BaseModel):
    """
    ëª¨ë“  ì§êµ°ì˜ í˜„ì§ì ì¸í„°ë·°ì—ì„œ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì¶”ì¶œ ëª¨ë¸
    """
    core_competencies_and_tools: List[str] = Field(
        description="í˜„ì§ìê°€ ê°•ì¡°í•˜ëŠ” í•´ë‹¹ ì§ë¬´ì˜ í•µì‹¬ ì—­ëŸ‰ê³¼, ì—…ë¬´ì— ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” í•„ìˆ˜ íˆ´(ì˜ˆ: MS Office, Slack, Jira, Salesforce, Figma, Adobe Photoshop) ë“±"
    )
    team_culture_and_workflow: str = Field(
        description="íŒ€ì˜ ì˜ì‚¬ì†Œí†µ ë°©ì‹, íšŒì˜ ë¬¸í™”, í˜‘ì—… í”„ë¡œì„¸ìŠ¤, ì„±ê³¼ í‰ê°€ ë°©ì‹ ë“± êµ¬ì²´ì ì¸ ì—…ë¬´ ìŠ¤íƒ€ì¼ê³¼ ì¡°ì§ ë¬¸í™”"
    )
    growth_and_career_path: str = Field(
        description="íšŒì‚¬ì—ì„œ ì œê³µí•˜ëŠ” êµìœ¡, ë©˜í† ë§ ì œë„ë‚˜ í˜„ì§ìê°€ ë§í•˜ëŠ” í˜„ì‹¤ì ì¸ ì»¤ë¦¬ì–´ ì„±ì¥ ê²½ë¡œ"
    )
    advice_for_applicants: List[str] = Field(
        description="í˜„ì§ìê°€ í•´ë‹¹ ì§ë¬´ ì§€ì›ìì—ê²Œ 'ì´ê²ƒë§Œì€ ê¼­ ì¤€ë¹„í•˜ë¼'ê³  ì¡°ì–¸í•˜ëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ì´ë‚˜ ì—­ëŸ‰"
    )

class GlobalSearchKeywords(BaseModel):
    """
    ê¸€ë¡œë²Œ íŠ¸ë Œë“œë¥¼ ë‹¤ê°ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í‚¤ì›Œë“œ ëª¨ë¸
    """
    core_technologies: List[str] = Field(
        description="ë¶„ì„ ê²°ê³¼ì˜ í•µì‹¬ì´ ë˜ëŠ” ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œ (ì˜ˆ: Deep Learning, Computer Vision)"
    )
    business_domains: List[str] = Field(
        description="ì–¸ê¸‰ëœ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ë° ì‚°ì—… ë„ë©”ì¸ í‚¤ì›Œë“œ (ì˜ˆ: Fintech, Robotics)"
    )
    emerging_roles: List[str] = Field(
        description="ìƒˆë¡­ê²Œ ë¶€ìƒí•˜ê±°ë‚˜ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§€ëŠ” ì§ë¬´ëª… (ì˜ˆ: Prompt Engineer, AI Ethicist, MLOps Specialist)"
    )
    problem_solution_keywords: List[str] = Field(
        description="ê¸°ìˆ ì´ í•´ê²°í•˜ë ¤ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œë‚˜ ì†”ë£¨ì…˜ í‚¤ì›Œë“œ (ì˜ˆ: Fraud Detection, Hyper-personalization, Digital Twin, ESG)"
    )

def domestic_job_analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    ì •ì œëœ ì§ë¬´ì™€ ê¸°ì—…ëª…ì„ ë°”íƒ•ìœ¼ë¡œ êµ­ë‚´ ì±„ìš© ì‹œì¥ì˜ ê° ì¸¡ë©´ì„ ë¶„ì„í•˜ê³  ì¢…í•©í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 2] êµ­ë‚´ ì±„ìš© ì‹œì¥ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    target_jobs_list = state["target_job"]
    refined_companies = state["target_company"]
    target_job_title = ", ".join(target_jobs_list)
    job_query = f'"{" OR ".join(target_jobs_list)}"' if target_jobs_list else ""
    company_query = f'"{" OR ".join(refined_companies)}"' if refined_companies else ""
    api_keys = state['api_keys']

    # --- 2. ì±„ìš© ê³µê³  ë¶„ì„ ---
    print("\n1. ì±„ìš© ê³µê³  ë¶„ì„ ì¤‘...")
    postings_web_results = tavily_web_search.invoke(
        " ".join(part for part in [company_query, job_query, "ì‹ ì… ì±„ìš© ê³µê³  ìê²©ìš”ê±´ ìš°ëŒ€ì‚¬í•­"] if part)
    )

    postings_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['posting_analysis']
    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": postings_web_results
    }

    postings_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=postings_prompt,
    pydantic_model=PostingAnalysisOutput,
    llm=llm,
    log_message="-> Helper: ì±„ìš© ê³µê³  ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )


    # --- 3. í•©ê²© í›„ê¸° ë¶„ì„ ---
    print("\n2. í•©ê²© í›„ê¸° ë¶„ì„ ì¤‘...")
    current_year = datetime.now().year
    years_query = " OR ".join(str(y) for y in range(current_year, current_year - 3, -1))

    reviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} ì‹ ì… í•©ê²© OR ë©´ì ‘ í›„ê¸° ({years_query}) site:velog.io OR site:tistory.com OR site:brunch.co.kr'
    )

    youtube_review_topic_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['youtube_review_topic']
    youtube_review_topic_chain = ChatPromptTemplate.from_template(youtube_review_topic_prompt) | llm | StrOutputParser()

    youtube_review_topic = youtube_review_topic_chain.invoke({
        "companies": refined_companies,
        "jobs": target_jobs_list
    })
    youtube_review_summary_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['youtube_review_summary']

    youtube_summary = analyze_youtube_topic(
        topic=youtube_review_topic,
        analysis_prompt=youtube_review_summary_prompt,
        num_to_analyze=2,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )


    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì™€ ìœ íŠœë¸Œ ë¶„ì„ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ë¶„ì„
    # --- 4b. ìˆ˜ì§‘ëœ ë°ì´í„° ì¢…í•© ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    combined_reviews  = f"--- ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ìµœê·¼ 3ë…„) ---\n{reviews_web_results}\n\n--- ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ (ìµœê·¼ 3ë…„) ---\n{youtube_summary}"

    total_review_summary_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['web_youtube_review_total_summary']
    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_reviews
    }

    reviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=total_review_summary_prompt,
    pydantic_model=ReviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: í•©ê²© í›„ê¸° ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    # --- 4. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ---
    print("\n3. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ì¤‘...")
    interviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} "í˜„ì§ì ì¸í„°ë·°" OR "ì§ì¥ì¸ ë¸Œì´ë¡œê·¸" OR "ì¼í•˜ëŠ” ë°©ì‹" OR "íŒ€ ë¬¸í™”" OR "ì»¤ë¦¬ì–´" site:tistory.com OR site:brunch.co.kr'
    )

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ìœ íŠœë¸Œ ê²€ìƒ‰ ì£¼ì œì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    youtube_interview_topic_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['youtube_interview_topic']
    topic_generation_chain = ChatPromptTemplate.from_template(youtube_interview_topic_prompt) | llm | StrOutputParser()

    interview_topic = topic_generation_chain.invoke({
        "companies": refined_companies, # ì •ì œëœ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸
        "jobs": target_jobs_list      # ì •ì œëœ ì§ë¬´ëª… ë¦¬ìŠ¤íŠ¸
    })
    interview_prompt = f"'{target_job_title}' ì§ë¬´ í˜„ì§ìë¡œì„œ ì¼í•˜ëŠ” ë°©ì‹, ì¡°ì§ ë¬¸í™”, ì‹ ì…ì—ê²Œ í•„ìš”í•œ ì—­ëŸ‰ì— ëŒ€í•´ ë§í•˜ëŠ” ë¶€ë¶„ì„ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜."
    youtube_interview_summary = analyze_youtube_topic(
        topic=interview_topic,
        analysis_prompt=interview_prompt,
        num_to_analyze=1,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )

    # --- 5b. ìˆ˜ì§‘ëœ ë°ì´í„° ì¢…í•© ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    print("-> í…ìŠ¤íŠ¸ì™€ ìœ íŠœë¸Œ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    combined_interviews = f"""
    --- Text-based Interview Search Results ---
    {interviews_web_results}

    --- YouTube Interview Analysis Summary ---
    {youtube_interview_summary}
    """

    youtube_interview_summary_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['youtube_interview_summary']
    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_interviews
    }

    interviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=youtube_interview_summary_prompt,
    pydantic_model=InterviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    print("-> ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

    # --- 6. ëª¨ë“  ë¶„ì„ ê²°ê³¼ ì¢…í•© ë° í‚¤ì›Œë“œ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ) ---
    combined_analysis_dict = {
        "postings_analysis": postings_analysis,
        "reviews_analysis": reviews_analysis,
        # "interviews_analysis": interviews_analysis
    }

    combined_analysis_str = json.dumps(combined_analysis_dict, ensure_ascii=False, indent=2)
    prompt_variable_inputs = {
    "market_analysis_json": combined_analysis_str
    }
    domestic_keyword_extract_prompt = DOMESTIC_JOB_ANALYSIS_PROMPT['domestic_keyword_extract']

    # [ìˆ˜ì •] ë²”ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Global Trend ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
    global_search_keywords = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=domestic_keyword_extract_prompt,
    pydantic_model=GlobalSearchKeywords,
    llm=llm,
    log_message="-> Helper: Global Trend ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰..."
    )

    # --- 6. ìµœì¢… ë°˜í™˜ ---
    print("--- êµ­ë‚´ ì±„ìš© ì‹œì¥ ë¶„ì„ ì™„ë£Œ ---")
    return {
        "domestic_analysis_components": {
            "postings_analysis": postings_analysis,
            "reviews_analysis": reviews_analysis,
            "interviews_analysis": interviews_analysis
        },
        "domestic_keywords": global_search_keywords
    }

class GlobalTrendsOutput(BaseModel):
    """
    ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ê¸° ìœ„í•œ ëª¨ë¸
    """
    future_outlook: str = Field(
        description="ì´ ì§ë¬´ì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±ì— ëŒ€í•œ ë¯¸ë˜ ì „ë§ ì¢…í•© ë¶„ì„ (1-2 ë¬¸ì¥ ìš”ì•½)"
    )
    key_technology_shifts: List[str] = Field(
        description="í˜„ì¬ ë– ì˜¤ë¥´ê±°ë‚˜, ì•ìœ¼ë¡œ í•„ìˆ˜ê°€ ë  êµ¬ì²´ì ì¸ ìµœì‹  ê¸°ìˆ , íˆ´, í”Œë«í¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"
    )
    changing_market_demands: List[str] = Field(
        description="ê¸°ì—…ë“¤ì´ ì´ ì§ë¬´ì— ëŒ€í•´ ìƒˆë¡­ê²Œ ìš”êµ¬í•˜ê¸° ì‹œì‘í•œ êµ¬ì²´ì ì¸ ì—­ëŸ‰ì´ë‚˜ ê²½í—˜ ë¦¬ìŠ¤íŠ¸"
    )
    key_messages_from_leaders: str = Field(description="ì—…ê³„ ë¦¬ë”ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê°•ì¡°í•˜ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ë©”ì‹œì§€ë‚˜ ì¸ìš©êµ¬ (1-2ê°œ)")

def global_trend_analysis_node(state: AgentState) -> dict:
    """
    êµ­ë‚´ ë¶„ì„ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ì•¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    ê¸°ìˆ /ì‹œì¥/ë¦¬ë”ì‹­ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ê¸€ë¡œë²Œ ë™í–¥ì„ ë„ì¶œí•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 3] ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    domestic_keywords = state["domestic_keywords"]
    target_job_title = ", ".join(state["target_job"])
    target_companies = state["target_company"]
    api_keys = state["api_keys"]
    one_year_ago_str = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

    # ----------------------------------------------------------------------------------

    # --- 2. ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ , ì‹œì¥, ë¦¬ë”ì‹­) ---

    # 2a. ê¸°ìˆ  íŠ¸ë Œë“œ (Tavily)
    print("\n1. ê¸°ìˆ  íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    tech_trends_results = ""
    web_search_query_prompt = GLOBAL_TREND_ANALYSIS_PROMPT['web_search_query']
    try:
        search_query_chain = ChatPromptTemplate.from_template(web_search_query_prompt) | llm | JsonOutputParser()
        search_queries = search_query_chain.invoke({
            "keywords_str": json.dumps(domestic_keywords, ensure_ascii=False),
            "job_title": target_job_title
        })

        # [ìˆ˜ì •] ê° ê²€ìƒ‰ì–´ì— 'after:'ë¥¼ ì¶”ê°€í•˜ì—¬ 1ë…„ ì´ë‚´ë¡œ ì‹œê°„ ì œí•œ
        timed_search_queries = [f"{q} after:{one_year_ago_str}" for q in search_queries]
        print(f"-> ìƒì„±ëœ ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ì–´: {timed_search_queries}")

        tech_trends_results_list = tavily_web_search.batch(timed_search_queries)
        tech_trends_results = "\n\n---\n\n".join([str(result) for result in tech_trends_results_list])
    except Exception as e:
        print(f"-> ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ({e}).")
        tech_trends_results = "ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    # ----------------------------------------------------------------------------------

    # 2b. ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ (News API)
    print("\n2. ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    market_trends_results = ""
    if target_companies:
        try:
            # ì‚°ì—… ë¶„ì•¼ ì¶”ë¡  (ê¸°ì¡´ê³¼ ë™ì¼)
            main_company = target_companies[0]
            industry_name_prompt = f"The company '{main_company}' primarily operates in which industry? Provide a concise, common English industry name (e.g., 'E-commerce', 'Semiconductor', 'Financial Services')."
            industry_name = llm.invoke(industry_name_prompt).content.strip()
            print(f"-> '{main_company}'ì˜ í•µì‹¬ ì‚°ì—… ë¶„ì•¼ ì¶”ë¡ : {industry_name}")

            # í•œê¸€ ì§ë¬´ëª…ì„ ì˜ì–´ë¡œ ë²ˆì—­ (ê¸°ì¡´ê³¼ ë™ì¼)
            korean_job_title = ", ".join(state["target_job"])
            translation_prompt = f"Translate the following Korean job titles into a concise, comma-separated English string: '{korean_job_title}'"
            english_job_titles_str = llm.invoke(translation_prompt).content.strip()
            print(f"-> ì§ë¬´ëª… ì˜ë¬¸ ë²ˆì—­ ì™„ë£Œ: {english_job_titles_str}")

            # 1. ê° ì§ë¬´ëª…ì˜ ì–‘ìª½ì— í°ë”°ì˜´í‘œ(")ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            # -> ["Machine Learning Engineer", "Data Scientist"]
            job_titles_list = [job.strip() for job in english_job_titles_str.split(',')]
            quoted_job_titles = [f'"{job}"' for job in job_titles_list]

            # 2. í°ë”°ì˜´í‘œë¡œ ë¬¶ì¸ ì§ë¬´ëª…ë“¤ì„ " OR "ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
            # -> "Machine Learning Engineer" OR "Data Scientist"
            or_separated_jobs = " OR ".join(quoted_job_titles)

            # 3. ìµœì¢…ì ìœ¼ë¡œ ì–‘ìª½ì„ ê´„í˜¸()ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
            # -> ("Machine Learning Engineer" OR "Data Scientist")
            job_titles_query_part = f'({or_separated_jobs})'

            news_query = f'"{industry_name}" AND {job_titles_query_part} AND (hiring OR skill OR future OR trend)'
            print(f"-> News API ê²€ìƒ‰ì–´: {news_query}")

            market_trends_results = search_global_news.invoke({
                "query": news_query,
                "from_date": one_year_ago_str
            })
        except Exception as e:
            print(f"-> ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ({e}).")
            market_trends_results = "ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    else:
        print("-> í¬ë§ ê¸°ì—…ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # ----------------------------------------------------------------------------------

    # 2c. ë¦¬ë”ì‹­ íŠ¸ë Œë“œ (YouTube)
    print("\n3. ë¦¬ë”ì‹­(ê¶Œìœ„ì ë¹„ì „) íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    vision_analysis_summary = ""

    if target_companies:
        try :
            main_company = target_companies[0]
            youtube_conference_keyword_extractor_prompt = GLOBAL_TREND_ANALYSIS_PROMPT['youtube_conference_keyword_extractor']
            key_figures_chain = ChatPromptTemplate.from_template(youtube_conference_keyword_extractor_prompt) | llm | JsonOutputParser()
            key_figures_list = key_figures_chain.invoke({
            "main_company": main_company,
            "target_job_title": target_job_title
            })

            analysis_results = []
            for figure_name in key_figures_list:
                # ê° ì¸ë¬¼ì— ëŒ€í•œ ê²€ìƒ‰ ì£¼ì œì™€ ë¶„ì„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
                vision_topic = f'"{figure_name}" conference OR interview'
                vision_prompt = f"'{figure_name}'ì˜ ê°•ì—° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, '{target_job_title}' ì§ë¬´ì™€ ê´€ë ¨ëœ ë¯¸ë˜ ë¹„ì „, ê¸°ìˆ  ì² í•™, ê·¸ë¦¬ê³  ì—…ê³„ì— ë˜ì§€ëŠ” í•µì‹¬ ë©”ì‹œì§€ë¥¼ ìš”ì•½í•´ì¤˜."

                # ê° ì¸ë¬¼ë³„ë¡œ ì˜ìƒ 1ê°œë§Œ ë¶„ì„í•˜ì—¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                summary = analyze_youtube_topic(
                    topic=vision_topic,
                    analysis_prompt=vision_prompt,
                    api_key=api_keys.youtube_api_key,
                    lang_code='en',
                    max_results=20,  # ì¸ë¬¼ë‹¹ 3ê°œ í›„ë³´ ê²€ìƒ‰
                    num_to_analyze=1, # ê·¸ ì¤‘ 1ê°œë§Œ ë¶„ì„
                    transcripts_only=True
                )
                analysis_results.append(summary)

            # ê°œë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ë¡œ í•©ì¹¨
            vision_analysis_summary = "\n\n---\n\n".join(analysis_results)

        except Exception as e:
            print(f"-> ê¶Œìœ„ì ë¹„ì „ ë¶„ì„ ì‹¤íŒ¨ ({e}).")
            vision_analysis_summary = "ê¶Œìœ„ì ë¹„ì „ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    else:
        vision_analysis_summary = "í¬ë§ ê¸°ì—…ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ë¦¬ë”ì‹­ íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."

    # --- 3. ëª¨ë“  ì •ë³´ ì¢…í•© ë° ìµœì¢… ë¶„ì„ ---
    print("\n4. ëª¨ë“  ì •ë³´ ì¢…í•© ë° ìµœì¢… íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
    combined_results = f"""
    --- Technical Trends (from Keyword Search) ---
    {tech_trends_results}

    --- Market & Industry Trends (from News Search) ---
    {market_trends_results}

    --- Vision from Industry Leader (from YouTube) ---
    {vision_analysis_summary}
    """

    total_golbal_trend_summary_prompt = GLOBAL_TREND_ANALYSIS_PROMPT['total_golbal_trend_summary']

    # [ìˆ˜ì •] ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ì™€ Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ extract_structured_data_flexible í˜¸ì¶œ
    prompt_variable_inputs = {
        "target_job_title": target_job_title,
        "search_results": combined_results
    }

    global_trends_analysis = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=total_golbal_trend_summary_prompt,
        pydantic_model=GlobalTrendsOutput,
        llm=llm,
        log_message="-> Helper: ìµœì¢… ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    return {
        "global_trends": global_trends_analysis
    }

class GapAnalysisOutput(BaseModel):
    """
    ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ê°•ì , ì•½ì , ê¸°íšŒë¥¼ ì§„ë‹¨í•˜ëŠ” ëª¨ë¸
    """
    strengths: List[str] = Field(
        description="ì‚¬ìš©ìê°€ í˜„ì¬ ë³´ìœ í•œ ì—­ëŸ‰/ê²½í—˜ ì¤‘, ì‹œì¥ì˜ ìš”êµ¬ì‚¬í•­ ë° íŠ¸ë Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ëª…í™•í•œ ê°•ì  ë¦¬ìŠ¤íŠ¸"
    )
    weaknesses: List[str] = Field(
        description="ì‹œì¥ì˜ ìš”êµ¬ì‚¬í•­ ë° íŠ¸ë Œë“œì— ë¹„í•´ ì‚¬ìš©ìê°€ ëª…ë°±íˆ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ëŠ” ì—­ëŸ‰/ê²½í—˜ ë¦¬ìŠ¤íŠ¸ (ë³´ì™„ì )"
    )
    opportunities: List[str] = Field(
        description="ì‚¬ìš©ìì˜ ê°•ì ì„ ê¸€ë¡œë²Œ íŠ¸ë Œë“œì™€ ì—°ê²°í•˜ì—¬, ê²½ìŸë ¥ì„ í•œ ë‹¨ê³„ ë” ë†’ì¼ ìˆ˜ ìˆëŠ” ê¸°íšŒ ì˜ì—­ ë¦¬ìŠ¤íŠ¸"
    )
    summary: str = Field(
        description="ì‚¬ìš©ìì˜ í˜„ì¬ ìƒíƒœì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ë° ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•œ ë°©í–¥ì„± ìš”ì•½ (2-3 ë¬¸ì¥)"
    )

def gap_analysis_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ì í”„ë¡œí•„ê³¼ êµ­ë‚´ì™¸ ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
    ì‚¬ìš©ìì˜ ê°•ì , ì•½ì , ê¸°íšŒë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì‚¬ìš©ì í”„ë¡œí•„ ì²˜ë¦¬ ë¡œì§ ê°•í™”)
    """

    print("\n--- [Step 4] ì‚¬ìš©ì í”„ë¡œí•„ ë° ì‹œì¥ ìš”êµ¬ì‚¬í•­ ê°­ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")
    print(state)
    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    user_profile = state["user_profile_structured"]
    domestic_analysis = state["domestic_analysis_components"]
    global_trends = state["global_trends"]
    llm = state["llm"]

    user_profile_str = json.dumps(user_profile, ensure_ascii=False, indent=2)
    market_analysis_str = json.dumps({
        "domestic_analysis": domestic_analysis,
        "global_trends": global_trends
    }, ensure_ascii=False, indent=2)

    # --- 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    gap_analysis_prompt = GAP_ANALYSIS_PROMPT['gap_analysis']
    prompt_variable_inputs = {
        "user_profile_str": user_profile_str,
        "market_analysis_str": market_analysis_str
    }

    gap_analysis_result = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=gap_analysis_prompt,
        pydantic_model=GapAnalysisOutput,
        llm=llm,
        log_message="-> Helper: ì‚¬ìš©ì ê°•ì , ì•½ì , ê¸°íšŒ ë¶„ì„ ì‹¤í–‰..."
    )

    return {
        "gap_analysis": gap_analysis_result
    }

class StructuredReasoning(BaseModel):
    """
    ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  êµ¬ì¡°í™”ëœ íŒë‹¨ ê·¼ê±° ëª¨ë¸
    """
    based_on_strengths: List[str] = Field(description="íŒë‹¨ì˜ ê·¼ê±°ê°€ ëœ ì‚¬ìš©ìì˜ í•µì‹¬ ê°•ì ")
    based_on_weaknesses: List[str] = Field(description="íŒë‹¨ì˜ ê·¼ê±°ê°€ ëœ ì‚¬ìš©ìì˜ í•µì‹¬ ì•½ì ")
    based_on_questions: str = Field(description="íŒë‹¨ì˜ ê·¼ê±°ê°€ ëœ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìš”ì•½")

class RouterOutput(BaseModel):
    """
    ë‹¤ìŒ í–‰ë™ê³¼ ê·¸ì— ëŒ€í•œ ì´ìœ (ìì—°ì–´/êµ¬ì¡°í™”)ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë¸
    """
    next_action: Literal["recommend_learning", "recommend_storytelling"] = Field(
        description="ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œì˜ ì´ë¦„. ë°˜ë“œì‹œ 'recommend_learning' ë˜ëŠ” 'recommend_storytelling' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•¨."
    )
    reasoning_narrative: str = Field(
        description="ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„, ì¹œì ˆí•˜ê³  ëª…í™•í•œ íŒë‹¨ ì´ìœ  (ìì—°ì–´, 2-3 ë¬¸ì¥)."
    )
    reasoning_structured: StructuredReasoning = Field(
        description="ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í• , íŒë‹¨ì˜ í•µì‹¬ ê·¼ê±°ê°€ ë˜ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°."
    )

def llm_router_node(state: AgentState) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ê³ ,
    ê·¸ ì´ìœ ë¥¼ 'ìì—°ì–´'ì™€ 'êµ¬ì¡°í™”ëœ ë°ì´í„°' ë‘ ê°€ì§€ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 5] LLM ê¸°ë°˜ ë‹¤ìŒ í–‰ë™ ê²°ì • (Router) ---")

    # 1. stateì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    user_questions = state.get("user_questions", "")
    gap_analysis = state.get("gap_analysis", {})
    llm = state.get("llm")

    # 2. LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    routing_context_str = json.dumps({
        "user_questions": user_questions,
        "gap_analysis": gap_analysis
    }, ensure_ascii=False, indent=2)

    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ
    routing_prompt = LLM_ROUTER_PROMPT['routing']
    prompt_variable_inputs = {
        "routing_context": routing_context_str
    }

    router_result = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=routing_prompt,
        pydantic_model=RouterOutput,
        llm=llm,
        log_message="-> Helper: ë‹¤ìŒ í–‰ë™ ë° ì´ìœ  ë¶„ì„ ì¤‘..."
    )

    # 4. ë‘ ê°€ì§€ í˜•íƒœì˜ ì´ìœ ë¥¼ ëª¨ë‘ stateì— ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ë°˜í™˜
    next_action = router_result.get("next_action", "recommend_storytelling")
    reasoning_narrative = router_result.get("reasoning_narrative", "íŒë‹¨ ì´ìœ (ìì—°ì–´) ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    reasoning_structured = router_result.get("reasoning_structured", {})

    print(f"-> [LLM ë¼ìš°í„°] ê²°ì •: {next_action}")
    print(f"-> [LLM ë¼ìš°í„°] ì´ìœ  (ì‚¬ìš©ììš©): {reasoning_narrative}")
    print(f"-> [LLM ë¼ìš°í„°] ì´ìœ  (ì‹œìŠ¤í…œìš©): {reasoning_structured}")

    return {
        "next_action": next_action,
        "routing_reason_narrative": reasoning_narrative,
        "routing_reason_structured": reasoning_structured
    }

class Resource(BaseModel):
    """
    ê°œë³„ í•™ìŠµ ìë£Œ(ì•„í‹°í´, ë…¼ë¬¸ ë“±)ë¥¼ ìœ„í•œ ëª¨ë¸
    """
    title: str = Field(description="ìë£Œì˜ ì œëª©")
    url: str = Field(description="ìë£Œë¡œ ë°”ë¡œ ì—°ê²°ë˜ëŠ” URL")

class RecommendedTopic(BaseModel):
    """
    í•˜ë‚˜ì˜ ì™„ì„±ëœ í•™ìŠµ ì£¼ì œ ì¶”ì²œ ì„¸íŠ¸ë¥¼ ìœ„í•œ ëª¨ë¸
    """
    topic_name: str = Field(description="ì¶”ì²œí•˜ëŠ” í•™ìŠµ ì£¼ì œì— ëŒ€í•œ ë™ê¸°ë¶€ì—¬ê°€ ë˜ëŠ” ì´ë¦„ (ì˜ˆ: MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì²«ê±¸ìŒ)")
    relevance_summary: str = Field(description="ì™œ ì§€ê¸ˆ ì´ ì£¼ì œë¥¼ ê³µë¶€í•´ì•¼ í•˜ëŠ”ì§€, ì‚¬ìš©ìì˜ ì•½ì ê³¼ ì—°ê²°í•˜ì—¬ ì„¤ëª…í•˜ëŠ” ìš”ì•½ (1-2 ë¬¸ì¥)")
    foundational_resources: List[Resource] = Field(description="Tavily ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì°¾ì€, ê°œë… í•™ìŠµ ë° ì‹¤ìŠµì— ì¢‹ì€ ê¸°ì´ˆ/ì‹¤ìš© ìë£Œ ë¦¬ìŠ¤íŠ¸ (ë¸”ë¡œê·¸, íŠœí† ë¦¬ì–¼ ë“±)")
    deep_dive_topics: List[Resource] = Field(description="arXiv ê²€ìƒ‰ì„ í†µí•´ ì°¾ì€, ë” ê¹Šì€ í•™ìˆ ì  íƒêµ¬ë¥¼ ìœ„í•œ ì‹¬í™” ì£¼ì œ ë¦¬ìŠ¤íŠ¸ (ìµœì‹  ë…¼ë¬¸ ë“±)")

class LearningRecommendationOutput(BaseModel):
    """
    ìµœì¢… í•™ìŠµ ì£¼ì œ ì¶”ì²œ ê²°ê³¼ ëª¨ë¸
    """
    recommendations: List[RecommendedTopic] = Field(description="ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•˜ëŠ” ë§ì¶¤í˜• í•™ìŠµ ì£¼ì œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 2-3ê°œ)")

def recommend_learning_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìì˜ ì•½ì ì„ ê¸°ë°˜ìœ¼ë¡œ Tavilyì™€ arXivì—ì„œ ìë£Œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    êµ¬ì²´ì ì¸ í•™ìŠµ ì£¼ì œì™€ ë¡œë“œë§µì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 6-A] í•™ìŠµ ì£¼ì œ ì¶”ì²œ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    gap_analysis = state.get("gap_analysis", {})
    routing_reason = state.get("routing_reason_structured", {})
    weaknesses = gap_analysis.get("weaknesses", [])
    strengths = gap_analysis.get("strengths", [])
    llm = state.get("llm")

    if not weaknesses:
        print("-> ë¶„ì„ëœ ì•½ì ì´ ì—†ì–´ í•™ìŠµ ì£¼ì œ ì¶”ì²œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {"learning_recommendations": {"recommendations": []}}

    # --- 2. LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± ---
    study_keyword_generator_prompt = RECOMMEND_LEARNING['study_keyword_generator']
    search_keywords = (llm | JsonOutputParser()).invoke(study_keyword_generator_prompt)
    print(f"-> ìƒì„±ëœ í•™ìŠµ ìë£Œ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")

    # --- 3. Tavilyì™€ arXivì—ì„œ ìë£Œ ê²€ìƒ‰ ---
    all_resources_text = ""
    try:
        print("-> Tavily ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (ê¸°ì´ˆ/ì‹¤ìš© ìë£Œ ìˆ˜ì§‘)...")
        tavily_results = tavily_tool.batch(search_keywords)
        all_resources_text += f"--- Web Search Tutorials & Articles ---\n{tavily_results}\n\n"

        print("-> arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰ (ì‹¬í™” ìë£Œ ìˆ˜ì§‘)...")
        arxiv_query = next((kw for kw in search_keywords if "paper" in kw or "research" in kw), search_keywords[-1])
        arxiv_results = arxiv_tool.run(arxiv_query)
        all_resources_text += f"--- arXiv Research Papers ---\n{arxiv_results}"
    except Exception as e:
        print(f"-> ìë£Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- 4. LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì¶”ì²œ ìƒì„± ---
    study_recommend_summary_prompt = RECOMMEND_LEARNING['study_recommend_summary']

    prompt_variable_inputs = {
        "strengths_str": str(strengths),
        "weaknesses_str": str(weaknesses),
        "reasoning_str": str(routing_reason),
        "resources_text": all_resources_text
    }

    learning_recommendations = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=study_recommend_summary_prompt,
        pydantic_model=LearningRecommendationOutput,
        llm=llm,
        log_message="-> Helper: ìµœì¢… í•™ìŠµ ê³„íš ë° ìë£Œ ì¶”ì²œ ìƒì„± ì¤‘..."
    )

    return {
        "learning_recommendations": learning_recommendations
    }

class StorytellingTip(BaseModel):
    """
    ê°œë³„ ìŠ¤í† ë¦¬í…”ë§ íŒê³¼ ê²½í—˜ ì œì•ˆì„ êµ¬ì¡°í™”í•˜ê¸° ìœ„í•œ ëª¨ë¸
    """
    original_experience: str = Field(description="ì‚¬ìš©ìê°€ ì´ë¯¸ ë³´ìœ í•œ í•µì‹¬ ê²½í—˜ ë˜ëŠ” ê°•ì ")
    suggested_story: str = Field(description="ê¸°ì¡´ ê²½í—˜ì„ ì‹œì¥ì˜ íŠ¸ë Œë“œë‚˜ ìš”êµ¬ì‚¬í•­ê³¼ ì—°ê²°í•˜ì—¬, ë©´ì ‘ì´ë‚˜ ìê¸°ì†Œê°œì„œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ìŠ¤í† ë¦¬í…”ë§ ìŠ¤í¬ë¦½íŠ¸")
    bridging_experience_idea: str = Field(description="ê¸°ì¡´ ê²½í—˜ì„ í•œ ë‹¨ê³„ ë°œì „ì‹œí‚¤ê¸° ìœ„í•´ ë‹¨ê¸°ê°„ì— ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” 'ì—°ê²°ê³ ë¦¬' í† ì´ í”„ë¡œì íŠ¸ ë˜ëŠ” í•™ìŠµ í™œë™ ì•„ì´ë””ì–´")

class StorytellingRecommendationOutput(BaseModel):
    """
    ìµœì¢… ìŠ¤í† ë¦¬í…”ë§ ì¶”ì²œ ê²°ê³¼ ëª¨ë¸
    """
    recommendations: List[StorytellingTip] = Field(description="ì‚¬ìš©ìì˜ ê°•ì ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ë§ì¶¤í˜• ìŠ¤í† ë¦¬í…”ë§ íŒ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 2-3ê°œ)")

def recommend_storytelling_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìì˜ ê°•ì ì„ ì‹œì¥ì˜ ê¸°íšŒì™€ ì—°ê²°í•˜ê³ , 'ì—°ê²°ê³ ë¦¬ ê²½í—˜'ì„ ì œì•ˆí•˜ì—¬
    ë§¤ë ¥ì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ ìŠ¤í† ë¦¬ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 6-B] ìŠ¤í† ë¦¬í…”ë§ ì¶”ì²œ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    gap_analysis = state.get("gap_analysis", {})
    strengths = gap_analysis.get("strengths", [])
    opportunities = gap_analysis.get("opportunities", [])
    user_experience = state.get("user_profile_structured", {}).get("experience_specs", "")
    llm = state.get("llm")
    tools = state.get("tools", {})
    tavily_tool = tools.get("tavily")

    if not strengths:
        print("-> ë¶„ì„ëœ ê°•ì ì´ ì—†ì–´ ìŠ¤í† ë¦¬í…”ë§ ì¶”ì²œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {"storytelling_recommendations": {"recommendations": []}}

    # --- 2. Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  íŠ¸ë Œë“œ ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ---
    # ê¸°íšŒ(Opportunities)ë¥¼ ê²€ìƒ‰ì–´ë¡œ í™œìš©í•˜ì—¬ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘
    search_keywords = [f"{opp} latest trends" for opp in opportunities[:2]] # ìµœëŒ€ 2ê°œì˜ ê¸°íšŒì— ëŒ€í•´ ê²€ìƒ‰
    print(f"-> ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")

    context_text = ""
    try:
        if tavily_tool and search_keywords:
            tavily_results = tavily_tool.batch(search_keywords)
            context_text = f"--- Latest Market Trends & Context ---\n{tavily_results}\n\n"
    except Exception as e:
        print(f"-> ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- 3. LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ìŠ¤í† ë¦¬í…”ë§ ì¶”ì²œ ìƒì„± ---
    recommend_storytelling_summary_prompt = RECOMMEND_STORYTELLING_PROMPT['recommend_storytelling_summary']

    prompt_variable_inputs = {
        "strengths_str": str(strengths),
        "opportunities_str": str(opportunities),
        "user_experience_str": user_experience,
        "context_text": context_text
    }

    storytelling_recommendations = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=recommend_storytelling_summary_prompt,
        pydantic_model=StorytellingRecommendationOutput,
        llm=llm,
        log_message="-> Helper: ìµœì¢… ìŠ¤í† ë¦¬í…”ë§ ì „ëµ ë° ê²½í—˜ ì œì•ˆ ìƒì„± ì¤‘..."
    )

    return {
        "storytelling_recommendations": storytelling_recommendations
    }

class FinalReportOutput(BaseModel):
    """
    ìµœì¢… ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¥¼ ë‹´ê¸° ìœ„í•œ ëª¨ë¸
    """
    report: str = Field(description="ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±ëœ ìµœì¢… ë³´ê³ ì„œ")

def final_report_node(state: AgentState) -> dict:
    """
    ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìì—ê²Œ ì œê³µí• 
    ìµœì¢…ì ì¸ ìì—°ì–´ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 7] ìµœì¢… ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    # ë³´ê³ ì„œ ì‘ì„±ì— í•„ìš”í•œ ëª¨ë“  ì¬ë£Œë¥¼ stateì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    user_profile = state.get("user_profile_structured", {})
    gap_analysis = state.get("gap_analysis", {})
    routing_reason = state.get("routing_reason_narrative", "")
    learning_rec = state.get("learning_recommendations", {})
    storytelling_rec = state.get("storytelling_recommendations", {})
    llm = state.get("llm")

    # ì¶”ì²œ ê²°ê³¼ë¥¼ ì„ íƒ (ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼)
    recommendations = learning_rec if learning_rec.get("recommendations") else storytelling_rec

    # --- 2. LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ìƒì„± ---
    report_context_str = json.dumps({
        "user_profile": user_profile,
        "gap_analysis": gap_analysis,
        "routing_reason": routing_reason,
        "recommendations": recommendations
    }, ensure_ascii=False, indent=2)

    # --- 3. ìµœì¢… ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ---
    report_summary_prompt = FINAL_REPORT_PROMPT['report_summary']

    prompt_variable_inputs = {
        "report_context": report_context_str
    }

    final_report_result = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=report_summary_prompt,
        pydantic_model=FinalReportOutput,
        llm=llm,
        log_message="-> Helper: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘..."
    )

    report_text = final_report_result.get("report", "ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    return {
        "final_report": report_text
    }

### ìˆ˜í–‰ ì˜ˆì œ ì½”ë“œ
import pprint
from langgraph.graph import StateGraph, END
from googleapiclient.discovery import build
from langchain_openai import ChatOpenAI
from langchain_community.tools import ArxivQueryRun, TavilySearchResults

if __name__ == '__main__':
    # --- 1. ì¤€ë¹„ ë‹¨ê³„: API, ì„œë¹„ìŠ¤, LLM, ë„êµ¬ ê°ì²´ ìƒì„± ---
    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("âœ… API ë° ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸ”´ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
        exit()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    tavily_tool = TavilySearchResults(max_results=3)
    arxiv_tool = ArxivQueryRun()

    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    workflow = StateGraph(AgentState)

    # ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
    workflow.add_node("user_profiling", user_profiling_node)
    workflow.add_node("domestic_analysis", domestic_job_analysis_node)
    workflow.add_node("global_trends", global_trend_analysis_node)
    workflow.add_node("gap_analysis", gap_analysis_node)
    workflow.add_node("router", llm_router_node)
    workflow.add_node("recommend_learning", recommend_learning_node)
    workflow.add_node("recommend_storytelling", recommend_storytelling_node)
    workflow.add_node("final_report", final_report_node)

    # ë…¸ë“œ ê°„ì˜ ì‹¤í–‰ íë¦„ ì •ì˜
    workflow.set_entry_point("user_profiling")
    workflow.add_edge("user_profiling", "domestic_analysis")
    workflow.add_edge("domestic_analysis", "global_trends")
    workflow.add_edge("global_trends", "gap_analysis")
    workflow.add_edge("gap_analysis", "router")

    # ë¼ìš°í„°ì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ì¶”ì²œ ë…¸ë“œë¡œ ë¶„ê¸°
    workflow.add_conditional_edges(
        "router",
        lambda state: state["next_action"],
        {
            "recommend_learning": "recommend_learning",
            "recommend_storytelling": "recommend_storytelling"
        }
    )

    # ì¶”ì²œ ë…¸ë“œ ì‹¤í–‰ í›„, ìµœì¢… ë³´ê³ ì„œ ë…¸ë“œë¡œ ê²°ê³¼ë¥¼ ëª¨ìŒ
    workflow.add_edge("recommend_learning", "final_report")
    workflow.add_edge("recommend_storytelling", "final_report")

    # ìµœì¢… ë³´ê³ ì„œ ë…¸ë“œê°€ ëë‚˜ë©´ ê·¸ë˜í”„ ì¢…ë£Œ
    workflow.add_edge("final_report", END)

    app = workflow.compile()

    # --- 3. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ì •ì˜ ---
    # (í•„ìš”ì— ë”°ë¼ user_input ë‚´ìš©ì„ ë°”ê¾¸ì–´ í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”)
    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ì—”ì§€ë‹ˆì–´",
        "í¬ë§ ê¸°ì—…": ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤"],
        "í•™ë…„/í•™ê¸°": "4í•™ë…„ 1í•™ê¸°",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì»´í“¨í„°ê³µí•™ê³¼",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "Python, SQL, PyTorch, AWS S3/EC2 ê¸°ë³¸ ì‚¬ìš© ê²½í—˜, ì •ë³´ì²˜ë¦¬ê¸°ì‚¬",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ìº¡ìŠ¤í†¤ ë””ìì¸ í”„ë¡œì íŠ¸ (PyTorch ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ ë° ë°°í¬ ì‹œë„)",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "MLOps ë¶„ì•¼ë¡œ ì „ë¬¸ì„±ì„ í‚¤ìš°ê³  ì‹¶ì€ë°, ì–´ë–¤ ê¸°ìˆ ì„ ë” ê³µë¶€í•´ì•¼ í• ê¹Œìš”?"
    }

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm,
        "tools": {"tavily": tavily_tool, "arxiv": arxiv_tool}
    }

    # --- 4. ê·¸ë˜í”„ ì‹¤í–‰ ---
    print("\nğŸš€ ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘")
    print("="*80)

    # streamì„ ì‚¬ìš©í•˜ë©´ ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ìµœì¢… ê²°ê³¼ë§Œ ë³´ë ¤ë©´ response = app.invoke(initial_state)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    response = None
    for state_update in app.stream(initial_state):
        node_name = list(state_update.keys())[0]
        node_output = state_update.get(node_name)

        print(f"\n--- ğŸ“Œ [ë…¸ë“œ: {node_name}] ì‹¤í–‰ ì™„ë£Œ ---")
        pprint.pprint(node_output)

        # ìŠ¤íŠ¸ë¦¼ì˜ ë§ˆì§€ë§‰ ìƒíƒœê°€ ìµœì¢… ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.
        response = state_update

    print("\n\nâœ… ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")

    # --- 5. ìµœì¢… ê²°ê³¼ë¬¼ ì¶œë ¥ ---
    if response:
        final_report = response.get("final_report", {}).get("final_report", "ìµœì¢… ë³´ê³ ì„œë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("\n\n" + "="*80)
        print("âœ¨ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ ë° ì¶”ì²œ ë³´ê³ ì„œ âœ¨")
        print("="*80)
        # Markdown í˜•ì‹ì˜ ë³´ê³ ì„œë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
        from IPython.display import display, Markdown
        display(Markdown(final_report))
        print("="*80)
    else:
        print("\n\nğŸ”´ ìµœì¢… ìƒíƒœë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
