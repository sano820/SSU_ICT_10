# -*- coding: utf-8 -*-
"""report_version2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PF-_W9sAUP6YeB_dboSCzPG9BRQPIrKF
"""

!pip install -q openai==1.101.0 langchain==0.3.27 langchain-core==0.3.74 langchain-openai==0.3.30 langchain-community==0.3.27 pytube==15.0.0 tavily-python==0.7.11 youtube-search-python==1.6.6 langgraph==0.6.4 arxiv==2.2.0 pymupdf==1.26.3 youtube_transcript_api==1.2.2

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')

# %cd /content/drive/MyDrive/ICT_프로젝트

import json
from datetime import datetime, timedelta  # 날짜 계산을 위해 추가

# OpenAI & LangChain
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser, PydanticOutputParser

# LangChain Community Tools & Loaders
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import ArxivLoader
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# LangGraph
from langgraph.graph import StateGraph, END

# Typing
from typing import TypedDict, List, Dict, Any, Optional, Type

# Tools
from tools import analyze_video_content, search_naver_news, search_global_news, search_arxiv_papers, tavily_web_search, find_videos_with_transcripts, analyze_youtube_topic

import os
from google.colab import userdata

os.environ['OPENAI_API_KEY'] = userdata.get('ssu')
os.environ['TAVILY_API_KEY'] = userdata.get('tavily')
os.environ['YOUTUBE_API_KEY'] = userdata.get('youtube')
os.environ['NAVER_CLIENT_ID'] = userdata.get('naver_client_id')
os.environ['NAVER_CLIENT_SECRET'] = userdata.get('naver_client_secret')
os.environ['NEWS_API_KEY'] = userdata.get('news_api_key')

class AgentAPIs:
    """
    에이전트가 사용하는 모든 API 키를 로드하고 관리하는 클래스.
    """
    def __init__(self):
        print("-> API 키 로딩 시작...")

        # userdata에서 각 API 키를 불러옵니다.
        self.openai_api_key = userdata.get('ssu')
        self.tavily_api_key = userdata.get('tavily')
        self.youtube_api_key = userdata.get('youtube')
        self.naver_client_id = userdata.get('naver_client_id')
        self.naver_client_secret = userdata.get('naver_client_secret')
        self.news_api_key = userdata.get('news_api_key')

        # 필수 키들이 모두 로드되었는지 확인합니다.
        if not all([self.openai_api_key, self.tavily_api_key, self.youtube_api_key]):
            raise ValueError("필수 API 키(OpenAI, Tavily, YouTube)가 Colab Secrets에 설정되지 않았습니다.")

        print("✅ 모든 API 키 로딩 완료.")

    def set_env_variables(self):
        """
        로드된 키들을 환경 변수로 설정합니다. (기존 방식과 호환성을 위해)
        """
        print("-> 환경 변수 설정 중...")
        os.environ['OPENAI_API_KEY'] = self.openai_api_key
        os.environ['TAVILY_API_KEY'] = self.tavily_api_key
        os.environ['YOUTUBE_API_KEY'] = self.youtube_api_key
        os.environ['NAVER_CLIENT_ID'] = self.naver_client_id
        os.environ['NAVER_CLIENT_SECRET'] = self.naver_client_secret
        os.environ['NEWS_API_KEY'] = self.news_api_key
        print("✅ 환경 변수 설정 완료.")

class UserProfile(TypedDict):
    academic_year: Optional[str]
    major: Optional[str]
    skills_and_certs: Optional[str]
    experience_specs: Optional[str]
    goals: Optional[str]
    narrative_summary: str

class GlobalTrends(TypedDict):
    generated_keywords: str
    prediction: str
    distilled_prediction: str

class AcademicResearch(TypedDict):
    summary: str
    distilled_summary: str

class StorytellingStrategy(TypedDict):
    experience_plan: str
    storytelling_example: str

class AgentState(TypedDict):
    user_profile_raw: Dict[str, Any]  # 사용자가 입력한 원본 정보

    # --- user_profiling_node의 결과 ---
    user_profile: UserProfile
    target_job: List[str]
    target_company: List[str]
    user_questions: Optional[str]

    # --- domestic_job_analysis_node의 결과 ---
    # 더 세분화된 정보를 직접 저장합니다.
    domestic_analysis_components: Optional[Dict[str, str]]
    domestic_keywords: Optional[Dict[str, Any]]

    global_trends: Optional[Any] # GlobalTrends
    academic_research: Optional[Any] # AcademicResearch
    final_report: Optional[str]
    api_keys: Optional[AgentAPIs]
    gap_analysis: Optional[Dict[str, Any]]

class RefinedCompanies(BaseModel):
    companies: List[str] = Field(description="정제된 개별 공식 기업명 리스트")

class PostingAnalysisOutput(BaseModel):
    """채용 공고 분석 결과 모델"""
    role_goal: str = Field(description="이 직무가 달성해야 할 정량적/정성적 목표")
    key_responsibilities: List[str] = Field(description="목표 달성을 위한 구체적인 주요 책임 리스트")
    hard_skills: List[str] = Field(description="필수적인 툴과 기술명 리스트")
    collaboration_process: str = Field(description="어떤 동료와 어떤 방식으로 협업하는지에 대한 구체적인 프로세스")
    preferred_experiences: List[str] = Field(description="선호하는 구체적인 경험 리스트")

class ReviewAnalysisOutput(BaseModel):
    """
    모든 직군의 '합격 후기'에서 범용적으로 사용할 수 있는 데이터 추출 모델.
    '어떻게 합격했는가?'라는 채용 프로세스 단계별 핵심 전략에 초점을 맞춥니다.
    """
    document_strategy: List[str] = Field(
        description="서류 전형(자기소개서, 이력서) 통과를 위해 합격자가 강조했던 핵심 경험, 역량, 또는 작성 전략"
    )
    test_strategy: List[str] = Field(
        description="인적성, NCS, 논술, 코딩 테스트, 과제 등 필기/과제 전형의 종류와 구체적인 준비 방법 또는 팁"
    )
    job_interview_strategy: List[str] = Field(
        description="1차 면접(실무진 면접)에서 직무 역량을 어필하기 위해 합격자가 사용한 전략이나 받았던 핵심 질문"
    )
    final_interview_strategy: List[str] = Field(
        description="최종 면접(임원 면접)에서 인성이나 조직 적합성을 어필하기 위해 합격자가 사용한 전략이나 받았던 핵심 질문"
    )
    critical_success_factor: str = Field(
        description="합격자가 스스로 생각하는, 합격에 가장 결정적이었던 자신만의 차별화 포인트 (가장 중요한 것 한 가지)"
    )

class InterviewAnalysisOutput(BaseModel):
    """
    모든 직군의 현직자 인터뷰에서 범용적으로 사용할 수 있는 데이터 추출 모델
    """
    core_competencies_and_tools: List[str] = Field(
        description="현직자가 강조하는 해당 직무의 핵심 역량과, 업무에 실제로 사용하는 필수 툴(예: MS Office, Slack, Jira, Salesforce, Figma, Adobe Photoshop) 등"
    )
    team_culture_and_workflow: str = Field(
        description="팀의 의사소통 방식, 회의 문화, 협업 프로세스, 성과 평가 방식 등 구체적인 업무 스타일과 조직 문화"
    )
    growth_and_career_path: str = Field(
        description="회사에서 제공하는 교육, 멘토링 제도나 현직자가 말하는 현실적인 커리어 성장 경로"
    )
    advice_for_applicants: List[str] = Field(
        description="현직자가 해당 직무 지원자에게 '이것만은 꼭 준비하라'고 조언하는 구체적인 경험이나 역량"
    )

class GlobalSearchKeywords(BaseModel):
    """
    글로벌 트렌드를 다각적으로 검색하기 위한 키워드 모델
    """
    core_technologies: List[str] = Field(
        description="분석 결과의 핵심이 되는 주요 기술 키워드 (예: Deep Learning, Computer Vision)"
    )
    business_domains: List[str] = Field(
        description="언급된 주요 비즈니스 및 산업 도메인 키워드 (예: Fintech, Robotics)"
    )
    emerging_roles: List[str] = Field(
        description="새롭게 부상하거나 중요도가 높아지는 직무명 (예: Prompt Engineer, AI Ethicist, MLOps Specialist)"
    )
    problem_solution_keywords: List[str] = Field(
        description="기술이 해결하려는 구체적인 문제나 솔루션 키워드 (예: Fraud Detection, Hyper-personalization, Digital Twin, ESG)"
    )

class GlobalTrendsOutput(BaseModel):
    """
    글로벌 트렌드 분석 결과를 구조화하기 위한 모델
    """
    future_outlook: str = Field(
        description="이 직무의 역할과 중요성에 대한 미래 전망 종합 분석 (1-2 문장 요약)"
    )
    key_technology_shifts: List[str] = Field(
        description="현재 떠오르거나, 앞으로 필수가 될 구체적인 최신 기술, 툴, 플랫폼 키워드 리스트"
    )
    changing_market_demands: List[str] = Field(
        description="기업들이 이 직무에 대해 새롭게 요구하기 시작한 구체적인 역량이나 경험 리스트"
    )
    key_messages_from_leaders: str = Field(description="업계 리더들이 공통적으로 강조하는 가장 핵심적인 메시지나 인용구 (1-2개)")

class GapAnalysisOutput(BaseModel):
    """
    사용자 프로필과 시장 분석 결과를 비교하여 강점, 약점, 기회를 진단하는 모델
    """
    strengths: List[str] = Field(
        description="사용자가 현재 보유한 역량/경험 중, 시장의 요구사항 및 트렌드와 일치하는 명확한 강점 리스트"
    )
    weaknesses: List[str] = Field(
        description="시장의 요구사항 및 트렌드에 비해 사용자가 명백히 부족하거나 없는 역량/경험 리스트 (보완점)"
    )
    opportunities: List[str] = Field(
        description="사용자의 강점을 글로벌 트렌드와 연결하여, 경쟁력을 한 단계 더 높일 수 있는 기회 영역 리스트"
    )
    summary: str = Field(
        description="사용자의 현재 상태에 대한 종합적인 평가 및 다음 단계에 대한 방향성 요약 (2-3 문장)"
    )

# 키워드 추출 함수

def extract_structured_data_flexible(
    prompt_inputs: Dict[str, Any],
    extraction_prompt_template: str,
    pydantic_model: Type[BaseModel],
    llm: Any,
    log_message: str
) -> Dict[str, Any]:
    """
    주어진 '어떤' 입력 변수(prompt_inputs)와 Pydantic 모델을 사용하여
    구조화된 데이터를 추출하는 더 유연한 범용 헬퍼 함수.
    """
    print(log_message)

    try:
        parser = PydanticOutputParser(pydantic_object=pydantic_model)

        prompt = ChatPromptTemplate.from_template(
            template=extraction_prompt_template,
            partial_variables={"format_instructions": parser.get_format_instructions()},
        )

        extractor_chain = prompt | llm | parser
        # [수정] 인자로 받은 prompt_inputs 딕셔너리를 그대로 invoke에 전달
        extracted_data = extractor_chain.invoke(prompt_inputs)
        return extracted_data.model_dump()

    except Exception as e:
        print(f"-> ⚠️ 구조화된 데이터 추출 실패 (오류: {e}), 빈 딕셔너리를 반환합니다.")
        return {}

def user_profiling_node(state: AgentState) -> Dict[str, Any]:
    """
    사용자가 입력한 원본 정보를 정제/구조화하고,
    '고민 또는 궁금한 점'을 별도의 키로 추출하여 반환합니다.
    """
    print("\n--- [Step 0] 사용자 정보 구조화 노드 실행 ---")

    user_info = state["user_profile_raw"]

    # key_to_label 정의 및 profile_text 생성
    key_to_label = {
        "목표 직무": "목표 직무",
        "희망 기업": "희망 기업",
        "학년/학기": "학년/학기",
        "재학 여부": "재학 여부",
        "전공 및 복수(부)전공": "전공",
        "보유 기술 및 자격증": "보유 기술 및 자격증",
        "관련 경험 및 스펙" : "관련 경험 및 스펙",
        "관심 분야 및 목표": "관심 분야 및 목표",
        "고민 또는 궁금한 점": "고민 또는 궁금한 점"
    }
    target_job = user_info.get("목표 직무", "지정되지 않음")
    target_company = user_info.get("희망 기업", [])
    profile_parts = [f"- {label}: {user_info.get(key)}" for key, label in key_to_label.items() if user_info.get(key) and str(user_info.get(key)).strip()]
    profile_text = "\n".join(profile_parts) if profile_parts else "입력된 사용자 정보가 없습니다."

    prompt_template = """
    당신은 학생 정보를 정확하게 파싱하고 구조화하는 데이터 처리 전문가입니다.
    아래 [학생 정보]를 바탕으로, 각 항목의 핵심 내용을 추출하여 유효한 JSON 객체로 답변해주세요.
    응답은 오직 JSON 객체만을 포함해야 합니다.

    ---
    [학생 정보]
    {profile_text}
    ---

    **[분석 및 추출 요청]**
    1.  **`skills_and_certs`**: '보유 기술 및 자격증' 항목에서 프로그래밍 언어, 툴, 자격증 이름 등 핵심 기술 역량을 간결하게 요약해주세요.
    2.  **`experience_specs`**: '관련 경험 및 스펙' 항목에서 프로젝트, 인턴, 수상 경력 등 구체적인 경험을 요약해주세요.
    3.  나머지 필드(`academic_year`, `major`, `goals`)도 각각 해당하는 내용을 채워주세요. 정보가 없다면 빈 문자열("")로 두세요.
    4.  **`narrative_summary`**: 모든 정보를 종합하여 '{target_job}' 직무 목표에 맞춰 2-3 문장의 자연어 프로필을 요약해주세요.

    **[JSON 출력 형식]**
    {{
        "academic_year": "...",
        "major": "...",
        "skills_and_certs": "...",
        "experience_specs": "...",
        "goals": "...",
        "narrative_summary": "..."
    }}
    """

    parser = JsonOutputParser(pydantic_object=UserProfile)
    prompt = ChatPromptTemplate.from_template(template=prompt_template)
    profiling_chain = prompt | llm | parser

    try:
        structured_profile = profiling_chain.invoke({
            "profile_text": profile_text,
            "target_job": target_job
        })
        print("-> 구조화된 사용자 프로필 생성 완료.")
    except Exception as e:
        print(f"-> 프로필 생성 체인 실행 실패: {e}\n-> 대체 프로필을 생성합니다.")
        structured_profile = {"narrative_summary": "사용자 정보를 분석하는 데 실패했습니다."}

    # 사용자의 핵심 질문을 별도로 추출
    user_questions = user_info.get("고민 또는 궁금한 점", "")
    print(f"-> 추출된 사용자 질문: '{user_questions}'")

    original_target_job = user_info.get("목표 직무", "")
    refined_jobs = [original_target_job] if original_target_job else [] # 기본값은 원본값을 담은 리스트

    # --- 💡 LLM을 이용한 직무명 구체화 로직 ---
    if original_target_job:
        print(f"-> 목표 직무 구체화 시작 (입력: '{original_target_job}')")

        # 1. 직무명 구체화를 위한 프롬프트와 체인 정의
        JOB_REFINEMENT_PROMPT = """
        당신은 모든 산업 분야의 직무를 깊이 이해하는 전문 커리어 컨설턴트입니다.
        아래 '입력 직무명'이 추상적이거나 포괄적인 경우, 사용자의 의도를 추론하여 실제 채용 시장에서 통용되는 **가장 대표적이고 구체적인 직무명 2~3개를 추천**해주세요.

        **지시사항:**
        - 만약 입력이 이미 구체적이라면, 해당 직무명 하나만 리스트에 담아 반환해주세요.
        - 응답은 오직 JSON 문자열 배열(string array) 형식이어야 합니다. (예: ["직무명1", "직무명2"])
        - 다른 설명은 절대 추가하지 마세요.

        **예시:**
        - 입력: "개발자"
        - 출력: ["백엔드 개발자", "프론트엔드 개발자", "앱 개발자"]

        - 입력: "마케팅"
        - 출력: ["콘텐츠 마케터", "퍼포먼스 마케터", "브랜드 마케터"]

        - 입력: "증권"
        - 출력: ["애널리스트", "IB (투자은행)", "PB (프라이빗 뱅커)"]

        - 입력: "디자인"
        - 출력: ["UI/UX 디자이너", "BX 디자이너", "그래픽 디자이너"]

        - 입력: "회계사"
        - 출력: ["회계사"]

        ---
        입력 직무명: {original_job}
        """

        try:
            # 체인 구성 및 호출
            parser = JsonOutputParser()
            prompt = ChatPromptTemplate.from_template(JOB_REFINEMENT_PROMPT)
            refinement_chain = prompt | llm | parser

            refined_jobs = refinement_chain.invoke({"original_job": original_target_job})

            print(f"-> ✨ 구체화된 직무 목록: {refined_jobs}")

        except Exception as e:
            # LLM 파싱 실패 또는 오류 발생 시 처리
            print(f"-> ⚠️ 직무명 구체화 실패 (오류: {e}), 원본 값을 사용합니다.")
            refined_jobs = [original_target_job]
    else:
        # 목표 직무를 입력하지 않은 경우 처리
        print("-> 목표 직무가 입력되지 않았습니다.")
        refined_jobs = []

    original_target_companies = user_info.get("희망 기업", [])
    refined_companies = []
    # --- 💡 LLM을 이용한 기업명 정제 로직 ---
    if original_target_companies:
        print(f"-> 희망 기업 목록 정제 시작 (입력: {original_target_companies})")

        # 1. 함수 내에서 프롬프트와 체인을 직접 정의
        COMPANY_REFINEMENT_PROMPT = """
        당신은 대한민국의 주요 기업과 산업별 그룹/용어(예: 네카라쿠배, 금융공기업 A매치)를 정확히 이해하는 채용 전문가입니다.
        아래 '입력 리스트'에 포함된 회사명 또는 그룹명을 실제 웹 검색에 사용될 수 있는 **개별 공식 기업명 리스트**로 변환해주세요.

        **지시사항:**
        - 그룹명이나 축약어는 해당 그룹에 속한 개별 기업명으로 모두 풀어주세요.
        - 이미 개별 기업명인 경우, 그대로 유지해주세요.
        - 응답은 오직 JSON 문자열 배열(string array) 형식이어야 합니다. 다른 설명은 절대 추가하지 마세요.

        **예시:**
        - 입력: ["네카라쿠배"]
        - 출력: ["네이버", "카카오", "라인", "쿠팡", "배달의민족"]

        - 입력: ["A매치 금융공기업"]
        - 출력: ["한국은행", "금융감독원", "산업은행", "수출입은행", "예금보험공사"]

        - 입력: ["삼성"]
        - 출력: ["삼성전자, 삼성SDI, 삼성증권, 삼성카드, 삼성물산"]

        ---
        입력 리스트: {company_list_str}
        """

        try:
            # 2. LLM이 Pydantic 모델에 맞춰 출력하도록 강제합니다.
            structured_llm = llm.with_structured_output(RefinedCompanies)
            prompt = ChatPromptTemplate.from_template(COMPANY_REFINEMENT_PROMPT)
            refinement_chain = prompt | structured_llm

            company_list_str = ", ".join(original_target_companies)

            # 3. 체인을 호출하면 Pydantic 모델 객체가 반환됩니다.
            result_model = refinement_chain.invoke({"company_list_str": company_list_str})
            refined_companies = result_model.companies # .companies 속성으로 리스트에 접근

            print(f"-> ✨ 정제된 기업 목록: {refined_companies}")

        except Exception as e:
            print(f"-> ⚠️ 기업명 정제 실패 (오류: {e}), 원본 값을 사용합니다.")
            refined_companies = original_target_companies
    else:
        # 4. 희망 기업을 입력하지 않은 경우 처리
        print("-> 희망 기업이 입력되지 않았습니다.")

    # 3. 반환값의 'target_company'에 **정제된 기업 리스트**를 담아줍니다.
    return {
        "user_profile": structured_profile,
        "target_job": refined_jobs,
        "target_company": refined_companies,
        "user_questions": user_questions,
        "llm" : llm
    }

def domestic_job_analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    정제된 직무와 기업명을 바탕으로 국내 채용 시장의 각 측면을 분석하고 종합합니다.
    """
    print("\n--- [Step 2] 국내 채용 시장 분석 노드 실행 ---")

    # --- 1. 사전 준비 ---
    target_jobs_list = state["target_job"]
    refined_companies = state["target_company"]
    target_job_title = ", ".join(target_jobs_list)
    job_query = f'"{" OR ".join(target_jobs_list)}"' if target_jobs_list else ""
    company_query = f'"{" OR ".join(refined_companies)}"' if refined_companies else ""
    api_keys = state['api_keys']

    # --- 2. 채용 공고 분석 ---
    print("\n1. 채용 공고 분석 중...")
    postings_web_results = tavily_web_search.invoke(
        " ".join(part for part in [company_query, job_query, "신입 채용 공고 자격요건 우대사항"] if part)
    )

    postings_prompt_template = """
    **페르소나 (Persona):**
    당신은 주어진 텍스트에서 '인용 가능한 구체적인 사실(Quoted Fact)'만을 정확하게 뽑아내는 정보 추출 AI입니다. 당신은 절대 추론하거나 요약하지 않습니다.

    **임무 (Mission):**
    {target_job_title} 직무의 주어진 채용 공고 내용({search_results})에서 아래 규칙에 따라 정보를 추출하여 {format_instructions}에 명시된 JSON 형식으로 출력합니다.

    **규칙 (Strict Rules):**
    1.  **추상적 표현 금지:** '원활한 소통', '적극적인 참여', '열정', '책임감'과 같이 정성적이고 추상적인 표현은 절대 생성하지 마시오.
    2.  **사실 기반 추출:** 반드시 원문에서 근거를 찾을 수 있는 고유명사(프로그래밍 언어, 프레임워크, 툴 이름)나 구체적인 행위, 프로세스만 추출하시오.
    3.  **정보 없음 처리:** 만약 원문에서 특정 항목에 대한 구체적인 정보를 찾을 수 없다면, 해당 필드는 빈 리스트 `[]`나 '정보 없음' 문자열을 값으로 사용하시오.
    4.  **정확한 분류:** `hard_skills`에는 오직 기술 스택(언어, 프레임워크, 툴)만 포함하고, '경력'이나 '경험'과 같은 내용은 `preferred_experiences` 항목에만 포함시키시오.
    5.  **언어 통일:** 모든 결과는 반드시 한국어로 작성하시오.

    **예시 (Examples):**

    --- [IT/개발 직군 예시] ---
    - **나쁜 예시:**
      ```json
      {{
        "hard_skills": ["문제 해결 능력", "커뮤니케이션 스킬"],
        "preferred_experiences": ["최신 기술에 대한 관심"]
      }}

    - **좋은 예시:**
      {{
      "hard_skills": ["Python", "PyTorch", "AWS S3", "Docker"],
      "collaboration_process": "2주 단위 스프린트로 Jira 이슈 관리, 매일 데일리 스크럼 진행",
      "preferred_experiences": ["MSA 기반 서비스 개발 및 운영 경험"]
      }}

    --- [금융/기획 직군 예시] ---
        - **나쁜 예시:**
      {{
      "hard_skills": ["성실함", "꼼꼼함"],
      "preferred_experiences": ["금융 산업에 대한 높은 이해도"]
      }}

    - **좋은 예시:**
      {{
      "hard_skills": ["재무회계 지식", "MS Excel (피벗, VLOOKUP)", "투자자산운용사 자격증"],
      "preferred_experiences": ["M&A 프로젝트 참여 경험", "기업 가치 평가(Valuation) 보고서 작성 경험"]
      }}

    --- [마케팅/콘텐츠 직군 예시] ---
     - **나쁜 예시:**
      {{
      "hard_skills": ["창의력", "트렌드에 민감함"],
      "key_responsibilities": ["브랜드 인지도 상승"]
      }}

    - **좋은 예시:**
      {{
      "hard_skills": ["GA(Google Analytics) 활용 능력", "페이스북 광고 관리자 경험", "SEO 기본 지식"],
      "key_responsibilities": ["주 2회 블로그 콘텐츠 작성 및 발행", "월별 성과 보고서 작성"]
      }}

      '''
    """

    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": postings_web_results
    }

    postings_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=postings_prompt_template,
    pydantic_model=PostingAnalysisOutput,
    llm=llm,
    log_message="-> Helper: 채용 공고 분석 및 구조화 실행..."
    )


    # --- 3. 합격 후기 분석 ---
    print("\n2. 합격 후기 분석 중...")
    current_year = datetime.now().year
    years_query = " OR ".join(str(y) for y in range(current_year, current_year - 3, -1))

    reviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} 신입 합격 OR 면접 후기 ({years_query}) site:velog.io OR site:tistory.com OR site:brunch.co.kr'
    )

    REVIEW_TOPIC_PROMPT = """
    당신은 유튜브 검색 전문가입니다.
    아래 주어진 '목표 기업'과 '목표 직무'를 바탕으로, '신입 합격 후기' 영상을 찾기 위한 가장 효과적인 유튜브 검색어 한 줄을 한국어로 생성해주세요.

    **지시사항:**
    - "신입 합격 후기", "취준", "서류 스펙", "면접후기", "합격 브이로그" 등 관련성 높은 키워드를 다양하게 조합하여 검색어를 만들어주세요.

    **범주화 규칙 (매우 중요):**
    - **'목표 기업'**이 여러 개일 경우: 개별 회사명을 모두 나열하지 말고, 기업들의 공통점을 나타내는 하나의 포괄적인 범주로 변환하세요.
      - (예시 1) ["네이버", "카카오", "라인"] -> "IT 대기업" 또는 "서비스 기업"
      - (예시 2) ["한국은행", "금융감독원"] -> "금융 공기업"
      - (예시 3) ["컬리", "토스"] -> "유니콘 스타트업"
    - **'목표 직무'**가 여러 개일 경우: 직무들을 대표하는 상위 직군이나 기술 분야로 묶어서 하나의 단어로 만드세요.
      - (예시 1) ["백엔드 개발자", "서버 개발자"] -> "서버 개발자"
      - (예시 2) ["머신러닝 엔지니어", "AI 연구원"] -> "AI/ML 직무"

    - 최종 검색어는 위의 규칙에 따라 변환된 범주와 다른 키워드들을 조합하여 **단 한 줄의 문자열**이어야 합니다.

    **목표 기업:** {companies}
    **목표 직무:** {jobs}
    """
    review_topic_chain = ChatPromptTemplate.from_template(REVIEW_TOPIC_PROMPT) | llm | StrOutputParser()

    youtube_review_topic = review_topic_chain.invoke({
        "companies": refined_companies,
        "jobs": target_jobs_list
    })
    youtube_review_prompt = f"""
    '{target_job_title}' 직무 신입 합격 후기 정보들을 종합하여, '검증 가능한 사실' 기반의 합격 전략을 아래 구조로 정리해줘.

    - **[합격자 프로필(Profile)]**: 합격자들의 공통적인 구체적 스펙은 무엇인가?
    - **[채용 프로세스별 준비 사항(Process Prep)]**: 각 단계별로 무엇을, 어떻게 준비했는가?
    - **[결정적 합격 증거(Actionable Evidence)]**: 합격자들이 제시하는 자신의 가장 강력한 경쟁력은 무엇인가?
    """

    youtube_summary = analyze_youtube_topic(
        topic=youtube_review_topic,
        analysis_prompt=youtube_review_prompt,
        num_to_analyze=2,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )


    # 웹 검색 결과와 유튜브 분석 결과를 합쳐서 최종 분석
    # --- 4b. 수집된 데이터 종합 및 헬퍼 함수 호출 ---
    combined_reviews  = f"--- 웹 검색 결과 (최근 3년) ---\n{reviews_web_results}\n\n--- 유튜브 영상 분석 (최근 3년) ---\n{youtube_summary}"

    reviews_prompt_template = """
    **페르소나 (Persona):**
    당신은 주어진 텍스트에서 '인용 가능한 구체적인 사실(Quoted Fact)'만을 정확하게 뽑아내는 정보 추출 AI입니다. 당신은 절대 추론하거나 요약하지 않습니다.

    **임무 (Mission):**
    {target_job_title} 직무의 주어진 합격 후기 및 현직자 조언({search_results})에서 아래 규칙에 따라 정보를 추출하여 {format_instructions}에 명시된 JSON 형식으로 출력합니다.

    **규칙 (Strict Rules):**
    1.  **추상적 조언 절대 금지:** "...할 수 있는 능력", "...하는 것이 중요", "...하도록 노력"과 같이 행동이 아닌 태도나 마음가짐에 대한 내용은 절대 추출하지 마시오. 이런 내용만 있다면 '정보 없음'으로 처리하시오.
    2.  **사실 기반 추출:** 반드시 원문(합격 후기)에서 언급된 구체적인 경험, 준비 과정, 기술, 프로젝트, KPI, 역할에 대한 내용만 추출하시오.
    3.  **정보 없음 처리:** 만약 원문에서 특정 항목에 대한 구체적인 정보를 찾을 수 없다면, 해당 필드는 '정보 없음' 문자열을 값으로 사용하시오.
    4.  **언어 통일:** 모든 결과는 반드시 한국어로 작성하시오.

    **예시 (Examples):**

    - **나쁜 예시 (Bad Example):**
      ```json
      {{
        "first_year_role": "팀에 기여하고 배우는 역할",
        "actionable_advice": ["문제 해결 능력을 키우세요", "소통 능력이 중요합니다"]
      }}

      ** 좋은 예시 (Good Example): **
      {{
      "first_year_role": "초기 3개월간 버그 수정 및 테스트 코드 작성 위주로 업무, 이후 작은 기능 개발 담당",
      "performance_metric": "주간 PR(Pull Request) 개수와 코드 리뷰 승인율",
      "actionable_advice": ["코딩 테스트 준비 시, 백준 골드 티어 수준의 다이나믹 프로그래밍 문제 풀이 경험이 중요함", "단순 CRUD가 아닌, 대용량 트래픽을 가정했을 때의 DB 인덱싱 전략에 대한 질문을 받았음"]
      }}
      '''
    """
    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_reviews
    }

    reviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=reviews_prompt_template,
    pydantic_model=ReviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: 합격 후기 분석 및 구조화 실행..."
    )

    # --- 4. 현직자 인터뷰 분석 ---
    print("\n3. 현직자 인터뷰 분석 중...")
    interviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} "현직자 인터뷰" OR "직장인 브이로그" OR "일하는 방식" OR "팀 문화" OR "커리어" site:tistory.com OR site:brunch.co.kr'
    )

    # LLM을 사용하여 최적의 유튜브 검색 주제어를 생성
    INTERVIEW_TOPIC_PROMPT = """
    당신은 유튜브 검색 전문가입니다.
    아래 주어진 '목표 기업'과 '목표 직무'를 바탕으로, 현직자들의 생활이나 업무 방식을 엿볼 수 있는 영상을 찾기 위한 가장 효과적인 유튜브 검색어 한 줄을 한국어로 생성해주세요.

    **지시사항:**
    - "현직자 인터뷰", "VLOG", "일하는 방식", "오피스 투어", "팀 소개", "개발자 일상" 등 관련성 높은 키워드를 다양하게 조합하여 검색어를 만들어주세요.

    **범주화 규칙 (매우 중요):**
    - **'목표 기업'**이 여러 개일 경우: 개별 회사명을 모두 나열하지 말고, 기업들의 공통점을 나타내는 하나의 포괄적인 범주로 변환하세요.
      - (예시 1) ["네이버", "카카오", "라인"] -> "IT 대기업" 또는 "네카라"
      - (예시 2) ["한국전력공사", "한국도로공사"] -> "주요 공기업"
      - (예시 3) ["컬리", "토스"] -> "유니콘 스타트업"
    - **'목표 직무'**가 여러 개일 경우: 직무들을 대표하는 상위 직군이나 기술 분야로 묶어서 하나의 단어로 만드세요.
      - (예시 1) ["백엔드 개발자", "서버 개발자"] -> "서버 개발"
      - (예시 2) ["데이터 분석가", "데이터 사이언티스트"] -> "데이터 직군"
      - (예시 3) ["머신러닝 엔지니어", "AI 연구원"] -> "AI/ML 직무"

    - 최종 검색어는 위의 규칙에 따라 변환된 범주와 다른 키워드들을 조합하여 **단 한 줄의 문자열**이어야 합니다.

    **목표 기업:** {companies}
    **목표 직무:** {jobs}
    """

    topic_generation_chain = ChatPromptTemplate.from_template(INTERVIEW_TOPIC_PROMPT) | llm | StrOutputParser()

    interview_topic = topic_generation_chain.invoke({
        "companies": refined_companies,
        "jobs": target_jobs_list
    })
    interview_prompt = f"'{target_job_title}' 직무 현직자로서 일하는 방식, 조직 문화, 신입에게 필요한 역량에 대해 말하는 부분을 핵심만 요약해줘."
    youtube_interview_summary = analyze_youtube_topic(
        topic=interview_topic,
        analysis_prompt=interview_prompt,
        num_to_analyze=1,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )

    # --- 5b. 수집된 데이터 종합 및 헬퍼 함수 호출 ---
    print("-> 텍스트와 유튜브 분석 결과를 종합하여 최종 요약 생성 중...")
    combined_interviews = f"""
    --- Text-based Interview Search Results ---
    {interviews_web_results}

    --- YouTube Interview Analysis Summary ---
    {youtube_interview_summary}
    """

    # 1. 프롬프트 템플릿: '현직자 인터뷰' 분석에 맞게 규칙과 예시를 구체화
    interviews_prompt_template = """
    **페르소나 (Persona):**
    당신은 주어진 텍스트에서 '인용 가능한 구체적인 사실(Quoted Fact)'만을 정확하게 뽑아내는 정보 추출 AI입니다.

    **임무 (Mission):**
    {target_job_title} 직무 현직자들의 경험담({search_results})에서 아래 규칙에 따라 정보를 추출하여 {format_instructions}에 명시된 JSON 형식으로 출력합니다.

    **규칙 (Strict Rules):**
    1.  **추상적 표현 금지:** '수평적인 문화', '자유로운 소통' 등 막연하고 일반적인 표현은 절대 생성하지 마시오.
    2.  **사실 기반 추출:** 반드시 원문에서 언급된 구체적인 업무 프로세스, 역량, 툴 이름, 사내 제도, 실질적인 조언만 추출하시오.
    3.  **정보 없음 처리:** 원문에서 구체적인 정보를 찾을 수 없다면, 해당 필드는 '정보 없음' 또는 빈 리스트 `[]`를 값으로 사용하시오.
    4.  **언어 통일:** 모든 결과는 반드시 한국어로 작성하시오.

    ---
    **최종 검증 (Final Verification):**
    JSON을 생성한 후, 스스로 아래 질문 3가지에 답해보시오. 만약 하나라도 '아니오'라면, 처음부터 다시 분석하여 규칙을 모두 만족하는 결과물을 만드시오.

    1.  `core_competencies_and_tools`에 직무명이나 분야 이름이 포함되어 있지는 않은가? (답: '아니오'. 반드시 실제 역량이나 툴 이름만 있어야 함)
    2.  `growth_and_career_path`에 영상이나 게시물의 제목이 그대로 들어가 있지는 않은가? (답: '아니오'. 반드시 본문 내용에서 추출한 구체적인 경로여야 함)
    3.  모든 결과값이 추상적이거나 일반적인 내용이 아니라, 구체적인 사실에 기반하고 있는가? (답: '예')
    ---

    이제 위 규칙과 최종 검증 절차에 따라 분석을 시작하세요.

    **예시 (Examples):**

    - **나쁜 예시 (Bad Example):**
      ```json
      {{
        "team_culture_and_process": "서로 존중하며 자유롭게 의견을 나눕니다.",
        "advice_for_newcomers": ["기본기를 탄탄히 하고 열심히 배우는 자세가 중요합니다."]
      }}

      ** 좋은 예시 (Good Example): **
      {{
      "day_in_the_life": "오전 9시 데일리 스크럼 후, 오전에는 주로 배정된 Jira 티켓의 버그를 수정하고 코드 리뷰를 진행합니다. 오후에는 다음 스프린트에 포함될 신규 기능 개발에 집중합니다.",
      "real_tech_stack": ["Kotlin", "Spring Boot", "JPA", "Kubernetes", "ArgoCD", "Grafana"],
      "team_culture_and_process": "2주 단위 애자일 스프린트로 운영되며, PR(Pull Request)은 최소 2명의 동료에게 Approve를 받아야 머지할 수 있습니다.",
      "growth_environment": "매주 금요일 오후에는 팀 내 기술 공유 세션이 있고, 분기별로 외부 컨퍼런스 참여를 지원해줍니다.",
      "advice_for_newcomers": ["입사 첫 달에는 우리 팀의 핵심 서비스인 'A'의 아키텍처 문서를 완독하는 것이 최우선 과제입니다.", "Git 브랜치 전략이 복잡하니 미리 숙지해오면 좋습니다."]
      }}
      '''
    """

    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_interviews
    }

    interviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=interviews_prompt_template,
    pydantic_model=InterviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: 현직자 인터뷰 분석 및 구조화 실행..."
    )

    print("-> 다음 노드로 전달할 핵심 키워드 추출 중...")

    # --- 6. 모든 분석 결과 종합 및 키워드 추출 (헬퍼 함수 호출) ---
    combined_analysis_dict = {
        "postings_analysis": postings_analysis,
        "reviews_analysis": reviews_analysis,
        # "interviews_analysis": interviews_analysis
    }

    combined_analysis_str = json.dumps(combined_analysis_dict, ensure_ascii=False, indent=2)

    keyword_extraction_prompt_template = """
    **페르소나 (Persona):**
    당신은 수십 개의 기술 및 비즈니스 문서를 분석하여 미래 트렌드를 예측하는 **전략가(Strategist)이자 미래학자(Futurist)**입니다.

    **임무 (Mission):**
    주어진 대한민국 채용 시장 분석 데이터({market_analysis_json})를 깊이 있게 분석하여, 글로벌 기술 및 커리어 트렌드를 검색하기 위한 핵심 키워드를 {format_instructions}에 명시된 JSON 형식으로 추출합니다.

    **규칙 (Strict Rules):**
    1.  **미래 지향적 분석:** 단순히 언급된 기술을 나열하는 것을 넘어, 데이터에 암시된 **미래 지향적이고 잠재력 있는 키워드**를 포착하시오.
    2.  **정확한 분류:** 각 키워드를 `core_technologies`, `business_domains`, `emerging_roles`, `problem_solution_keywords` 네 가지 분류에 맞게 정확히 할당하시오.
    3.  **핵심 위주 추출:** 너무 세부적이거나 중복되는 키워드는 제외하고, 가장 핵심적이고 대표적인 키워드만 추출하시오.
    4.  **언어 통일:** 모든 키워드는 글로벌 트렌드 검색에 용이하도록 한국어로 생성하시오.

    **사고 과정 예시 (Example of Thought Process):**
    - **입력 데이터:** "자율주행 로봇의 경로 탐색 알고리즘 개발"과 "데이터의 편향성 해결이 중요"라는 내용이 있음.
    - **사고:**
        1. '자율주행 로봇' -> `business_domains`은 '자율주행', '물류 테크'가 되겠군.
        2. '경로 탐색 알고리즘' -> `core_technologies`에 'SLAM'이나 '경로 탐색 알고리즘'을 추가해야겠다.
        3. '데이터 편향성 해결' -> `problem_solution_keywords`로 '설명가능 AI (XAI)'나 '알고리즘 윤리'를 떠올릴 수 있겠어.
        4. 이 직무는 -> `emerging_roles`로 '로보틱스 소프트웨어 엔지니어'라고 할 수 있겠다.
    - **결과:** 위 사고 과정을 거쳐 아래와 같은 JSON을 생성함.

    **좋은 예시 (Good Example):**
    ```json
    {{
        "core_technologies": ["Computer Vision", "Robotics", "SLAM"],
        "business_domains": ["Autonomous Vehicles", "Logistics Tech"],
        "emerging_roles": ["Robotics Software Engineer"],
        "problem_solution_keywords": ["Explainable AI (XAI)", "Algorithmic Ethics", "Pathfinding Algorithm"]
    }}
    """

    prompt_variable_inputs = {
    "market_analysis_json": combined_analysis_str
    }

    # 범용 함수를 사용하여 Global Trend 검색용 키워드 추출
    global_search_keywords = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=keyword_extraction_prompt_template,
    pydantic_model=GlobalSearchKeywords,
    llm=llm,
    log_message="-> Helper: Global Trend 검색용 키워드 추출 실행..."
    )

    # --- 6. 최종 반환 ---
    print("--- 국내 채용 시장 분석 완료 ---")
    return {
        "domestic_analysis_components": {
            "postings_analysis": postings_analysis,
            "reviews_analysis": reviews_analysis,
            "interviews_analysis": interviews_analysis
        },
        "domestic_keywords": global_search_keywords,
        "llm" : llm
    }

def global_trend_analysis_node(state: AgentState) -> dict:
    """
    국내 분석 키워드와 산업 분야를 바탕으로,
    기술/시장/리더십 트렌드를 분석하여 종합적인 글로벌 동향을 도출합니다.
    (구조화된 데이터 추출 방식으로 수정됨)
    """
    print("\n--- [Step 3] 글로벌 트렌드 분석 노드 실행 ---")

    # --- 1. 사전 준비 ---
    domestic_keywords = state["domestic_keywords"]
    target_job_title = ", ".join(state["target_job"])
    target_companies = state["target_company"]
    api_keys = state["api_keys"]
    one_year_ago_str = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

    # ----------------------------------------------------------------------------------

    # --- 2. 데이터 수집 (기술, 시장, 리더십) ---

    # 2a. 기술 트렌드 (Tavily)
    print("\n1. 기술 트렌드 데이터 수집 중...")
    tech_trends_results = ""
    SEARCH_QUERY_GENERATION_PROMPT = """
    You are an expert market researcher. Based on the following Korean keywords for a job role, generate 5 distinct and effective English search queries to research global and future trends.
    The queries should be concise and focus on future outlook, required skills, and market changes.
    Output your answer as a JSON array of strings. Do not include any other text.

    **Korean Keywords:** {keywords_str}
    **Job Title:** {job_title}
    """
    try:
        search_query_chain = ChatPromptTemplate.from_template(SEARCH_QUERY_GENERATION_PROMPT) | llm | JsonOutputParser()
        search_queries = search_query_chain.invoke({
            "keywords_str": json.dumps(domestic_keywords, ensure_ascii=False),
            "job_title": target_job_title
        })

        # 각 검색어에 'after:'를 추가하여 1년 이내로 시간 제한
        timed_search_queries = [f"{q} after:{one_year_ago_str}" for q in search_queries]
        print(f"-> 생성된 기술 트렌드 검색어: {timed_search_queries}")

        tech_trends_results_list = tavily_web_search.batch(timed_search_queries)
        tech_trends_results = "\n\n---\n\n".join([str(result) for result in tech_trends_results_list])
    except Exception as e:
        print(f"-> ⚠️ 기술 트렌드 검색 실패 ({e}).")
        tech_trends_results = "기술 트렌드 검색에 실패했습니다."

    # ----------------------------------------------------------------------------------

    # 2b. 시장/기업 트렌드 (News API)
    print("\n2. 시장/기업 트렌드 데이터 수집 중...")
    market_trends_results = ""
    if target_companies:
        try:
            # 산업 분야 추론
            main_company = target_companies[0]
            industry_name_prompt = f"The company '{main_company}' primarily operates in which industry? Provide a concise, common English industry name (e.g., 'E-commerce', 'Semiconductor', 'Financial Services')."
            industry_name = llm.invoke(industry_name_prompt).content.strip()
            print(f"-> '{main_company}'의 핵심 산업 분야 추론: {industry_name}")

            # 한글 직무명을 영어로 번역
            korean_job_title = ", ".join(state["target_job"])
            translation_prompt = f"Translate the following Korean job titles into a concise, comma-separated English string: '{korean_job_title}'"
            english_job_titles_str = llm.invoke(translation_prompt).content.strip()
            print(f"-> 직무명 영문 번역 완료: {english_job_titles_str}")

            # 1. 각 직무명의 양쪽에 큰따옴표(")를 추가합니다.
            # -> ["Machine Learning Engineer", "Data Scientist"]
            job_titles_list = [job.strip() for job in english_job_titles_str.split(',')]
            quoted_job_titles = [f'"{job}"' for job in job_titles_list]

            # 2. 큰따옴표로 묶인 직무명들을 " OR "로 연결합니다.
            # -> "Machine Learning Engineer" OR "Data Scientist"
            or_separated_jobs = " OR ".join(quoted_job_titles)

            # 3. 최종적으로 양쪽을 괄호()로 감싸줍니다.
            # -> ("Machine Learning Engineer" OR "Data Scientist")
            job_titles_query_part = f'({or_separated_jobs})'

            # 재구성된 쿼리를 사용하여 최종 검색어를 만듭니다.
            news_query = f'"{industry_name}" AND {job_titles_query_part} AND (hiring OR skill OR future OR trend)'
            print(f"-> News API 검색어: {news_query}")

            market_trends_results = search_global_news.invoke({
                "query": news_query,
                "from_date": one_year_ago_str
            })
        except Exception as e:
            print(f"-> ⚠️ 시장/기업 트렌드 검색 실패 ({e}).")
            market_trends_results = "시장/기업 트렌드 검색에 실패했습니다."
    else:
        print("-> 희망 기업이 지정되지 않아 시장/기업 트렌드 분석을 건너뜁니다.")

    # ----------------------------------------------------------------------------------

    # 2c. 리더십 트렌드 (YouTube)
    print("\n3. 리더십(권위자 비전) 트렌드 데이터 수집 중...")
    vision_analysis_summary = ""

    if target_companies:
        try :
            main_company = target_companies[0]
            key_figures_prompt = """
            '{main_company}'가 속한 산업 또는 '{target_job_title}' 분야에서 가장 영향력 있는 글로벌 리더(CEO, 연구원, 구루 등)의 이름을 20명만 알려줘.

            **지시사항:**
            - 주로 **주요 글로벌 기술 컨퍼런스(예: NVIDIA GTC, Google I/O), TED, 공식적인 대학 강연** 등에서 발표하여, **공식적으로 녹화되고 양질의 자막이 제공될 가능성이 높은 인물**을 우선적으로 추천해줘.
            - 유튜브에서 양질의 영어 자막과 함께 강연을 쉽게 찾을 수 있는 인물을 고려해줘.
            - 응답은 영문 이름으로 구성된 JSON 리스트 형식이어야 합니다. (예: ["Jensen Huang", "Satya Nadella", "Demis Hassabis", "Andrew Ng"])
            """
            key_figures_chain = ChatPromptTemplate.from_template(key_figures_prompt) | llm | JsonOutputParser()
            key_figures_list = key_figures_chain.invoke({
            "main_company": main_company,
            "target_job_title": target_job_title
            })

            # 각 인물별로 따로따로 유튜브 분석을 실행하고 결과를 합칩니다.
            analysis_results = []
            for figure_name in key_figures_list:
                # 각 인물에 대한 검색 주제와 분석 프롬프트를 생성
                vision_topic = f'"{figure_name}" conference OR interview'
                vision_prompt = f"'{figure_name}'의 강연 내용을 바탕으로, '{target_job_title}' 직무와 관련된 미래 비전, 기술 철학, 그리고 업계에 던지는 핵심 메시지를 요약해줘."

                # 각 인물별로 영상 1개만 분석하여 결과 리스트에 추가
                summary = analyze_youtube_topic(
                    topic=vision_topic,
                    analysis_prompt=vision_prompt,
                    api_key=api_keys.youtube_api_key,
                    lang_code='en',
                    max_results=20,  # 인물당 3개 후보 검색
                    num_to_analyze=1, # 그 중 1개만 분석
                    transcripts_only=True
                )
                analysis_results.append(summary)

            # 개별 분석 결과를 최종적으로 하나로 합침
            vision_analysis_summary = "\n\n---\n\n".join(analysis_results)

        except Exception as e:
            print(f"-> ⚠️ 권위자 비전 분석 실패 ({e}).")
            vision_analysis_summary = "권위자 비전 분석에 실패했습니다."
    else:
        vision_analysis_summary = "희망 기업이 지정되지 않아 리더십 트렌드 분석을 건너뜁니다."

    # --- 3. 모든 정보 종합 및 최종 분석 ---
    print("\n4. 모든 정보 종합 및 최종 트렌드 분석 중...")
    combined_results = f"""
    --- Technical Trends (from Keyword Search) ---
    {tech_trends_results}

    --- Market & Industry Trends (from News Search) ---
    {market_trends_results}

    --- Vision from Industry Leader (from YouTube) ---
    {vision_analysis_summary}
    """

    final_analysis_prompt_template = """
    **페르소나 (Persona):**
    당신은 수백 개의 기술 아티클, 시장 보고서, 리더 인터뷰를 분석하여 미래 트렌드를 예측하고 구직자에게 actionable insight를 제공하는 **글로벌 기술 전략가(Global Technology Strategist)**입니다.

    **임무 (Mission):**
    주어진 3가지 종류의 글로벌 트렌드 데이터({search_results})를 종합적으로 분석하여, {target_job_title} 직무 지원자가 반드시 알아야 할 핵심 동향을 {format_instructions}에 명시된 JSON 형식으로 추출합니다.

    **규칙 (Strict Rules):**
    1.  **종합적 분석:** 세 가지 데이터 소스(Technical, Market, Vision)의 내용을 모두 종합하여 인사이트를 도출해야 합니다.
    2.  **구체성:** 'AI의 발전'과 같은 막연한 표현 대신, '코드 생성을 위한 생성형 AI(Generative AI for Code Generation)'처럼 구체적인 기술과 용도를 명시해야 합니다.
    3.  **구직자 관점:** 모든 분석은 최종적으로 '그래서 구직자가 무엇을 준비해야 하는가?'라는 관점에서 정리되어야 합니다.

    **좋은 예시 (Good Example):**
    ```json
    {{
      "key_technology_shifts": ["Generative AI for Code Generation", "MLOps for Production", "Vector Databases"],
      "changing_market_demands": ["Prompt Engineering skills required", "Experience with large-scale distributed training"]
    }}
    ```

    ---
    이제 위 규칙과 예시에 따라 분석을 시작하세요.
    """

    # 새로운 프롬프트와 Pydantic 모델을 사용하여 extract_structured_data_flexible 호출
    prompt_variable_inputs = {
        "target_job_title": target_job_title,
        "search_results": combined_results
    }

    global_trends_analysis = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=final_analysis_prompt_template,
        pydantic_model=GlobalTrendsOutput,
        llm=llm,
        log_message="-> Helper: 최종 글로벌 트렌드 분석 및 구조화 실행..."
    )

    return {
        "global_trends": global_trends_analysis,
        "llm": llm
    }

def gap_analysis_node(state: AgentState) -> dict:
    """
    사용자 프로필과 국내외 시장 분석 결과를 종합하여
    사용자의 강점, 약점, 기회를 분석합니다. (사용자 프로필 처리 로직 강화)
    """
    print("\n--- [Step 4] 사용자 프로필 및 시장 요구사항 갭 분석 노드 실행 ---")

    # --- 1. 사전 준비 ---
    user_profile = state["user_profile"]
    domestic_analysis = state["domestic_analysis_components"]
    global_trends = state["global_trends"]
    # llm = state["llm"]
    state['llm'] = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    llm = state['llm']

    user_profile_str = json.dumps(user_profile, ensure_ascii=False, indent=2)
    market_analysis_str = json.dumps({
        "domestic_analysis": domestic_analysis,
        "global_trends": global_trends
    }, ensure_ascii=False, indent=2)

    # --- 2. 프롬프트 엔지니어링 ---
    gap_analysis_prompt_template = """
    **페르소나 (Persona):**
    당신은 지원자의 프로필과 시장 분석 데이터를 비교하여 강점, 약점, 기회를 날카롭게 진단하는 전문 HR 컨설턴트이자 커리어 코치입니다.

    **임무 (Mission):**
    아래에 주어진 [사용자 프로필]과 [시장 분석 데이터]를 종합적으로 비교 분석하여, {format_instructions}에 명시된 JSON 형식으로 진단 결과를 생성합니다.

    **규칙 (Strict Rules):**
    1.  **데이터 기반 분석:** 반드시 주어진 두 데이터에 근거하여 객관적으로 분석해야 합니다. 추측이나 일반적인 조언은 금지입니다.
    2.  **강점(Strengths) 정의:** [사용자 프로필]에 있고, [시장 분석 데이터]에서도 중요하게 요구하는 역량 또는 경험이어야 합니다.
    3.  **약점(Weaknesses) 정의:** [시장 분석 데이터]에서는 중요하게 요구하지만, [사용자 프로필]에는 명확하게 보이지 않는 역량 또는 경험이어야 합니다.
    4.  **기회(Opportunities) 정의:** 사용자의 '강점'을 '글로벌 트렌드'와 결합했을 때, 미래에 더 큰 경쟁력을 가질 수 있는 새로운 방향성을 의미합니다.
    5.  **언어 통일:** 모든 결과는 반드시 한국어로 작성하시오.

    ---
    **[사용자 프로필]**
    {user_profile_str}
    ---
    **[시장 분석 데이터]**
    {market_analysis_str}
    ---

    이제 위 두 정보를 바탕으로 분석을 시작하세요.
    """

    # --- 3. 헬퍼 함수 호출 ---
    prompt_variable_inputs = {
        "user_profile_str": user_profile_str,
        "market_analysis_str": market_analysis_str
    }

    gap_analysis_result = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=gap_analysis_prompt_template,
        pydantic_model=GapAnalysisOutput,
        llm=llm,
        log_message="-> Helper: 사용자 강점, 약점, 기회 분석 실행..."
    )

    return {
        "gap_analysis": gap_analysis_result
    }

"""# 수행 예시"""

import pprint
from langgraph.graph import StateGraph, END
from googleapiclient.discovery import build
from langchain_openai import ChatOpenAI

if __name__ == '__main__':
    # --- 1. 준비 단계 ---

    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("✅ YouTube 서비스 객체 생성 완료.")
    except Exception as e:
        print(f"🔴 API 키 또는 서비스 객체 생성 실패: {e}")
        exit()

    # --- 2. LangGraph 워크플로우 생성 및 구성 ---
    workflow = StateGraph(AgentState)

    # [수정] 4개의 노드를 모두 추가합니다.
    workflow.add_node("user_profiling", user_profiling_node)
    workflow.add_node("domestic_analysis", domestic_job_analysis_node)
    workflow.add_node("global_trends", global_trend_analysis_node)
    workflow.add_node("gap_analysis", gap_analysis_node) # <-- 신규 노드 추가

    # [수정] user_profiling -> domestic_analysis -> global_trends -> gap_analysis 순서로 실행 흐름을 정의합니다.
    workflow.set_entry_point("user_profiling")
    workflow.add_edge("user_profiling", "domestic_analysis")
    workflow.add_edge("domestic_analysis", "global_trends")
    workflow.add_edge("global_trends", "gap_analysis") # <-- 신규 연결선 추가
    workflow.add_edge("gap_analysis", END)             # <-- 마지막 노드를 gap_analysis로 변경

    app = workflow.compile()

    # --- 3. 테스트를 위한 초기 데이터 정의 ---
    user_input = {
        "목표 직무": "AI 관련",
        "희망 기업": ["네카라쿠배"],
        "학년/학기": "3학년 2학기",
        "전공 및 복수(부)전공": "전공 : 통계학과, 복수전공 : 컴퓨터학부",
        "보유 기술 및 자격증": "SQL, Python(scikit-learn, Pytorch) 기술 보유 및 SQLD, 사회조사분석사 자격증 보유",
        "관련 경험 및 스펙" : "XGBoost, SVM을 이용한 Human vs AI 코드 분류 모델 개발 프로젝트, SAINT 모델 활용하여 전력소비량 예측 모델 개발",
        "고민 또는 궁금한 점": "겨울방학에 어떤 공부나 기술 프로젝트를 해보면 좋을까요?"
    }

    # llm 객체 생성
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm
    }

    # --- 4. 그래프 실행 및 중간 과정 확인 ---
    print("\n🚀 전체 에이전트 실행 시작 (스트림 모드)")
    print("="*60)

    final_state = {}
    for state_update in app.stream(initial_state):
        node_name = list(state_update.keys())[0]
        node_output = state_update.get(node_name)

        print(f"\n--- 📌 [노드: {node_name}] 실행 완료 ---")
        pprint.pprint(node_output)

        if node_output is not None:
            final_state.update(node_output)

    print("\n\n✅ 전체 에이전트 실행 완료!")

    # --- 5. 최종 결과물 종합 출력 ---
    # 각 노드에서 반환한 키를 사용하여 최종 상태에서 데이터를 추출합니다.
    user_profile = final_state.get('user_profile', {})
    domestic_components = final_state.get('domestic_analysis_components', {})
    global_trends = final_state.get('global_trends', {})
    gap_analysis = final_state.get('gap_analysis', {}) # <-- 신규 결과 추출

    pprint.pprint(user_profile)

    # 국내 시장 분석 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 📂 국내 시장 구조화된 분석 결과 >>>")
    print("="*60)
    pprint.pprint(domestic_components)

    # 글로벌 트렌드 분석 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 🌏 글로벌 트렌드 분석 결과 >>>")
    print("="*60)
    pprint.pprint(global_trends)

    # [신규] 사용자 최종 진단 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 📊 사용자 최종 진단 결과 >>>")
    print("="*60)
    pprint.pprint(gap_analysis)
    print("="*60)

import pprint
from langgraph.graph import StateGraph, END
from googleapiclient.discovery import build
from langchain_openai import ChatOpenAI

if __name__ == '__main__':
    # --- 1. 준비 단계 ---

    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("✅ YouTube 서비스 객체 생성 완료.")
    except Exception as e:
        print(f"🔴 API 키 또는 서비스 객체 생성 실패: {e}")
        exit()

    # --- 2. LangGraph 워크플로우 생성 및 구성 ---
    workflow = StateGraph(AgentState)

    # [수정] 4개의 노드를 모두 추가합니다.
    workflow.add_node("user_profiling", user_profiling_node)
    workflow.add_node("domestic_analysis", domestic_job_analysis_node)
    workflow.add_node("global_trends", global_trend_analysis_node)
    workflow.add_node("gap_analysis", gap_analysis_node) # <-- 신규 노드 추가

    # [수정] user_profiling -> domestic_analysis -> global_trends -> gap_analysis 순서로 실행 흐름을 정의합니다.
    workflow.set_entry_point("user_profiling")
    workflow.add_edge("user_profiling", "domestic_analysis")
    workflow.add_edge("domestic_analysis", "global_trends")
    workflow.add_edge("global_trends", "gap_analysis") # <-- 신규 연결선 추가
    workflow.add_edge("gap_analysis", END)             # <-- 마지막 노드를 gap_analysis로 변경

    app = workflow.compile()

    # --- 3. 테스트를 위한 초기 데이터 정의 ---
    user_input = {
        "목표 직무": "은행 또는 증권",
        "희망 기업": ["금융공기업"],
        "학년/학기": "2학년 2학기",
        "전공 및 복수(부)전공": "전공 : 경영학부",
        "보유 기술 및 자격증": "외부 기업분석 공모전 금상 및 모의주식투자 대회 최우수상, 신용분석사 및 투자운용사 자격증 보유",
        "관련 경험 및 스펙" : "은행 서포터즈 활동 6개월",
        "고민 또는 궁금한 점": "어떤 스펙을 더 쌓는게 좋을까?"
    }

    # llm 객체 생성
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm
    }

    # --- 4. 그래프 실행 및 중간 과정 확인 ---
    print("\n🚀 전체 에이전트 실행 시작 (스트림 모드)")
    print("="*60)

    final_state = {}
    for state_update in app.stream(initial_state):
        node_name = list(state_update.keys())[0]
        node_output = state_update.get(node_name)

        print(f"\n--- 📌 [노드: {node_name}] 실행 완료 ---")
        pprint.pprint(node_output)

        if node_output is not None:
            final_state.update(node_output)

    print("\n\n✅ 전체 에이전트 실행 완료!")

    # --- 5. 최종 결과물 종합 출력 ---
    # 각 노드에서 반환한 키를 사용하여 최종 상태에서 데이터를 추출합니다.
    user_profile = final_state.get('user_profile', {})
    domestic_components = final_state.get('domestic_analysis_components', {})
    global_trends = final_state.get('global_trends', {})
    gap_analysis = final_state.get('gap_analysis', {}) # <-- 신규 결과 추출

    pprint.pprint(user_profile)

    # 국내 시장 분석 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 📂 국내 시장 구조화된 분석 결과 >>>")
    print("="*60)
    pprint.pprint(domestic_components)

    # 글로벌 트렌드 분석 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 🌏 글로벌 트렌드 분석 결과 >>>")
    print("="*60)
    pprint.pprint(global_trends)

    # [신규] 사용자 최종 진단 결과 출력
    print("\n\n" + "="*60)
    print("           <<< 📊 사용자 최종 진단 결과 >>>")
    print("="*60)
    pprint.pprint(gap_analysis)
    print("="*60)
