# -*- coding: utf-8 -*-
"""report_version2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PF-_W9sAUP6YeB_dboSCzPG9BRQPIrKF
"""

!pip install -q openai==1.101.0 langchain==0.3.27 langchain-core==0.3.74 langchain-openai==0.3.30 langchain-community==0.3.27 pytube==15.0.0 tavily-python==0.7.11 youtube-search-python==1.6.6 langgraph==0.6.4 arxiv==2.2.0 pymupdf==1.26.3 youtube_transcript_api==1.2.2

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')

# %cd /content/drive/MyDrive/ICT_í”„ë¡œì íŠ¸

import json
from datetime import datetime, timedelta  # ë‚ ì§œ ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€

# OpenAI & LangChain
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser, PydanticOutputParser

# LangChain Community Tools & Loaders
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import ArxivLoader
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# LangGraph
from langgraph.graph import StateGraph, END

# Typing
from typing import TypedDict, List, Dict, Any, Optional, Type

# Tools
from tools import analyze_video_content, search_naver_news, search_global_news, search_arxiv_papers, tavily_web_search, find_videos_with_transcripts, analyze_youtube_topic

import os
from google.colab import userdata

os.environ['OPENAI_API_KEY'] = userdata.get('ssu')
os.environ['TAVILY_API_KEY'] = userdata.get('tavily')
os.environ['YOUTUBE_API_KEY'] = userdata.get('youtube')
os.environ['NAVER_CLIENT_ID'] = userdata.get('naver_client_id')
os.environ['NAVER_CLIENT_SECRET'] = userdata.get('naver_client_secret')
os.environ['NEWS_API_KEY'] = userdata.get('news_api_key')

class AgentAPIs:
    """
    ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  API í‚¤ë¥¼ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        print("-> API í‚¤ ë¡œë”© ì‹œì‘...")

        # userdataì—ì„œ ê° API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        self.openai_api_key = userdata.get('ssu')
        self.tavily_api_key = userdata.get('tavily')
        self.youtube_api_key = userdata.get('youtube')
        self.naver_client_id = userdata.get('naver_client_id')
        self.naver_client_secret = userdata.get('naver_client_secret')
        self.news_api_key = userdata.get('news_api_key')

        # í•„ìˆ˜ í‚¤ë“¤ì´ ëª¨ë‘ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if not all([self.openai_api_key, self.tavily_api_key, self.youtube_api_key]):
            raise ValueError("í•„ìˆ˜ API í‚¤(OpenAI, Tavily, YouTube)ê°€ Colab Secretsì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print("âœ… ëª¨ë“  API í‚¤ ë¡œë”© ì™„ë£Œ.")

    def set_env_variables(self):
        """
        ë¡œë“œëœ í‚¤ë“¤ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´)
        """
        print("-> í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì¤‘...")
        os.environ['OPENAI_API_KEY'] = self.openai_api_key
        os.environ['TAVILY_API_KEY'] = self.tavily_api_key
        os.environ['YOUTUBE_API_KEY'] = self.youtube_api_key
        os.environ['NAVER_CLIENT_ID'] = self.naver_client_id
        os.environ['NAVER_CLIENT_SECRET'] = self.naver_client_secret
        os.environ['NEWS_API_KEY'] = self.news_api_key
        print("âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ.")

class UserProfile(TypedDict):
    academic_year: Optional[str]
    major: Optional[str]
    skills_and_certs: Optional[str]
    experience_specs: Optional[str]
    goals: Optional[str]
    narrative_summary: str

class GlobalTrends(TypedDict):
    generated_keywords: str
    prediction: str
    distilled_prediction: str

class AcademicResearch(TypedDict):
    summary: str
    distilled_summary: str

class StorytellingStrategy(TypedDict):
    experience_plan: str
    storytelling_example: str

class AgentState(TypedDict):
    user_profile_raw: Dict[str, Any]  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì •ë³´

    # --- user_profiling_nodeì˜ ê²°ê³¼ ---
    user_profile: UserProfile
    target_job: List[str]
    target_company: List[str]
    user_questions: Optional[str]

    # --- domestic_job_analysis_nodeì˜ ê²°ê³¼ ---
    # ë” ì„¸ë¶„í™”ëœ ì •ë³´ë¥¼ ì§ì ‘ ì €ì¥í•©ë‹ˆë‹¤.
    domestic_analysis_components: Optional[Dict[str, str]]
    domestic_keywords: Optional[Dict[str, Any]]

    global_trends: Optional[Any] # GlobalTrends
    academic_research: Optional[Any] # AcademicResearch
    final_report: Optional[str]
    api_keys: Optional[AgentAPIs]
    gap_analysis: Optional[Dict[str, Any]]

class RefinedCompanies(BaseModel):
    companies: List[str] = Field(description="ì •ì œëœ ê°œë³„ ê³µì‹ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸")

class PostingAnalysisOutput(BaseModel):
    """ì±„ìš© ê³µê³  ë¶„ì„ ê²°ê³¼ ëª¨ë¸"""
    role_goal: str = Field(description="ì´ ì§ë¬´ê°€ ë‹¬ì„±í•´ì•¼ í•  ì •ëŸ‰ì /ì •ì„±ì  ëª©í‘œ")
    key_responsibilities: List[str] = Field(description="ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì£¼ìš” ì±…ì„ ë¦¬ìŠ¤íŠ¸")
    hard_skills: List[str] = Field(description="í•„ìˆ˜ì ì¸ íˆ´ê³¼ ê¸°ìˆ ëª… ë¦¬ìŠ¤íŠ¸")
    collaboration_process: str = Field(description="ì–´ë–¤ ë™ë£Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í˜‘ì—…í•˜ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”„ë¡œì„¸ìŠ¤")
    preferred_experiences: List[str] = Field(description="ì„ í˜¸í•˜ëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ ë¦¬ìŠ¤íŠ¸")

class ReviewAnalysisOutput(BaseModel):
    """
    ëª¨ë“  ì§êµ°ì˜ 'í•©ê²© í›„ê¸°'ì—ì„œ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì¶”ì¶œ ëª¨ë¸.
    'ì–´ë–»ê²Œ í•©ê²©í–ˆëŠ”ê°€?'ë¼ëŠ” ì±„ìš© í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ë³„ í•µì‹¬ ì „ëµì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.
    """
    document_strategy: List[str] = Field(
        description="ì„œë¥˜ ì „í˜•(ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ) í†µê³¼ë¥¼ ìœ„í•´ í•©ê²©ìê°€ ê°•ì¡°í–ˆë˜ í•µì‹¬ ê²½í—˜, ì—­ëŸ‰, ë˜ëŠ” ì‘ì„± ì „ëµ"
    )
    test_strategy: List[str] = Field(
        description="ì¸ì ì„±, NCS, ë…¼ìˆ , ì½”ë”© í…ŒìŠ¤íŠ¸, ê³¼ì œ ë“± í•„ê¸°/ê³¼ì œ ì „í˜•ì˜ ì¢…ë¥˜ì™€ êµ¬ì²´ì ì¸ ì¤€ë¹„ ë°©ë²• ë˜ëŠ” íŒ"
    )
    job_interview_strategy: List[str] = Field(
        description="1ì°¨ ë©´ì ‘(ì‹¤ë¬´ì§„ ë©´ì ‘)ì—ì„œ ì§ë¬´ ì—­ëŸ‰ì„ ì–´í•„í•˜ê¸° ìœ„í•´ í•©ê²©ìê°€ ì‚¬ìš©í•œ ì „ëµì´ë‚˜ ë°›ì•˜ë˜ í•µì‹¬ ì§ˆë¬¸"
    )
    final_interview_strategy: List[str] = Field(
        description="ìµœì¢… ë©´ì ‘(ì„ì› ë©´ì ‘)ì—ì„œ ì¸ì„±ì´ë‚˜ ì¡°ì§ ì í•©ì„±ì„ ì–´í•„í•˜ê¸° ìœ„í•´ í•©ê²©ìê°€ ì‚¬ìš©í•œ ì „ëµì´ë‚˜ ë°›ì•˜ë˜ í•µì‹¬ ì§ˆë¬¸"
    )
    critical_success_factor: str = Field(
        description="í•©ê²©ìê°€ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ëŠ”, í•©ê²©ì— ê°€ì¥ ê²°ì •ì ì´ì—ˆë˜ ìì‹ ë§Œì˜ ì°¨ë³„í™” í¬ì¸íŠ¸ (ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ í•œ ê°€ì§€)"
    )

class InterviewAnalysisOutput(BaseModel):
    """
    ëª¨ë“  ì§êµ°ì˜ í˜„ì§ì ì¸í„°ë·°ì—ì„œ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì¶”ì¶œ ëª¨ë¸
    """
    core_competencies_and_tools: List[str] = Field(
        description="í˜„ì§ìê°€ ê°•ì¡°í•˜ëŠ” í•´ë‹¹ ì§ë¬´ì˜ í•µì‹¬ ì—­ëŸ‰ê³¼, ì—…ë¬´ì— ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” í•„ìˆ˜ íˆ´(ì˜ˆ: MS Office, Slack, Jira, Salesforce, Figma, Adobe Photoshop) ë“±"
    )
    team_culture_and_workflow: str = Field(
        description="íŒ€ì˜ ì˜ì‚¬ì†Œí†µ ë°©ì‹, íšŒì˜ ë¬¸í™”, í˜‘ì—… í”„ë¡œì„¸ìŠ¤, ì„±ê³¼ í‰ê°€ ë°©ì‹ ë“± êµ¬ì²´ì ì¸ ì—…ë¬´ ìŠ¤íƒ€ì¼ê³¼ ì¡°ì§ ë¬¸í™”"
    )
    growth_and_career_path: str = Field(
        description="íšŒì‚¬ì—ì„œ ì œê³µí•˜ëŠ” êµìœ¡, ë©˜í† ë§ ì œë„ë‚˜ í˜„ì§ìê°€ ë§í•˜ëŠ” í˜„ì‹¤ì ì¸ ì»¤ë¦¬ì–´ ì„±ì¥ ê²½ë¡œ"
    )
    advice_for_applicants: List[str] = Field(
        description="í˜„ì§ìê°€ í•´ë‹¹ ì§ë¬´ ì§€ì›ìì—ê²Œ 'ì´ê²ƒë§Œì€ ê¼­ ì¤€ë¹„í•˜ë¼'ê³  ì¡°ì–¸í•˜ëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ì´ë‚˜ ì—­ëŸ‰"
    )

class GlobalSearchKeywords(BaseModel):
    """
    ê¸€ë¡œë²Œ íŠ¸ë Œë“œë¥¼ ë‹¤ê°ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í‚¤ì›Œë“œ ëª¨ë¸
    """
    core_technologies: List[str] = Field(
        description="ë¶„ì„ ê²°ê³¼ì˜ í•µì‹¬ì´ ë˜ëŠ” ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œ (ì˜ˆ: Deep Learning, Computer Vision)"
    )
    business_domains: List[str] = Field(
        description="ì–¸ê¸‰ëœ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ë° ì‚°ì—… ë„ë©”ì¸ í‚¤ì›Œë“œ (ì˜ˆ: Fintech, Robotics)"
    )
    emerging_roles: List[str] = Field(
        description="ìƒˆë¡­ê²Œ ë¶€ìƒí•˜ê±°ë‚˜ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§€ëŠ” ì§ë¬´ëª… (ì˜ˆ: Prompt Engineer, AI Ethicist, MLOps Specialist)"
    )
    problem_solution_keywords: List[str] = Field(
        description="ê¸°ìˆ ì´ í•´ê²°í•˜ë ¤ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œë‚˜ ì†”ë£¨ì…˜ í‚¤ì›Œë“œ (ì˜ˆ: Fraud Detection, Hyper-personalization, Digital Twin, ESG)"
    )

class GlobalTrendsOutput(BaseModel):
    """
    ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ê¸° ìœ„í•œ ëª¨ë¸
    """
    future_outlook: str = Field(
        description="ì´ ì§ë¬´ì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±ì— ëŒ€í•œ ë¯¸ë˜ ì „ë§ ì¢…í•© ë¶„ì„ (1-2 ë¬¸ì¥ ìš”ì•½)"
    )
    key_technology_shifts: List[str] = Field(
        description="í˜„ì¬ ë– ì˜¤ë¥´ê±°ë‚˜, ì•ìœ¼ë¡œ í•„ìˆ˜ê°€ ë  êµ¬ì²´ì ì¸ ìµœì‹  ê¸°ìˆ , íˆ´, í”Œë«í¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"
    )
    changing_market_demands: List[str] = Field(
        description="ê¸°ì—…ë“¤ì´ ì´ ì§ë¬´ì— ëŒ€í•´ ìƒˆë¡­ê²Œ ìš”êµ¬í•˜ê¸° ì‹œì‘í•œ êµ¬ì²´ì ì¸ ì—­ëŸ‰ì´ë‚˜ ê²½í—˜ ë¦¬ìŠ¤íŠ¸"
    )
    key_messages_from_leaders: str = Field(description="ì—…ê³„ ë¦¬ë”ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê°•ì¡°í•˜ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ë©”ì‹œì§€ë‚˜ ì¸ìš©êµ¬ (1-2ê°œ)")

class GapAnalysisOutput(BaseModel):
    """
    ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ê°•ì , ì•½ì , ê¸°íšŒë¥¼ ì§„ë‹¨í•˜ëŠ” ëª¨ë¸
    """
    strengths: List[str] = Field(
        description="ì‚¬ìš©ìê°€ í˜„ì¬ ë³´ìœ í•œ ì—­ëŸ‰/ê²½í—˜ ì¤‘, ì‹œì¥ì˜ ìš”êµ¬ì‚¬í•­ ë° íŠ¸ë Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ëª…í™•í•œ ê°•ì  ë¦¬ìŠ¤íŠ¸"
    )
    weaknesses: List[str] = Field(
        description="ì‹œì¥ì˜ ìš”êµ¬ì‚¬í•­ ë° íŠ¸ë Œë“œì— ë¹„í•´ ì‚¬ìš©ìê°€ ëª…ë°±íˆ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ëŠ” ì—­ëŸ‰/ê²½í—˜ ë¦¬ìŠ¤íŠ¸ (ë³´ì™„ì )"
    )
    opportunities: List[str] = Field(
        description="ì‚¬ìš©ìì˜ ê°•ì ì„ ê¸€ë¡œë²Œ íŠ¸ë Œë“œì™€ ì—°ê²°í•˜ì—¬, ê²½ìŸë ¥ì„ í•œ ë‹¨ê³„ ë” ë†’ì¼ ìˆ˜ ìˆëŠ” ê¸°íšŒ ì˜ì—­ ë¦¬ìŠ¤íŠ¸"
    )
    summary: str = Field(
        description="ì‚¬ìš©ìì˜ í˜„ì¬ ìƒíƒœì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ë° ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•œ ë°©í–¥ì„± ìš”ì•½ (2-3 ë¬¸ì¥)"
    )

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜

def extract_structured_data_flexible(
    prompt_inputs: Dict[str, Any],
    extraction_prompt_template: str,
    pydantic_model: Type[BaseModel],
    llm: Any,
    log_message: str
) -> Dict[str, Any]:
    """
    ì£¼ì–´ì§„ 'ì–´ë–¤' ì…ë ¥ ë³€ìˆ˜(prompt_inputs)ì™€ Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë” ìœ ì—°í•œ ë²”ìš© í—¬í¼ í•¨ìˆ˜.
    """
    print(log_message)

    try:
        parser = PydanticOutputParser(pydantic_object=pydantic_model)

        prompt = ChatPromptTemplate.from_template(
            template=extraction_prompt_template,
            partial_variables={"format_instructions": parser.get_format_instructions()},
        )

        extractor_chain = prompt | llm | parser
        # [ìˆ˜ì •] ì¸ìë¡œ ë°›ì€ prompt_inputs ë”•ì…”ë„ˆë¦¬ë¥¼ ê·¸ëŒ€ë¡œ invokeì— ì „ë‹¬
        extracted_data = extractor_chain.invoke(prompt_inputs)
        return extracted_data.model_dump()

    except Exception as e:
        print(f"-> âš ï¸ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return {}

def user_profiling_node(state: AgentState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì •ë³´ë¥¼ ì •ì œ/êµ¬ì¡°í™”í•˜ê³ ,
    'ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì 'ì„ ë³„ë„ì˜ í‚¤ë¡œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 0] ì‚¬ìš©ì ì •ë³´ êµ¬ì¡°í™” ë…¸ë“œ ì‹¤í–‰ ---")

    user_info = state["user_profile_raw"]

    # key_to_label ì •ì˜ ë° profile_text ìƒì„±
    key_to_label = {
        "ëª©í‘œ ì§ë¬´": "ëª©í‘œ ì§ë¬´",
        "í¬ë§ ê¸°ì—…": "í¬ë§ ê¸°ì—…",
        "í•™ë…„/í•™ê¸°": "í•™ë…„/í•™ê¸°",
        "ì¬í•™ ì—¬ë¶€": "ì¬í•™ ì—¬ë¶€",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ê³µ",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™",
        "ê´€ì‹¬ ë¶„ì•¼ ë° ëª©í‘œ": "ê´€ì‹¬ ë¶„ì•¼ ë° ëª©í‘œ",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì "
    }
    target_job = user_info.get("ëª©í‘œ ì§ë¬´", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
    target_company = user_info.get("í¬ë§ ê¸°ì—…", [])
    profile_parts = [f"- {label}: {user_info.get(key)}" for key, label in key_to_label.items() if user_info.get(key) and str(user_info.get(key)).strip()]
    profile_text = "\n".join(profile_parts) if profile_parts else "ì…ë ¥ëœ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

    prompt_template = """
    ë‹¹ì‹ ì€ í•™ìƒ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ íŒŒì‹±í•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ [í•™ìƒ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê° í•­ëª©ì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ìœ íš¨í•œ JSON ê°ì²´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì‘ë‹µì€ ì˜¤ì§ JSON ê°ì²´ë§Œì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

    ---
    [í•™ìƒ ì •ë³´]
    {profile_text}
    ---

    **[ë¶„ì„ ë° ì¶”ì¶œ ìš”ì²­]**
    1.  **`skills_and_certs`**: 'ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦' í•­ëª©ì—ì„œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´, íˆ´, ìê²©ì¦ ì´ë¦„ ë“± í•µì‹¬ ê¸°ìˆ  ì—­ëŸ‰ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    2.  **`experience_specs`**: 'ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™' í•­ëª©ì—ì„œ í”„ë¡œì íŠ¸, ì¸í„´, ìˆ˜ìƒ ê²½ë ¥ ë“± êµ¬ì²´ì ì¸ ê²½í—˜ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    3.  ë‚˜ë¨¸ì§€ í•„ë“œ(`academic_year`, `major`, `goals`)ë„ ê°ê° í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì±„ì›Œì£¼ì„¸ìš”. ì •ë³´ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‘ì„¸ìš”.
    4.  **`narrative_summary`**: ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ '{target_job}' ì§ë¬´ ëª©í‘œì— ë§ì¶° 2-3 ë¬¸ì¥ì˜ ìì—°ì–´ í”„ë¡œí•„ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

    **[JSON ì¶œë ¥ í˜•ì‹]**
    {{
        "academic_year": "...",
        "major": "...",
        "skills_and_certs": "...",
        "experience_specs": "...",
        "goals": "...",
        "narrative_summary": "..."
    }}
    """

    parser = JsonOutputParser(pydantic_object=UserProfile)
    prompt = ChatPromptTemplate.from_template(template=prompt_template)
    profiling_chain = prompt | llm | parser

    try:
        structured_profile = profiling_chain.invoke({
            "profile_text": profile_text,
            "target_job": target_job
        })
        print("-> êµ¬ì¡°í™”ëœ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"-> í”„ë¡œí•„ ìƒì„± ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}\n-> ëŒ€ì²´ í”„ë¡œí•„ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        structured_profile = {"narrative_summary": "ì‚¬ìš©ì ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

    # ì‚¬ìš©ìì˜ í•µì‹¬ ì§ˆë¬¸ì„ ë³„ë„ë¡œ ì¶”ì¶œ
    user_questions = user_info.get("ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ", "")
    print(f"-> ì¶”ì¶œëœ ì‚¬ìš©ì ì§ˆë¬¸: '{user_questions}'")

    original_target_job = user_info.get("ëª©í‘œ ì§ë¬´", "")
    refined_jobs = [original_target_job] if original_target_job else [] # ê¸°ë³¸ê°’ì€ ì›ë³¸ê°’ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸

    # --- ğŸ’¡ LLMì„ ì´ìš©í•œ ì§ë¬´ëª… êµ¬ì²´í™” ë¡œì§ ---
    if original_target_job:
        print(f"-> ëª©í‘œ ì§ë¬´ êµ¬ì²´í™” ì‹œì‘ (ì…ë ¥: '{original_target_job}')")

        # 1. ì§ë¬´ëª… êµ¬ì²´í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ ì •ì˜
        JOB_REFINEMENT_PROMPT = """
        ë‹¹ì‹ ì€ ëª¨ë“  ì‚°ì—… ë¶„ì•¼ì˜ ì§ë¬´ë¥¼ ê¹Šì´ ì´í•´í•˜ëŠ” ì „ë¬¸ ì»¤ë¦¬ì–´ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ 'ì…ë ¥ ì§ë¬´ëª…'ì´ ì¶”ìƒì ì´ê±°ë‚˜ í¬ê´„ì ì¸ ê²½ìš°, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì¶”ë¡ í•˜ì—¬ ì‹¤ì œ ì±„ìš© ì‹œì¥ì—ì„œ í†µìš©ë˜ëŠ” **ê°€ì¥ ëŒ€í‘œì ì´ê³  êµ¬ì²´ì ì¸ ì§ë¬´ëª… 2~3ê°œë¥¼ ì¶”ì²œ**í•´ì£¼ì„¸ìš”.

        **ì§€ì‹œì‚¬í•­:**
        - ë§Œì•½ ì…ë ¥ì´ ì´ë¯¸ êµ¬ì²´ì ì´ë¼ë©´, í•´ë‹¹ ì§ë¬´ëª… í•˜ë‚˜ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        - ì‘ë‹µì€ ì˜¤ì§ JSON ë¬¸ìì—´ ë°°ì—´(string array) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: ["ì§ë¬´ëª…1", "ì§ë¬´ëª…2"])
        - ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        **ì˜ˆì‹œ:**
        - ì…ë ¥: "ê°œë°œì"
        - ì¶œë ¥: ["ë°±ì—”ë“œ ê°œë°œì", "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì", "ì•± ê°œë°œì"]

        - ì…ë ¥: "ë§ˆì¼€íŒ…"
        - ì¶œë ¥: ["ì½˜í…ì¸  ë§ˆì¼€í„°", "í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€í„°", "ë¸Œëœë“œ ë§ˆì¼€í„°"]

        - ì…ë ¥: "ì¦ê¶Œ"
        - ì¶œë ¥: ["ì• ë„ë¦¬ìŠ¤íŠ¸", "IB (íˆ¬ìì€í–‰)", "PB (í”„ë¼ì´ë¹— ë±…ì»¤)"]

        - ì…ë ¥: "ë””ìì¸"
        - ì¶œë ¥: ["UI/UX ë””ìì´ë„ˆ", "BX ë””ìì´ë„ˆ", "ê·¸ë˜í”½ ë””ìì´ë„ˆ"]

        - ì…ë ¥: "íšŒê³„ì‚¬"
        - ì¶œë ¥: ["íšŒê³„ì‚¬"]

        ---
        ì…ë ¥ ì§ë¬´ëª…: {original_job}
        """

        try:
            # ì²´ì¸ êµ¬ì„± ë° í˜¸ì¶œ
            parser = JsonOutputParser()
            prompt = ChatPromptTemplate.from_template(JOB_REFINEMENT_PROMPT)
            refinement_chain = prompt | llm | parser

            refined_jobs = refinement_chain.invoke({"original_job": original_target_job})

            print(f"-> âœ¨ êµ¬ì²´í™”ëœ ì§ë¬´ ëª©ë¡: {refined_jobs}")

        except Exception as e:
            # LLM íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬
            print(f"-> âš ï¸ ì§ë¬´ëª… êµ¬ì²´í™” ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ì›ë³¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            refined_jobs = [original_target_job]
    else:
        # ëª©í‘œ ì§ë¬´ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        print("-> ëª©í‘œ ì§ë¬´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        refined_jobs = []

    original_target_companies = user_info.get("í¬ë§ ê¸°ì—…", [])
    refined_companies = []
    # --- ğŸ’¡ LLMì„ ì´ìš©í•œ ê¸°ì—…ëª… ì •ì œ ë¡œì§ ---
    if original_target_companies:
        print(f"-> í¬ë§ ê¸°ì—… ëª©ë¡ ì •ì œ ì‹œì‘ (ì…ë ¥: {original_target_companies})")

        # 1. í•¨ìˆ˜ ë‚´ì—ì„œ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ì„ ì§ì ‘ ì •ì˜
        COMPANY_REFINEMENT_PROMPT = """
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì£¼ìš” ê¸°ì—…ê³¼ ì‚°ì—…ë³„ ê·¸ë£¹/ìš©ì–´(ì˜ˆ: ë„¤ì¹´ë¼ì¿ ë°°, ê¸ˆìœµê³µê¸°ì—… Aë§¤ì¹˜)ë¥¼ ì •í™•íˆ ì´í•´í•˜ëŠ” ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ 'ì…ë ¥ ë¦¬ìŠ¤íŠ¸'ì— í¬í•¨ëœ íšŒì‚¬ëª… ë˜ëŠ” ê·¸ë£¹ëª…ì„ ì‹¤ì œ ì›¹ ê²€ìƒ‰ì— ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” **ê°œë³„ ê³µì‹ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸**ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

        **ì§€ì‹œì‚¬í•­:**
        - ê·¸ë£¹ëª…ì´ë‚˜ ì¶•ì•½ì–´ëŠ” í•´ë‹¹ ê·¸ë£¹ì— ì†í•œ ê°œë³„ ê¸°ì—…ëª…ìœ¼ë¡œ ëª¨ë‘ í’€ì–´ì£¼ì„¸ìš”.
        - ì´ë¯¸ ê°œë³„ ê¸°ì—…ëª…ì¸ ê²½ìš°, ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”.
        - ì‘ë‹µì€ ì˜¤ì§ JSON ë¬¸ìì—´ ë°°ì—´(string array) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        **ì˜ˆì‹œ:**
        - ì…ë ¥: ["ë„¤ì¹´ë¼ì¿ ë°°"]
        - ì¶œë ¥: ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ë¼ì¸", "ì¿ íŒ¡", "ë°°ë‹¬ì˜ë¯¼ì¡±"]

        - ì…ë ¥: ["Aë§¤ì¹˜ ê¸ˆìœµê³µê¸°ì—…"]
        - ì¶œë ¥: ["í•œêµ­ì€í–‰", "ê¸ˆìœµê°ë…ì›", "ì‚°ì—…ì€í–‰", "ìˆ˜ì¶œì…ì€í–‰", "ì˜ˆê¸ˆë³´í—˜ê³µì‚¬"]

        - ì…ë ¥: ["ì‚¼ì„±"]
        - ì¶œë ¥: ["ì‚¼ì„±ì „ì, ì‚¼ì„±SDI, ì‚¼ì„±ì¦ê¶Œ, ì‚¼ì„±ì¹´ë“œ, ì‚¼ì„±ë¬¼ì‚°"]

        ---
        ì…ë ¥ ë¦¬ìŠ¤íŠ¸: {company_list_str}
        """

        try:
            # 2. LLMì´ Pydantic ëª¨ë¸ì— ë§ì¶° ì¶œë ¥í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
            structured_llm = llm.with_structured_output(RefinedCompanies)
            prompt = ChatPromptTemplate.from_template(COMPANY_REFINEMENT_PROMPT)
            refinement_chain = prompt | structured_llm

            company_list_str = ", ".join(original_target_companies)

            # 3. ì²´ì¸ì„ í˜¸ì¶œí•˜ë©´ Pydantic ëª¨ë¸ ê°ì²´ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.
            result_model = refinement_chain.invoke({"company_list_str": company_list_str})
            refined_companies = result_model.companies # .companies ì†ì„±ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì ‘ê·¼

            print(f"-> âœ¨ ì •ì œëœ ê¸°ì—… ëª©ë¡: {refined_companies}")

        except Exception as e:
            print(f"-> âš ï¸ ê¸°ì—…ëª… ì •ì œ ì‹¤íŒ¨ (ì˜¤ë¥˜: {e}), ì›ë³¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            refined_companies = original_target_companies
    else:
        # 4. í¬ë§ ê¸°ì—…ì„ ì…ë ¥í•˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        print("-> í¬ë§ ê¸°ì—…ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 3. ë°˜í™˜ê°’ì˜ 'target_company'ì— **ì •ì œëœ ê¸°ì—… ë¦¬ìŠ¤íŠ¸**ë¥¼ ë‹´ì•„ì¤ë‹ˆë‹¤.
    return {
        "user_profile": structured_profile,
        "target_job": refined_jobs,
        "target_company": refined_companies,
        "user_questions": user_questions,
        "llm" : llm
    }

def domestic_job_analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    ì •ì œëœ ì§ë¬´ì™€ ê¸°ì—…ëª…ì„ ë°”íƒ•ìœ¼ë¡œ êµ­ë‚´ ì±„ìš© ì‹œì¥ì˜ ê° ì¸¡ë©´ì„ ë¶„ì„í•˜ê³  ì¢…í•©í•©ë‹ˆë‹¤.
    """
    print("\n--- [Step 2] êµ­ë‚´ ì±„ìš© ì‹œì¥ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    target_jobs_list = state["target_job"]
    refined_companies = state["target_company"]
    target_job_title = ", ".join(target_jobs_list)
    job_query = f'"{" OR ".join(target_jobs_list)}"' if target_jobs_list else ""
    company_query = f'"{" OR ".join(refined_companies)}"' if refined_companies else ""
    api_keys = state['api_keys']

    # --- 2. ì±„ìš© ê³µê³  ë¶„ì„ ---
    print("\n1. ì±„ìš© ê³µê³  ë¶„ì„ ì¤‘...")
    postings_web_results = tavily_web_search.invoke(
        " ".join(part for part in [company_query, job_query, "ì‹ ì… ì±„ìš© ê³µê³  ìê²©ìš”ê±´ ìš°ëŒ€ì‚¬í•­"] if part)
    )

    postings_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ 'ì¸ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤(Quoted Fact)'ë§Œì„ ì •í™•í•˜ê²Œ ë½‘ì•„ë‚´ëŠ” ì •ë³´ ì¶”ì¶œ AIì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì ˆëŒ€ ì¶”ë¡ í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    {target_job_title} ì§ë¬´ì˜ ì£¼ì–´ì§„ ì±„ìš© ê³µê³  ë‚´ìš©({search_results})ì—ì„œ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ì¶”ìƒì  í‘œí˜„ ê¸ˆì§€:** 'ì›í™œí•œ ì†Œí†µ', 'ì ê·¹ì ì¸ ì°¸ì—¬', 'ì—´ì •', 'ì±…ì„ê°'ê³¼ ê°™ì´ ì •ì„±ì ì´ê³  ì¶”ìƒì ì¸ í‘œí˜„ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
    2.  **ì‚¬ì‹¤ ê¸°ë°˜ ì¶”ì¶œ:** ë°˜ë“œì‹œ ì›ë¬¸ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê³ ìœ ëª…ì‚¬(í”„ë¡œê·¸ë˜ë° ì–¸ì–´, í”„ë ˆì„ì›Œí¬, íˆ´ ì´ë¦„)ë‚˜ êµ¬ì²´ì ì¸ í–‰ìœ„, í”„ë¡œì„¸ìŠ¤ë§Œ ì¶”ì¶œí•˜ì‹œì˜¤.
    3.  **ì •ë³´ ì—†ìŒ ì²˜ë¦¬:** ë§Œì•½ ì›ë¬¸ì—ì„œ íŠ¹ì • í•­ëª©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ í•„ë“œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ `[]`ë‚˜ 'ì •ë³´ ì—†ìŒ' ë¬¸ìì—´ì„ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹œì˜¤.
    4.  **ì •í™•í•œ ë¶„ë¥˜:** `hard_skills`ì—ëŠ” ì˜¤ì§ ê¸°ìˆ  ìŠ¤íƒ(ì–¸ì–´, í”„ë ˆì„ì›Œí¬, íˆ´)ë§Œ í¬í•¨í•˜ê³ , 'ê²½ë ¥'ì´ë‚˜ 'ê²½í—˜'ê³¼ ê°™ì€ ë‚´ìš©ì€ `preferred_experiences` í•­ëª©ì—ë§Œ í¬í•¨ì‹œí‚¤ì‹œì˜¤.
    5.  **ì–¸ì–´ í†µì¼:** ëª¨ë“  ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤.

    **ì˜ˆì‹œ (Examples):**

    --- [IT/ê°œë°œ ì§êµ° ì˜ˆì‹œ] ---
    - **ë‚˜ìœ ì˜ˆì‹œ:**
      ```json
      {{
        "hard_skills": ["ë¬¸ì œ í•´ê²° ëŠ¥ë ¥", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬"],
        "preferred_experiences": ["ìµœì‹  ê¸°ìˆ ì— ëŒ€í•œ ê´€ì‹¬"]
      }}

    - **ì¢‹ì€ ì˜ˆì‹œ:**
      {{
      "hard_skills": ["Python", "PyTorch", "AWS S3", "Docker"],
      "collaboration_process": "2ì£¼ ë‹¨ìœ„ ìŠ¤í”„ë¦°íŠ¸ë¡œ Jira ì´ìŠˆ ê´€ë¦¬, ë§¤ì¼ ë°ì¼ë¦¬ ìŠ¤í¬ëŸ¼ ì§„í–‰",
      "preferred_experiences": ["MSA ê¸°ë°˜ ì„œë¹„ìŠ¤ ê°œë°œ ë° ìš´ì˜ ê²½í—˜"]
      }}

    --- [ê¸ˆìœµ/ê¸°íš ì§êµ° ì˜ˆì‹œ] ---
        - **ë‚˜ìœ ì˜ˆì‹œ:**
      {{
      "hard_skills": ["ì„±ì‹¤í•¨", "ê¼¼ê¼¼í•¨"],
      "preferred_experiences": ["ê¸ˆìœµ ì‚°ì—…ì— ëŒ€í•œ ë†’ì€ ì´í•´ë„"]
      }}

    - **ì¢‹ì€ ì˜ˆì‹œ:**
      {{
      "hard_skills": ["ì¬ë¬´íšŒê³„ ì§€ì‹", "MS Excel (í”¼ë²—, VLOOKUP)", "íˆ¬ììì‚°ìš´ìš©ì‚¬ ìê²©ì¦"],
      "preferred_experiences": ["M&A í”„ë¡œì íŠ¸ ì°¸ì—¬ ê²½í—˜", "ê¸°ì—… ê°€ì¹˜ í‰ê°€(Valuation) ë³´ê³ ì„œ ì‘ì„± ê²½í—˜"]
      }}

    --- [ë§ˆì¼€íŒ…/ì½˜í…ì¸  ì§êµ° ì˜ˆì‹œ] ---
     - **ë‚˜ìœ ì˜ˆì‹œ:**
      {{
      "hard_skills": ["ì°½ì˜ë ¥", "íŠ¸ë Œë“œì— ë¯¼ê°í•¨"],
      "key_responsibilities": ["ë¸Œëœë“œ ì¸ì§€ë„ ìƒìŠ¹"]
      }}

    - **ì¢‹ì€ ì˜ˆì‹œ:**
      {{
      "hard_skills": ["GA(Google Analytics) í™œìš© ëŠ¥ë ¥", "í˜ì´ìŠ¤ë¶ ê´‘ê³  ê´€ë¦¬ì ê²½í—˜", "SEO ê¸°ë³¸ ì§€ì‹"],
      "key_responsibilities": ["ì£¼ 2íšŒ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì‘ì„± ë° ë°œí–‰", "ì›”ë³„ ì„±ê³¼ ë³´ê³ ì„œ ì‘ì„±"]
      }}

      '''
    """

    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": postings_web_results
    }

    postings_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=postings_prompt_template,
    pydantic_model=PostingAnalysisOutput,
    llm=llm,
    log_message="-> Helper: ì±„ìš© ê³µê³  ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )


    # --- 3. í•©ê²© í›„ê¸° ë¶„ì„ ---
    print("\n2. í•©ê²© í›„ê¸° ë¶„ì„ ì¤‘...")
    current_year = datetime.now().year
    years_query = " OR ".join(str(y) for y in range(current_year, current_year - 3, -1))

    reviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} ì‹ ì… í•©ê²© OR ë©´ì ‘ í›„ê¸° ({years_query}) site:velog.io OR site:tistory.com OR site:brunch.co.kr'
    )

    REVIEW_TOPIC_PROMPT = """
    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì£¼ì–´ì§„ 'ëª©í‘œ ê¸°ì—…'ê³¼ 'ëª©í‘œ ì§ë¬´'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ì‹ ì… í•©ê²© í›„ê¸°' ì˜ìƒì„ ì°¾ê¸° ìœ„í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ìœ íŠœë¸Œ ê²€ìƒ‰ì–´ í•œ ì¤„ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

    **ì§€ì‹œì‚¬í•­:**
    - "ì‹ ì… í•©ê²© í›„ê¸°", "ì·¨ì¤€", "ì„œë¥˜ ìŠ¤í™", "ë©´ì ‘í›„ê¸°", "í•©ê²© ë¸Œì´ë¡œê·¸" ë“± ê´€ë ¨ì„± ë†’ì€ í‚¤ì›Œë“œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

    **ë²”ì£¼í™” ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):**
    - **'ëª©í‘œ ê¸°ì—…'**ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°: ê°œë³„ íšŒì‚¬ëª…ì„ ëª¨ë‘ ë‚˜ì—´í•˜ì§€ ë§ê³ , ê¸°ì—…ë“¤ì˜ ê³µí†µì ì„ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ì˜ í¬ê´„ì ì¸ ë²”ì£¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
      - (ì˜ˆì‹œ 1) ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ë¼ì¸"] -> "IT ëŒ€ê¸°ì—…" ë˜ëŠ” "ì„œë¹„ìŠ¤ ê¸°ì—…"
      - (ì˜ˆì‹œ 2) ["í•œêµ­ì€í–‰", "ê¸ˆìœµê°ë…ì›"] -> "ê¸ˆìœµ ê³µê¸°ì—…"
      - (ì˜ˆì‹œ 3) ["ì»¬ë¦¬", "í† ìŠ¤"] -> "ìœ ë‹ˆì½˜ ìŠ¤íƒ€íŠ¸ì—…"
    - **'ëª©í‘œ ì§ë¬´'**ê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°: ì§ë¬´ë“¤ì„ ëŒ€í‘œí•˜ëŠ” ìƒìœ„ ì§êµ°ì´ë‚˜ ê¸°ìˆ  ë¶„ì•¼ë¡œ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë§Œë“œì„¸ìš”.
      - (ì˜ˆì‹œ 1) ["ë°±ì—”ë“œ ê°œë°œì", "ì„œë²„ ê°œë°œì"] -> "ì„œë²„ ê°œë°œì"
      - (ì˜ˆì‹œ 2) ["ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´", "AI ì—°êµ¬ì›"] -> "AI/ML ì§ë¬´"

    - ìµœì¢… ê²€ìƒ‰ì–´ëŠ” ìœ„ì˜ ê·œì¹™ì— ë”°ë¼ ë³€í™˜ëœ ë²”ì£¼ì™€ ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ì„ ì¡°í•©í•˜ì—¬ **ë‹¨ í•œ ì¤„ì˜ ë¬¸ìì—´**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

    **ëª©í‘œ ê¸°ì—…:** {companies}
    **ëª©í‘œ ì§ë¬´:** {jobs}
    """
    review_topic_chain = ChatPromptTemplate.from_template(REVIEW_TOPIC_PROMPT) | llm | StrOutputParser()

    youtube_review_topic = review_topic_chain.invoke({
        "companies": refined_companies,
        "jobs": target_jobs_list
    })
    youtube_review_prompt = f"""
    '{target_job_title}' ì§ë¬´ ì‹ ì… í•©ê²© í›„ê¸° ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬, 'ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤' ê¸°ë°˜ì˜ í•©ê²© ì „ëµì„ ì•„ë˜ êµ¬ì¡°ë¡œ ì •ë¦¬í•´ì¤˜.

    - **[í•©ê²©ì í”„ë¡œí•„(Profile)]**: í•©ê²©ìë“¤ì˜ ê³µí†µì ì¸ êµ¬ì²´ì  ìŠ¤í™ì€ ë¬´ì—‡ì¸ê°€?
    - **[ì±„ìš© í”„ë¡œì„¸ìŠ¤ë³„ ì¤€ë¹„ ì‚¬í•­(Process Prep)]**: ê° ë‹¨ê³„ë³„ë¡œ ë¬´ì—‡ì„, ì–´ë–»ê²Œ ì¤€ë¹„í–ˆëŠ”ê°€?
    - **[ê²°ì •ì  í•©ê²© ì¦ê±°(Actionable Evidence)]**: í•©ê²©ìë“¤ì´ ì œì‹œí•˜ëŠ” ìì‹ ì˜ ê°€ì¥ ê°•ë ¥í•œ ê²½ìŸë ¥ì€ ë¬´ì—‡ì¸ê°€?
    """

    youtube_summary = analyze_youtube_topic(
        topic=youtube_review_topic,
        analysis_prompt=youtube_review_prompt,
        num_to_analyze=2,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )


    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì™€ ìœ íŠœë¸Œ ë¶„ì„ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… ë¶„ì„
    # --- 4b. ìˆ˜ì§‘ëœ ë°ì´í„° ì¢…í•© ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    combined_reviews  = f"--- ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ìµœê·¼ 3ë…„) ---\n{reviews_web_results}\n\n--- ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ (ìµœê·¼ 3ë…„) ---\n{youtube_summary}"

    reviews_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ 'ì¸ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤(Quoted Fact)'ë§Œì„ ì •í™•í•˜ê²Œ ë½‘ì•„ë‚´ëŠ” ì •ë³´ ì¶”ì¶œ AIì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì ˆëŒ€ ì¶”ë¡ í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    {target_job_title} ì§ë¬´ì˜ ì£¼ì–´ì§„ í•©ê²© í›„ê¸° ë° í˜„ì§ì ì¡°ì–¸({search_results})ì—ì„œ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ì¶”ìƒì  ì¡°ì–¸ ì ˆëŒ€ ê¸ˆì§€:** "...í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥", "...í•˜ëŠ” ê²ƒì´ ì¤‘ìš”", "...í•˜ë„ë¡ ë…¸ë ¥"ê³¼ ê°™ì´ í–‰ë™ì´ ì•„ë‹Œ íƒœë„ë‚˜ ë§ˆìŒê°€ì§ì— ëŒ€í•œ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¶œí•˜ì§€ ë§ˆì‹œì˜¤. ì´ëŸ° ë‚´ìš©ë§Œ ìˆë‹¤ë©´ 'ì •ë³´ ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬í•˜ì‹œì˜¤.
    2.  **ì‚¬ì‹¤ ê¸°ë°˜ ì¶”ì¶œ:** ë°˜ë“œì‹œ ì›ë¬¸(í•©ê²© í›„ê¸°)ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ê²½í—˜, ì¤€ë¹„ ê³¼ì •, ê¸°ìˆ , í”„ë¡œì íŠ¸, KPI, ì—­í• ì— ëŒ€í•œ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì‹œì˜¤.
    3.  **ì •ë³´ ì—†ìŒ ì²˜ë¦¬:** ë§Œì•½ ì›ë¬¸ì—ì„œ íŠ¹ì • í•­ëª©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ í•„ë“œëŠ” 'ì •ë³´ ì—†ìŒ' ë¬¸ìì—´ì„ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹œì˜¤.
    4.  **ì–¸ì–´ í†µì¼:** ëª¨ë“  ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤.

    **ì˜ˆì‹œ (Examples):**

    - **ë‚˜ìœ ì˜ˆì‹œ (Bad Example):**
      ```json
      {{
        "first_year_role": "íŒ€ì— ê¸°ì—¬í•˜ê³  ë°°ìš°ëŠ” ì—­í• ",
        "actionable_advice": ["ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‚¤ìš°ì„¸ìš”", "ì†Œí†µ ëŠ¥ë ¥ì´ ì¤‘ìš”í•©ë‹ˆë‹¤"]
      }}

      ** ì¢‹ì€ ì˜ˆì‹œ (Good Example): **
      {{
      "first_year_role": "ì´ˆê¸° 3ê°œì›”ê°„ ë²„ê·¸ ìˆ˜ì • ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± ìœ„ì£¼ë¡œ ì—…ë¬´, ì´í›„ ì‘ì€ ê¸°ëŠ¥ ê°œë°œ ë‹´ë‹¹",
      "performance_metric": "ì£¼ê°„ PR(Pull Request) ê°œìˆ˜ì™€ ì½”ë“œ ë¦¬ë·° ìŠ¹ì¸ìœ¨",
      "actionable_advice": ["ì½”ë”© í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì‹œ, ë°±ì¤€ ê³¨ë“œ í‹°ì–´ ìˆ˜ì¤€ì˜ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ í’€ì´ ê²½í—˜ì´ ì¤‘ìš”í•¨", "ë‹¨ìˆœ CRUDê°€ ì•„ë‹Œ, ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ê°€ì •í–ˆì„ ë•Œì˜ DB ì¸ë±ì‹± ì „ëµì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ì•˜ìŒ"]
      }}
      '''
    """
    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_reviews
    }

    reviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=reviews_prompt_template,
    pydantic_model=ReviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: í•©ê²© í›„ê¸° ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    # --- 4. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ---
    print("\n3. í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ì¤‘...")
    interviews_web_results = tavily_web_search.invoke(
        f'{company_query} {job_query} "í˜„ì§ì ì¸í„°ë·°" OR "ì§ì¥ì¸ ë¸Œì´ë¡œê·¸" OR "ì¼í•˜ëŠ” ë°©ì‹" OR "íŒ€ ë¬¸í™”" OR "ì»¤ë¦¬ì–´" site:tistory.com OR site:brunch.co.kr'
    )

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ìœ íŠœë¸Œ ê²€ìƒ‰ ì£¼ì œì–´ë¥¼ ìƒì„±
    INTERVIEW_TOPIC_PROMPT = """
    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì£¼ì–´ì§„ 'ëª©í‘œ ê¸°ì—…'ê³¼ 'ëª©í‘œ ì§ë¬´'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í˜„ì§ìë“¤ì˜ ìƒí™œì´ë‚˜ ì—…ë¬´ ë°©ì‹ì„ ì—¿ë³¼ ìˆ˜ ìˆëŠ” ì˜ìƒì„ ì°¾ê¸° ìœ„í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ìœ íŠœë¸Œ ê²€ìƒ‰ì–´ í•œ ì¤„ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

    **ì§€ì‹œì‚¬í•­:**
    - "í˜„ì§ì ì¸í„°ë·°", "VLOG", "ì¼í•˜ëŠ” ë°©ì‹", "ì˜¤í”¼ìŠ¤ íˆ¬ì–´", "íŒ€ ì†Œê°œ", "ê°œë°œì ì¼ìƒ" ë“± ê´€ë ¨ì„± ë†’ì€ í‚¤ì›Œë“œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

    **ë²”ì£¼í™” ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):**
    - **'ëª©í‘œ ê¸°ì—…'**ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°: ê°œë³„ íšŒì‚¬ëª…ì„ ëª¨ë‘ ë‚˜ì—´í•˜ì§€ ë§ê³ , ê¸°ì—…ë“¤ì˜ ê³µí†µì ì„ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ì˜ í¬ê´„ì ì¸ ë²”ì£¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
      - (ì˜ˆì‹œ 1) ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ë¼ì¸"] -> "IT ëŒ€ê¸°ì—…" ë˜ëŠ” "ë„¤ì¹´ë¼"
      - (ì˜ˆì‹œ 2) ["í•œêµ­ì „ë ¥ê³µì‚¬", "í•œêµ­ë„ë¡œê³µì‚¬"] -> "ì£¼ìš” ê³µê¸°ì—…"
      - (ì˜ˆì‹œ 3) ["ì»¬ë¦¬", "í† ìŠ¤"] -> "ìœ ë‹ˆì½˜ ìŠ¤íƒ€íŠ¸ì—…"
    - **'ëª©í‘œ ì§ë¬´'**ê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°: ì§ë¬´ë“¤ì„ ëŒ€í‘œí•˜ëŠ” ìƒìœ„ ì§êµ°ì´ë‚˜ ê¸°ìˆ  ë¶„ì•¼ë¡œ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë§Œë“œì„¸ìš”.
      - (ì˜ˆì‹œ 1) ["ë°±ì—”ë“œ ê°œë°œì", "ì„œë²„ ê°œë°œì"] -> "ì„œë²„ ê°œë°œ"
      - (ì˜ˆì‹œ 2) ["ë°ì´í„° ë¶„ì„ê°€", "ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸"] -> "ë°ì´í„° ì§êµ°"
      - (ì˜ˆì‹œ 3) ["ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´", "AI ì—°êµ¬ì›"] -> "AI/ML ì§ë¬´"

    - ìµœì¢… ê²€ìƒ‰ì–´ëŠ” ìœ„ì˜ ê·œì¹™ì— ë”°ë¼ ë³€í™˜ëœ ë²”ì£¼ì™€ ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ì„ ì¡°í•©í•˜ì—¬ **ë‹¨ í•œ ì¤„ì˜ ë¬¸ìì—´**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

    **ëª©í‘œ ê¸°ì—…:** {companies}
    **ëª©í‘œ ì§ë¬´:** {jobs}
    """

    topic_generation_chain = ChatPromptTemplate.from_template(INTERVIEW_TOPIC_PROMPT) | llm | StrOutputParser()

    interview_topic = topic_generation_chain.invoke({
        "companies": refined_companies,
        "jobs": target_jobs_list
    })
    interview_prompt = f"'{target_job_title}' ì§ë¬´ í˜„ì§ìë¡œì„œ ì¼í•˜ëŠ” ë°©ì‹, ì¡°ì§ ë¬¸í™”, ì‹ ì…ì—ê²Œ í•„ìš”í•œ ì—­ëŸ‰ì— ëŒ€í•´ ë§í•˜ëŠ” ë¶€ë¶„ì„ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜."
    youtube_interview_summary = analyze_youtube_topic(
        topic=interview_topic,
        analysis_prompt=interview_prompt,
        num_to_analyze=1,
        transcripts_only=True,
        api_key=api_keys.youtube_api_key
    )

    # --- 5b. ìˆ˜ì§‘ëœ ë°ì´í„° ì¢…í•© ë° í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    print("-> í…ìŠ¤íŠ¸ì™€ ìœ íŠœë¸Œ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
    combined_interviews = f"""
    --- Text-based Interview Search Results ---
    {interviews_web_results}

    --- YouTube Interview Analysis Summary ---
    {youtube_interview_summary}
    """

    # 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: 'í˜„ì§ì ì¸í„°ë·°' ë¶„ì„ì— ë§ê²Œ ê·œì¹™ê³¼ ì˜ˆì‹œë¥¼ êµ¬ì²´í™”
    interviews_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ 'ì¸ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤(Quoted Fact)'ë§Œì„ ì •í™•í•˜ê²Œ ë½‘ì•„ë‚´ëŠ” ì •ë³´ ì¶”ì¶œ AIì…ë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    {target_job_title} ì§ë¬´ í˜„ì§ìë“¤ì˜ ê²½í—˜ë‹´({search_results})ì—ì„œ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ì¶”ìƒì  í‘œí˜„ ê¸ˆì§€:** 'ìˆ˜í‰ì ì¸ ë¬¸í™”', 'ììœ ë¡œìš´ ì†Œí†µ' ë“± ë§‰ì—°í•˜ê³  ì¼ë°˜ì ì¸ í‘œí˜„ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
    2.  **ì‚¬ì‹¤ ê¸°ë°˜ ì¶”ì¶œ:** ë°˜ë“œì‹œ ì›ë¬¸ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤, ì—­ëŸ‰, íˆ´ ì´ë¦„, ì‚¬ë‚´ ì œë„, ì‹¤ì§ˆì ì¸ ì¡°ì–¸ë§Œ ì¶”ì¶œí•˜ì‹œì˜¤.
    3.  **ì •ë³´ ì—†ìŒ ì²˜ë¦¬:** ì›ë¬¸ì—ì„œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ í•„ë“œëŠ” 'ì •ë³´ ì—†ìŒ' ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ `[]`ë¥¼ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹œì˜¤.
    4.  **ì–¸ì–´ í†µì¼:** ëª¨ë“  ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤.

    ---
    **ìµœì¢… ê²€ì¦ (Final Verification):**
    JSONì„ ìƒì„±í•œ í›„, ìŠ¤ìŠ¤ë¡œ ì•„ë˜ ì§ˆë¬¸ 3ê°€ì§€ì— ë‹µí•´ë³´ì‹œì˜¤. ë§Œì•½ í•˜ë‚˜ë¼ë„ 'ì•„ë‹ˆì˜¤'ë¼ë©´, ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë¶„ì„í•˜ì—¬ ê·œì¹™ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ë¬¼ì„ ë§Œë“œì‹œì˜¤.

    1.  `core_competencies_and_tools`ì— ì§ë¬´ëª…ì´ë‚˜ ë¶„ì•¼ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆì§€ëŠ” ì•Šì€ê°€? (ë‹µ: 'ì•„ë‹ˆì˜¤'. ë°˜ë“œì‹œ ì‹¤ì œ ì—­ëŸ‰ì´ë‚˜ íˆ´ ì´ë¦„ë§Œ ìˆì–´ì•¼ í•¨)
    2.  `growth_and_career_path`ì— ì˜ìƒì´ë‚˜ ê²Œì‹œë¬¼ì˜ ì œëª©ì´ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°€ ìˆì§€ëŠ” ì•Šì€ê°€? (ë‹µ: 'ì•„ë‹ˆì˜¤'. ë°˜ë“œì‹œ ë³¸ë¬¸ ë‚´ìš©ì—ì„œ ì¶”ì¶œí•œ êµ¬ì²´ì ì¸ ê²½ë¡œì—¬ì•¼ í•¨)
    3.  ëª¨ë“  ê²°ê³¼ê°’ì´ ì¶”ìƒì ì´ê±°ë‚˜ ì¼ë°˜ì ì¸ ë‚´ìš©ì´ ì•„ë‹ˆë¼, êµ¬ì²´ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ê°€? (ë‹µ: 'ì˜ˆ')
    ---

    ì´ì œ ìœ„ ê·œì¹™ê³¼ ìµœì¢… ê²€ì¦ ì ˆì°¨ì— ë”°ë¼ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.

    **ì˜ˆì‹œ (Examples):**

    - **ë‚˜ìœ ì˜ˆì‹œ (Bad Example):**
      ```json
      {{
        "team_culture_and_process": "ì„œë¡œ ì¡´ì¤‘í•˜ë©° ììœ ë¡­ê²Œ ì˜ê²¬ì„ ë‚˜ëˆ•ë‹ˆë‹¤.",
        "advice_for_newcomers": ["ê¸°ë³¸ê¸°ë¥¼ íƒ„íƒ„íˆ í•˜ê³  ì—´ì‹¬íˆ ë°°ìš°ëŠ” ìì„¸ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤."]
      }}

      ** ì¢‹ì€ ì˜ˆì‹œ (Good Example): **
      {{
      "day_in_the_life": "ì˜¤ì „ 9ì‹œ ë°ì¼ë¦¬ ìŠ¤í¬ëŸ¼ í›„, ì˜¤ì „ì—ëŠ” ì£¼ë¡œ ë°°ì •ëœ Jira í‹°ì¼“ì˜ ë²„ê·¸ë¥¼ ìˆ˜ì •í•˜ê³  ì½”ë“œ ë¦¬ë·°ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì˜¤í›„ì—ëŠ” ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸ì— í¬í•¨ë  ì‹ ê·œ ê¸°ëŠ¥ ê°œë°œì— ì§‘ì¤‘í•©ë‹ˆë‹¤.",
      "real_tech_stack": ["Kotlin", "Spring Boot", "JPA", "Kubernetes", "ArgoCD", "Grafana"],
      "team_culture_and_process": "2ì£¼ ë‹¨ìœ„ ì• ìì¼ ìŠ¤í”„ë¦°íŠ¸ë¡œ ìš´ì˜ë˜ë©°, PR(Pull Request)ì€ ìµœì†Œ 2ëª…ì˜ ë™ë£Œì—ê²Œ Approveë¥¼ ë°›ì•„ì•¼ ë¨¸ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
      "growth_environment": "ë§¤ì£¼ ê¸ˆìš”ì¼ ì˜¤í›„ì—ëŠ” íŒ€ ë‚´ ê¸°ìˆ  ê³µìœ  ì„¸ì…˜ì´ ìˆê³ , ë¶„ê¸°ë³„ë¡œ ì™¸ë¶€ ì»¨í¼ëŸ°ìŠ¤ ì°¸ì—¬ë¥¼ ì§€ì›í•´ì¤ë‹ˆë‹¤.",
      "advice_for_newcomers": ["ì…ì‚¬ ì²« ë‹¬ì—ëŠ” ìš°ë¦¬ íŒ€ì˜ í•µì‹¬ ì„œë¹„ìŠ¤ì¸ 'A'ì˜ ì•„í‚¤í…ì²˜ ë¬¸ì„œë¥¼ ì™„ë…í•˜ëŠ” ê²ƒì´ ìµœìš°ì„  ê³¼ì œì…ë‹ˆë‹¤.", "Git ë¸Œëœì¹˜ ì „ëµì´ ë³µì¡í•˜ë‹ˆ ë¯¸ë¦¬ ìˆ™ì§€í•´ì˜¤ë©´ ì¢‹ìŠµë‹ˆë‹¤."]
      }}
      '''
    """

    prompt_variable_inputs = {
    "target_job_title": target_job_title,
    "search_results": combined_interviews
    }

    interviews_analysis = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=interviews_prompt_template,
    pydantic_model=InterviewAnalysisOutput,
    llm=llm,
    log_message="-> Helper: í˜„ì§ì ì¸í„°ë·° ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    print("-> ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

    # --- 6. ëª¨ë“  ë¶„ì„ ê²°ê³¼ ì¢…í•© ë° í‚¤ì›Œë“œ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ) ---
    combined_analysis_dict = {
        "postings_analysis": postings_analysis,
        "reviews_analysis": reviews_analysis,
        # "interviews_analysis": interviews_analysis
    }

    combined_analysis_str = json.dumps(combined_analysis_dict, ensure_ascii=False, indent=2)

    keyword_extraction_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ìˆ˜ì‹­ ê°œì˜ ê¸°ìˆ  ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•˜ëŠ” **ì „ëµê°€(Strategist)ì´ì ë¯¸ë˜í•™ì(Futurist)**ì…ë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    ì£¼ì–´ì§„ ëŒ€í•œë¯¼êµ­ ì±„ìš© ì‹œì¥ ë¶„ì„ ë°ì´í„°({market_analysis_json})ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬, ê¸€ë¡œë²Œ ê¸°ìˆ  ë° ì»¤ë¦¬ì–´ íŠ¸ë Œë“œë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ë¯¸ë˜ ì§€í–¥ì  ë¶„ì„:** ë‹¨ìˆœíˆ ì–¸ê¸‰ëœ ê¸°ìˆ ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ë°ì´í„°ì— ì•”ì‹œëœ **ë¯¸ë˜ ì§€í–¥ì ì´ê³  ì ì¬ë ¥ ìˆëŠ” í‚¤ì›Œë“œ**ë¥¼ í¬ì°©í•˜ì‹œì˜¤.
    2.  **ì •í™•í•œ ë¶„ë¥˜:** ê° í‚¤ì›Œë“œë¥¼ `core_technologies`, `business_domains`, `emerging_roles`, `problem_solution_keywords` ë„¤ ê°€ì§€ ë¶„ë¥˜ì— ë§ê²Œ ì •í™•íˆ í• ë‹¹í•˜ì‹œì˜¤.
    3.  **í•µì‹¬ ìœ„ì£¼ ì¶”ì¶œ:** ë„ˆë¬´ ì„¸ë¶€ì ì´ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” í‚¤ì›Œë“œëŠ” ì œì™¸í•˜ê³ , ê°€ì¥ í•µì‹¬ì ì´ê³  ëŒ€í‘œì ì¸ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì‹œì˜¤.
    4.  **ì–¸ì–´ í†µì¼:** ëª¨ë“  í‚¤ì›Œë“œëŠ” ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ê²€ìƒ‰ì— ìš©ì´í•˜ë„ë¡ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ì‹œì˜¤.

    **ì‚¬ê³  ê³¼ì • ì˜ˆì‹œ (Example of Thought Process):**
    - **ì…ë ¥ ë°ì´í„°:** "ììœ¨ì£¼í–‰ ë¡œë´‡ì˜ ê²½ë¡œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ"ê³¼ "ë°ì´í„°ì˜ í¸í–¥ì„± í•´ê²°ì´ ì¤‘ìš”"ë¼ëŠ” ë‚´ìš©ì´ ìˆìŒ.
    - **ì‚¬ê³ :**
        1. 'ììœ¨ì£¼í–‰ ë¡œë´‡' -> `business_domains`ì€ 'ììœ¨ì£¼í–‰', 'ë¬¼ë¥˜ í…Œí¬'ê°€ ë˜ê² êµ°.
        2. 'ê²½ë¡œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜' -> `core_technologies`ì— 'SLAM'ì´ë‚˜ 'ê²½ë¡œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜'ì„ ì¶”ê°€í•´ì•¼ê² ë‹¤.
        3. 'ë°ì´í„° í¸í–¥ì„± í•´ê²°' -> `problem_solution_keywords`ë¡œ 'ì„¤ëª…ê°€ëŠ¥ AI (XAI)'ë‚˜ 'ì•Œê³ ë¦¬ì¦˜ ìœ¤ë¦¬'ë¥¼ ë– ì˜¬ë¦´ ìˆ˜ ìˆê² ì–´.
        4. ì´ ì§ë¬´ëŠ” -> `emerging_roles`ë¡œ 'ë¡œë³´í‹±ìŠ¤ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´'ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤.
    - **ê²°ê³¼:** ìœ„ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³ ì•„ë˜ì™€ ê°™ì€ JSONì„ ìƒì„±í•¨.

    **ì¢‹ì€ ì˜ˆì‹œ (Good Example):**
    ```json
    {{
        "core_technologies": ["Computer Vision", "Robotics", "SLAM"],
        "business_domains": ["Autonomous Vehicles", "Logistics Tech"],
        "emerging_roles": ["Robotics Software Engineer"],
        "problem_solution_keywords": ["Explainable AI (XAI)", "Algorithmic Ethics", "Pathfinding Algorithm"]
    }}
    """

    prompt_variable_inputs = {
    "market_analysis_json": combined_analysis_str
    }

    # ë²”ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Global Trend ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
    global_search_keywords = extract_structured_data_flexible(
    prompt_inputs=prompt_variable_inputs,
    extraction_prompt_template=keyword_extraction_prompt_template,
    pydantic_model=GlobalSearchKeywords,
    llm=llm,
    log_message="-> Helper: Global Trend ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰..."
    )

    # --- 6. ìµœì¢… ë°˜í™˜ ---
    print("--- êµ­ë‚´ ì±„ìš© ì‹œì¥ ë¶„ì„ ì™„ë£Œ ---")
    return {
        "domestic_analysis_components": {
            "postings_analysis": postings_analysis,
            "reviews_analysis": reviews_analysis,
            "interviews_analysis": interviews_analysis
        },
        "domestic_keywords": global_search_keywords,
        "llm" : llm
    }

def global_trend_analysis_node(state: AgentState) -> dict:
    """
    êµ­ë‚´ ë¶„ì„ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ì•¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    ê¸°ìˆ /ì‹œì¥/ë¦¬ë”ì‹­ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ê¸€ë¡œë²Œ ë™í–¥ì„ ë„ì¶œí•©ë‹ˆë‹¤.
    (êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •ë¨)
    """
    print("\n--- [Step 3] ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    domestic_keywords = state["domestic_keywords"]
    target_job_title = ", ".join(state["target_job"])
    target_companies = state["target_company"]
    api_keys = state["api_keys"]
    one_year_ago_str = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

    # ----------------------------------------------------------------------------------

    # --- 2. ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ , ì‹œì¥, ë¦¬ë”ì‹­) ---

    # 2a. ê¸°ìˆ  íŠ¸ë Œë“œ (Tavily)
    print("\n1. ê¸°ìˆ  íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    tech_trends_results = ""
    SEARCH_QUERY_GENERATION_PROMPT = """
    You are an expert market researcher. Based on the following Korean keywords for a job role, generate 5 distinct and effective English search queries to research global and future trends.
    The queries should be concise and focus on future outlook, required skills, and market changes.
    Output your answer as a JSON array of strings. Do not include any other text.

    **Korean Keywords:** {keywords_str}
    **Job Title:** {job_title}
    """
    try:
        search_query_chain = ChatPromptTemplate.from_template(SEARCH_QUERY_GENERATION_PROMPT) | llm | JsonOutputParser()
        search_queries = search_query_chain.invoke({
            "keywords_str": json.dumps(domestic_keywords, ensure_ascii=False),
            "job_title": target_job_title
        })

        # ê° ê²€ìƒ‰ì–´ì— 'after:'ë¥¼ ì¶”ê°€í•˜ì—¬ 1ë…„ ì´ë‚´ë¡œ ì‹œê°„ ì œí•œ
        timed_search_queries = [f"{q} after:{one_year_ago_str}" for q in search_queries]
        print(f"-> ìƒì„±ëœ ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ì–´: {timed_search_queries}")

        tech_trends_results_list = tavily_web_search.batch(timed_search_queries)
        tech_trends_results = "\n\n---\n\n".join([str(result) for result in tech_trends_results_list])
    except Exception as e:
        print(f"-> âš ï¸ ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ({e}).")
        tech_trends_results = "ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    # ----------------------------------------------------------------------------------

    # 2b. ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ (News API)
    print("\n2. ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    market_trends_results = ""
    if target_companies:
        try:
            # ì‚°ì—… ë¶„ì•¼ ì¶”ë¡ 
            main_company = target_companies[0]
            industry_name_prompt = f"The company '{main_company}' primarily operates in which industry? Provide a concise, common English industry name (e.g., 'E-commerce', 'Semiconductor', 'Financial Services')."
            industry_name = llm.invoke(industry_name_prompt).content.strip()
            print(f"-> '{main_company}'ì˜ í•µì‹¬ ì‚°ì—… ë¶„ì•¼ ì¶”ë¡ : {industry_name}")

            # í•œê¸€ ì§ë¬´ëª…ì„ ì˜ì–´ë¡œ ë²ˆì—­
            korean_job_title = ", ".join(state["target_job"])
            translation_prompt = f"Translate the following Korean job titles into a concise, comma-separated English string: '{korean_job_title}'"
            english_job_titles_str = llm.invoke(translation_prompt).content.strip()
            print(f"-> ì§ë¬´ëª… ì˜ë¬¸ ë²ˆì—­ ì™„ë£Œ: {english_job_titles_str}")

            # 1. ê° ì§ë¬´ëª…ì˜ ì–‘ìª½ì— í°ë”°ì˜´í‘œ(")ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            # -> ["Machine Learning Engineer", "Data Scientist"]
            job_titles_list = [job.strip() for job in english_job_titles_str.split(',')]
            quoted_job_titles = [f'"{job}"' for job in job_titles_list]

            # 2. í°ë”°ì˜´í‘œë¡œ ë¬¶ì¸ ì§ë¬´ëª…ë“¤ì„ " OR "ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
            # -> "Machine Learning Engineer" OR "Data Scientist"
            or_separated_jobs = " OR ".join(quoted_job_titles)

            # 3. ìµœì¢…ì ìœ¼ë¡œ ì–‘ìª½ì„ ê´„í˜¸()ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
            # -> ("Machine Learning Engineer" OR "Data Scientist")
            job_titles_query_part = f'({or_separated_jobs})'

            # ì¬êµ¬ì„±ëœ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ê²€ìƒ‰ì–´ë¥¼ ë§Œë“­ë‹ˆë‹¤.
            news_query = f'"{industry_name}" AND {job_titles_query_part} AND (hiring OR skill OR future OR trend)'
            print(f"-> News API ê²€ìƒ‰ì–´: {news_query}")

            market_trends_results = search_global_news.invoke({
                "query": news_query,
                "from_date": one_year_ago_str
            })
        except Exception as e:
            print(f"-> âš ï¸ ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ({e}).")
            market_trends_results = "ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    else:
        print("-> í¬ë§ ê¸°ì—…ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ì‹œì¥/ê¸°ì—… íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # ----------------------------------------------------------------------------------

    # 2c. ë¦¬ë”ì‹­ íŠ¸ë Œë“œ (YouTube)
    print("\n3. ë¦¬ë”ì‹­(ê¶Œìœ„ì ë¹„ì „) íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    vision_analysis_summary = ""

    if target_companies:
        try :
            main_company = target_companies[0]
            key_figures_prompt = """
            '{main_company}'ê°€ ì†í•œ ì‚°ì—… ë˜ëŠ” '{target_job_title}' ë¶„ì•¼ì—ì„œ ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ê¸€ë¡œë²Œ ë¦¬ë”(CEO, ì—°êµ¬ì›, êµ¬ë£¨ ë“±)ì˜ ì´ë¦„ì„ 20ëª…ë§Œ ì•Œë ¤ì¤˜.

            **ì§€ì‹œì‚¬í•­:**
            - ì£¼ë¡œ **ì£¼ìš” ê¸€ë¡œë²Œ ê¸°ìˆ  ì»¨í¼ëŸ°ìŠ¤(ì˜ˆ: NVIDIA GTC, Google I/O), TED, ê³µì‹ì ì¸ ëŒ€í•™ ê°•ì—°** ë“±ì—ì„œ ë°œí‘œí•˜ì—¬, **ê³µì‹ì ìœ¼ë¡œ ë…¹í™”ë˜ê³  ì–‘ì§ˆì˜ ìë§‰ì´ ì œê³µë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì¸ë¬¼**ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•´ì¤˜.
            - ìœ íŠœë¸Œì—ì„œ ì–‘ì§ˆì˜ ì˜ì–´ ìë§‰ê³¼ í•¨ê»˜ ê°•ì—°ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì¸ë¬¼ì„ ê³ ë ¤í•´ì¤˜.
            - ì‘ë‹µì€ ì˜ë¬¸ ì´ë¦„ìœ¼ë¡œ êµ¬ì„±ëœ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: ["Jensen Huang", "Satya Nadella", "Demis Hassabis", "Andrew Ng"])
            """
            key_figures_chain = ChatPromptTemplate.from_template(key_figures_prompt) | llm | JsonOutputParser()
            key_figures_list = key_figures_chain.invoke({
            "main_company": main_company,
            "target_job_title": target_job_title
            })

            # ê° ì¸ë¬¼ë³„ë¡œ ë”°ë¡œë”°ë¡œ ìœ íŠœë¸Œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
            analysis_results = []
            for figure_name in key_figures_list:
                # ê° ì¸ë¬¼ì— ëŒ€í•œ ê²€ìƒ‰ ì£¼ì œì™€ ë¶„ì„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
                vision_topic = f'"{figure_name}" conference OR interview'
                vision_prompt = f"'{figure_name}'ì˜ ê°•ì—° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, '{target_job_title}' ì§ë¬´ì™€ ê´€ë ¨ëœ ë¯¸ë˜ ë¹„ì „, ê¸°ìˆ  ì² í•™, ê·¸ë¦¬ê³  ì—…ê³„ì— ë˜ì§€ëŠ” í•µì‹¬ ë©”ì‹œì§€ë¥¼ ìš”ì•½í•´ì¤˜."

                # ê° ì¸ë¬¼ë³„ë¡œ ì˜ìƒ 1ê°œë§Œ ë¶„ì„í•˜ì—¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                summary = analyze_youtube_topic(
                    topic=vision_topic,
                    analysis_prompt=vision_prompt,
                    api_key=api_keys.youtube_api_key,
                    lang_code='en',
                    max_results=20,  # ì¸ë¬¼ë‹¹ 3ê°œ í›„ë³´ ê²€ìƒ‰
                    num_to_analyze=1, # ê·¸ ì¤‘ 1ê°œë§Œ ë¶„ì„
                    transcripts_only=True
                )
                analysis_results.append(summary)

            # ê°œë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ë¡œ í•©ì¹¨
            vision_analysis_summary = "\n\n---\n\n".join(analysis_results)

        except Exception as e:
            print(f"-> âš ï¸ ê¶Œìœ„ì ë¹„ì „ ë¶„ì„ ì‹¤íŒ¨ ({e}).")
            vision_analysis_summary = "ê¶Œìœ„ì ë¹„ì „ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    else:
        vision_analysis_summary = "í¬ë§ ê¸°ì—…ì´ ì§€ì •ë˜ì§€ ì•Šì•„ ë¦¬ë”ì‹­ íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."

    # --- 3. ëª¨ë“  ì •ë³´ ì¢…í•© ë° ìµœì¢… ë¶„ì„ ---
    print("\n4. ëª¨ë“  ì •ë³´ ì¢…í•© ë° ìµœì¢… íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
    combined_results = f"""
    --- Technical Trends (from Keyword Search) ---
    {tech_trends_results}

    --- Market & Industry Trends (from News Search) ---
    {market_trends_results}

    --- Vision from Industry Leader (from YouTube) ---
    {vision_analysis_summary}
    """

    final_analysis_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ìˆ˜ë°± ê°œì˜ ê¸°ìˆ  ì•„í‹°í´, ì‹œì¥ ë³´ê³ ì„œ, ë¦¬ë” ì¸í„°ë·°ë¥¼ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•˜ê³  êµ¬ì§ìì—ê²Œ actionable insightë¥¼ ì œê³µí•˜ëŠ” **ê¸€ë¡œë²Œ ê¸°ìˆ  ì „ëµê°€(Global Technology Strategist)**ì…ë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    ì£¼ì–´ì§„ 3ê°€ì§€ ì¢…ë¥˜ì˜ ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë°ì´í„°({search_results})ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, {target_job_title} ì§ë¬´ ì§€ì›ìê°€ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  í•µì‹¬ ë™í–¥ì„ {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ì¢…í•©ì  ë¶„ì„:** ì„¸ ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤(Technical, Market, Vision)ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    2.  **êµ¬ì²´ì„±:** 'AIì˜ ë°œì „'ê³¼ ê°™ì€ ë§‰ì—°í•œ í‘œí˜„ ëŒ€ì‹ , 'ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ìƒì„±í˜• AI(Generative AI for Code Generation)'ì²˜ëŸ¼ êµ¬ì²´ì ì¸ ê¸°ìˆ ê³¼ ìš©ë„ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
    3.  **êµ¬ì§ì ê´€ì :** ëª¨ë“  ë¶„ì„ì€ ìµœì¢…ì ìœ¼ë¡œ 'ê·¸ë˜ì„œ êµ¬ì§ìê°€ ë¬´ì—‡ì„ ì¤€ë¹„í•´ì•¼ í•˜ëŠ”ê°€?'ë¼ëŠ” ê´€ì ì—ì„œ ì •ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    **ì¢‹ì€ ì˜ˆì‹œ (Good Example):**
    ```json
    {{
      "key_technology_shifts": ["Generative AI for Code Generation", "MLOps for Production", "Vector Databases"],
      "changing_market_demands": ["Prompt Engineering skills required", "Experience with large-scale distributed training"]
    }}
    ```

    ---
    ì´ì œ ìœ„ ê·œì¹™ê³¼ ì˜ˆì‹œì— ë”°ë¼ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.
    """

    # ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ì™€ Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ extract_structured_data_flexible í˜¸ì¶œ
    prompt_variable_inputs = {
        "target_job_title": target_job_title,
        "search_results": combined_results
    }

    global_trends_analysis = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=final_analysis_prompt_template,
        pydantic_model=GlobalTrendsOutput,
        llm=llm,
        log_message="-> Helper: ìµœì¢… ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ë° êµ¬ì¡°í™” ì‹¤í–‰..."
    )

    return {
        "global_trends": global_trends_analysis,
        "llm": llm
    }

def gap_analysis_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ì í”„ë¡œí•„ê³¼ êµ­ë‚´ì™¸ ì‹œì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
    ì‚¬ìš©ìì˜ ê°•ì , ì•½ì , ê¸°íšŒë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì‚¬ìš©ì í”„ë¡œí•„ ì²˜ë¦¬ ë¡œì§ ê°•í™”)
    """
    print("\n--- [Step 4] ì‚¬ìš©ì í”„ë¡œí•„ ë° ì‹œì¥ ìš”êµ¬ì‚¬í•­ ê°­ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")

    # --- 1. ì‚¬ì „ ì¤€ë¹„ ---
    user_profile = state["user_profile"]
    domestic_analysis = state["domestic_analysis_components"]
    global_trends = state["global_trends"]
    # llm = state["llm"]
    state['llm'] = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    llm = state['llm']

    user_profile_str = json.dumps(user_profile, ensure_ascii=False, indent=2)
    market_analysis_str = json.dumps({
        "domestic_analysis": domestic_analysis,
        "global_trends": global_trends
    }, ensure_ascii=False, indent=2)

    # --- 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ---
    gap_analysis_prompt_template = """
    **í˜ë¥´ì†Œë‚˜ (Persona):**
    ë‹¹ì‹ ì€ ì§€ì›ìì˜ í”„ë¡œí•„ê³¼ ì‹œì¥ ë¶„ì„ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ê°•ì , ì•½ì , ê¸°íšŒë¥¼ ë‚ ì¹´ë¡­ê²Œ ì§„ë‹¨í•˜ëŠ” ì „ë¬¸ HR ì»¨ì„¤í„´íŠ¸ì´ì ì»¤ë¦¬ì–´ ì½”ì¹˜ì…ë‹ˆë‹¤.

    **ì„ë¬´ (Mission):**
    ì•„ë˜ì— ì£¼ì–´ì§„ [ì‚¬ìš©ì í”„ë¡œí•„]ê³¼ [ì‹œì¥ ë¶„ì„ ë°ì´í„°]ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬, {format_instructions}ì— ëª…ì‹œëœ JSON í˜•ì‹ìœ¼ë¡œ ì§„ë‹¨ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    **ê·œì¹™ (Strict Rules):**
    1.  **ë°ì´í„° ê¸°ë°˜ ë¶„ì„:** ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë‘ ë°ì´í„°ì— ê·¼ê±°í•˜ì—¬ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
    2.  **ê°•ì (Strengths) ì •ì˜:** [ì‚¬ìš©ì í”„ë¡œí•„]ì— ìˆê³ , [ì‹œì¥ ë¶„ì„ ë°ì´í„°]ì—ì„œë„ ì¤‘ìš”í•˜ê²Œ ìš”êµ¬í•˜ëŠ” ì—­ëŸ‰ ë˜ëŠ” ê²½í—˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    3.  **ì•½ì (Weaknesses) ì •ì˜:** [ì‹œì¥ ë¶„ì„ ë°ì´í„°]ì—ì„œëŠ” ì¤‘ìš”í•˜ê²Œ ìš”êµ¬í•˜ì§€ë§Œ, [ì‚¬ìš©ì í”„ë¡œí•„]ì—ëŠ” ëª…í™•í•˜ê²Œ ë³´ì´ì§€ ì•ŠëŠ” ì—­ëŸ‰ ë˜ëŠ” ê²½í—˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    4.  **ê¸°íšŒ(Opportunities) ì •ì˜:** ì‚¬ìš©ìì˜ 'ê°•ì 'ì„ 'ê¸€ë¡œë²Œ íŠ¸ë Œë“œ'ì™€ ê²°í•©í–ˆì„ ë•Œ, ë¯¸ë˜ì— ë” í° ê²½ìŸë ¥ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    5.  **ì–¸ì–´ í†µì¼:** ëª¨ë“  ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤.

    ---
    **[ì‚¬ìš©ì í”„ë¡œí•„]**
    {user_profile_str}
    ---
    **[ì‹œì¥ ë¶„ì„ ë°ì´í„°]**
    {market_analysis_str}
    ---

    ì´ì œ ìœ„ ë‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.
    """

    # --- 3. í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ ---
    prompt_variable_inputs = {
        "user_profile_str": user_profile_str,
        "market_analysis_str": market_analysis_str
    }

    gap_analysis_result = extract_structured_data_flexible(
        prompt_inputs=prompt_variable_inputs,
        extraction_prompt_template=gap_analysis_prompt_template,
        pydantic_model=GapAnalysisOutput,
        llm=llm,
        log_message="-> Helper: ì‚¬ìš©ì ê°•ì , ì•½ì , ê¸°íšŒ ë¶„ì„ ì‹¤í–‰..."
    )

    return {
        "gap_analysis": gap_analysis_result
    }

"""# ìˆ˜í–‰ ì˜ˆì‹œ"""

import pprint
from langgraph.graph import StateGraph, END
from googleapiclient.discovery import build
from langchain_openai import ChatOpenAI

if __name__ == '__main__':
    # --- 1. ì¤€ë¹„ ë‹¨ê³„ ---

    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("âœ… YouTube ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸ”´ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
        exit()

    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    workflow = StateGraph(AgentState)

    # [ìˆ˜ì •] 4ê°œì˜ ë…¸ë“œë¥¼ ëª¨ë‘ ì¶”ê°€í•©ë‹ˆë‹¤.
    workflow.add_node("user_profiling", user_profiling_node)
    workflow.add_node("domestic_analysis", domestic_job_analysis_node)
    workflow.add_node("global_trends", global_trend_analysis_node)
    workflow.add_node("gap_analysis", gap_analysis_node) # <-- ì‹ ê·œ ë…¸ë“œ ì¶”ê°€

    # [ìˆ˜ì •] user_profiling -> domestic_analysis -> global_trends -> gap_analysis ìˆœì„œë¡œ ì‹¤í–‰ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.
    workflow.set_entry_point("user_profiling")
    workflow.add_edge("user_profiling", "domestic_analysis")
    workflow.add_edge("domestic_analysis", "global_trends")
    workflow.add_edge("global_trends", "gap_analysis") # <-- ì‹ ê·œ ì—°ê²°ì„  ì¶”ê°€
    workflow.add_edge("gap_analysis", END)             # <-- ë§ˆì§€ë§‰ ë…¸ë“œë¥¼ gap_analysisë¡œ ë³€ê²½

    app = workflow.compile()

    # --- 3. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ì •ì˜ ---
    user_input = {
        "ëª©í‘œ ì§ë¬´": "AI ê´€ë ¨",
        "í¬ë§ ê¸°ì—…": ["ë„¤ì¹´ë¼ì¿ ë°°"],
        "í•™ë…„/í•™ê¸°": "3í•™ë…„ 2í•™ê¸°",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ê³µ : í†µê³„í•™ê³¼, ë³µìˆ˜ì „ê³µ : ì»´í“¨í„°í•™ë¶€",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "SQL, Python(scikit-learn, Pytorch) ê¸°ìˆ  ë³´ìœ  ë° SQLD, ì‚¬íšŒì¡°ì‚¬ë¶„ì„ì‚¬ ìê²©ì¦ ë³´ìœ ",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "XGBoost, SVMì„ ì´ìš©í•œ Human vs AI ì½”ë“œ ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ í”„ë¡œì íŠ¸, SAINT ëª¨ë¸ í™œìš©í•˜ì—¬ ì „ë ¥ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ê²¨ìš¸ë°©í•™ì— ì–´ë–¤ ê³µë¶€ë‚˜ ê¸°ìˆ  í”„ë¡œì íŠ¸ë¥¼ í•´ë³´ë©´ ì¢‹ì„ê¹Œìš”?"
    }

    # llm ê°ì²´ ìƒì„±
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm
    }

    # --- 4. ê·¸ë˜í”„ ì‹¤í–‰ ë° ì¤‘ê°„ ê³¼ì • í™•ì¸ ---
    print("\nğŸš€ ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ (ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ)")
    print("="*60)

    final_state = {}
    for state_update in app.stream(initial_state):
        node_name = list(state_update.keys())[0]
        node_output = state_update.get(node_name)

        print(f"\n--- ğŸ“Œ [ë…¸ë“œ: {node_name}] ì‹¤í–‰ ì™„ë£Œ ---")
        pprint.pprint(node_output)

        if node_output is not None:
            final_state.update(node_output)

    print("\n\nâœ… ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")

    # --- 5. ìµœì¢… ê²°ê³¼ë¬¼ ì¢…í•© ì¶œë ¥ ---
    # ê° ë…¸ë“œì—ì„œ ë°˜í™˜í•œ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ìƒíƒœì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    user_profile = final_state.get('user_profile', {})
    domestic_components = final_state.get('domestic_analysis_components', {})
    global_trends = final_state.get('global_trends', {})
    gap_analysis = final_state.get('gap_analysis', {}) # <-- ì‹ ê·œ ê²°ê³¼ ì¶”ì¶œ

    pprint.pprint(user_profile)

    # êµ­ë‚´ ì‹œì¥ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸ“‚ êµ­ë‚´ ì‹œì¥ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(domestic_components)

    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸŒ ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(global_trends)

    # [ì‹ ê·œ] ì‚¬ìš©ì ìµœì¢… ì§„ë‹¨ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸ“Š ì‚¬ìš©ì ìµœì¢… ì§„ë‹¨ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(gap_analysis)
    print("="*60)

import pprint
from langgraph.graph import StateGraph, END
from googleapiclient.discovery import build
from langchain_openai import ChatOpenAI

if __name__ == '__main__':
    # --- 1. ì¤€ë¹„ ë‹¨ê³„ ---

    try:
        api_keys = AgentAPIs()
        youtube_service = build('youtube', 'v3', developerKey=api_keys.youtube_api_key)
        print("âœ… YouTube ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸ”´ API í‚¤ ë˜ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
        exit()

    # --- 2. LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ë° êµ¬ì„± ---
    workflow = StateGraph(AgentState)

    # [ìˆ˜ì •] 4ê°œì˜ ë…¸ë“œë¥¼ ëª¨ë‘ ì¶”ê°€í•©ë‹ˆë‹¤.
    workflow.add_node("user_profiling", user_profiling_node)
    workflow.add_node("domestic_analysis", domestic_job_analysis_node)
    workflow.add_node("global_trends", global_trend_analysis_node)
    workflow.add_node("gap_analysis", gap_analysis_node) # <-- ì‹ ê·œ ë…¸ë“œ ì¶”ê°€

    # [ìˆ˜ì •] user_profiling -> domestic_analysis -> global_trends -> gap_analysis ìˆœì„œë¡œ ì‹¤í–‰ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.
    workflow.set_entry_point("user_profiling")
    workflow.add_edge("user_profiling", "domestic_analysis")
    workflow.add_edge("domestic_analysis", "global_trends")
    workflow.add_edge("global_trends", "gap_analysis") # <-- ì‹ ê·œ ì—°ê²°ì„  ì¶”ê°€
    workflow.add_edge("gap_analysis", END)             # <-- ë§ˆì§€ë§‰ ë…¸ë“œë¥¼ gap_analysisë¡œ ë³€ê²½

    app = workflow.compile()

    # --- 3. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ì •ì˜ ---
    user_input = {
        "ëª©í‘œ ì§ë¬´": "ì€í–‰ ë˜ëŠ” ì¦ê¶Œ",
        "í¬ë§ ê¸°ì—…": ["ê¸ˆìœµê³µê¸°ì—…"],
        "í•™ë…„/í•™ê¸°": "2í•™ë…„ 2í•™ê¸°",
        "ì „ê³µ ë° ë³µìˆ˜(ë¶€)ì „ê³µ": "ì „ê³µ : ê²½ì˜í•™ë¶€",
        "ë³´ìœ  ê¸°ìˆ  ë° ìê²©ì¦": "ì™¸ë¶€ ê¸°ì—…ë¶„ì„ ê³µëª¨ì „ ê¸ˆìƒ ë° ëª¨ì˜ì£¼ì‹íˆ¬ì ëŒ€íšŒ ìµœìš°ìˆ˜ìƒ, ì‹ ìš©ë¶„ì„ì‚¬ ë° íˆ¬ììš´ìš©ì‚¬ ìê²©ì¦ ë³´ìœ ",
        "ê´€ë ¨ ê²½í—˜ ë° ìŠ¤í™" : "ì€í–‰ ì„œí¬í„°ì¦ˆ í™œë™ 6ê°œì›”",
        "ê³ ë¯¼ ë˜ëŠ” ê¶ê¸ˆí•œ ì ": "ì–´ë–¤ ìŠ¤í™ì„ ë” ìŒ“ëŠ”ê²Œ ì¢‹ì„ê¹Œ?"
    }

    # llm ê°ì²´ ìƒì„±
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    initial_state = {
        "user_profile_raw": user_input,
        "api_keys": api_keys,
        "youtube_service": youtube_service,
        "llm": llm
    }

    # --- 4. ê·¸ë˜í”„ ì‹¤í–‰ ë° ì¤‘ê°„ ê³¼ì • í™•ì¸ ---
    print("\nğŸš€ ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ (ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ)")
    print("="*60)

    final_state = {}
    for state_update in app.stream(initial_state):
        node_name = list(state_update.keys())[0]
        node_output = state_update.get(node_name)

        print(f"\n--- ğŸ“Œ [ë…¸ë“œ: {node_name}] ì‹¤í–‰ ì™„ë£Œ ---")
        pprint.pprint(node_output)

        if node_output is not None:
            final_state.update(node_output)

    print("\n\nâœ… ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")

    # --- 5. ìµœì¢… ê²°ê³¼ë¬¼ ì¢…í•© ì¶œë ¥ ---
    # ê° ë…¸ë“œì—ì„œ ë°˜í™˜í•œ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ìƒíƒœì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    user_profile = final_state.get('user_profile', {})
    domestic_components = final_state.get('domestic_analysis_components', {})
    global_trends = final_state.get('global_trends', {})
    gap_analysis = final_state.get('gap_analysis', {}) # <-- ì‹ ê·œ ê²°ê³¼ ì¶”ì¶œ

    pprint.pprint(user_profile)

    # êµ­ë‚´ ì‹œì¥ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸ“‚ êµ­ë‚´ ì‹œì¥ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(domestic_components)

    # ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸŒ ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(global_trends)

    # [ì‹ ê·œ] ì‚¬ìš©ì ìµœì¢… ì§„ë‹¨ ê²°ê³¼ ì¶œë ¥
    print("\n\n" + "="*60)
    print("           <<< ğŸ“Š ì‚¬ìš©ì ìµœì¢… ì§„ë‹¨ ê²°ê³¼ >>>")
    print("="*60)
    pprint.pprint(gap_analysis)
    print("="*60)
