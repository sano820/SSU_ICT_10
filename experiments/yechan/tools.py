# -*- coding: utf-8 -*-
"""tools_BACKUP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ys6LqMXIxvtukwWK4Mk729gFwUGNs8VC
"""

# !pip install langchain-community
import os
import requests
from datetime import datetime, timedelta
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from typing import List, Dict, Any
import glob

import os # os import ì¶”ê°€
from langchain_core.tools import tool
from googleapiclient.discovery import build

from langchain.tools import tool
from langchain_community.tools.youtube.search import YouTubeSearchTool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader
from langchain_community.document_loaders import ArxivLoader
from langchain_community.retrievers import TavilySearchAPIRetriever

"""ìœ íŠœë¸Œ ë°ì´í„° í¬ë¡¤ë§"""
def analyze_youtube_topic(
    topic: str,
    analysis_prompt: str,
    api_key: str,
    lang_code: str = "ko",
    max_results: int = 5,
    num_to_analyze: int = 2,
    transcripts_only: bool = True  # <-- [ìˆ˜ì •] ìë§‰ í•„í„°ë§ ì—¬ë¶€ë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ì¶”ê°€
) -> str:
    """
    ì£¼ì–´ì§„ ì£¼ì œ(topic)ë¡œ ìœ íŠœë¸Œ ì˜ìƒì„ ê²€ìƒ‰í•˜ê³ , ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸(analysis_prompt)ë¡œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    print(f"\n-> ìœ íŠœë¸Œ '{topic}' ì£¼ì œ ë¶„ì„ ì‹œì‘...")
    try:
        # [ìˆ˜ì •] ğŸ’¡ ì¸ìë¡œ ë°›ì€ transcripts_only ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        videos = find_videos_with_transcripts.invoke({
            "topic": topic,
            "api_key": api_key,
            "lang_code": lang_code,
            "transcripts_only": transcripts_only, # <-- ìˆ˜ì •ëœ ë¶€ë¶„
            "max_results": max_results
        })

        # --- ì´í•˜ í•¨ìˆ˜ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤ ---
        if not videos:
            print(f"-> ë¶„ì„í•  '{topic}' ê´€ë ¨ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return f"ê´€ë ¨ ìœ íŠœë¸Œ ì˜ìƒ ì—†ìŒ."
            
        videos_to_analyze = videos[:num_to_analyze]
        print(f"-> ì´ {len(videos)}ê°œì˜ ì˜ìƒì„ ì°¾ì•˜ìœ¼ë©°, ìƒìœ„ {len(videos_to_analyze)}ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤:")
        for video in videos_to_analyze:
            print(f"  - {video.get('title', 'ì œëª© ì—†ìŒ')}")

        analysis_tasks = [
            {
                "video_url": video['url'],
                "question": analysis_prompt
            }
            for video in videos_to_analyze
        ]
        
        analysis_results = analyze_video_content.batch(analysis_tasks)
        
        return "\n\n---\n\n".join(analysis_results)

    except Exception as e:
        print(f"-> âš ï¸ ìœ íŠœë¸Œ '{topic}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ìœ íŠœë¸Œ '{topic}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ."

@tool
def find_videos_with_transcripts(
    topic: str,
    api_key: str,
    max_results: int = 10,
    lang_code: str = 'en',
    transcripts_only: bool = True
) -> List[Dict[str, Any]]:
    """
    ì£¼ì–´ì§„ ì£¼ì œë¡œ ìœ íŠœë¸Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì˜ìƒì˜ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"-> '{topic}' ì£¼ì œë¡œ ì˜ìƒ ê²€ìƒ‰ ì‹œì‘ (ìë§‰ í•„í„°ë§: {transcripts_only}, ì–¸ì–´: {lang_code})")
    
    try:
        if not api_key:
            raise ValueError("API í‚¤ê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        youtube_service = build('youtube', 'v3', developerKey=api_key)
    except Exception as e:
        print(f"-> âš ï¸ YouTube API ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”. (ì˜¤ë¥˜: {e})")
        return []

    try:
        search_response = youtube_service.search().list(
            q=topic,
            part='snippet',
            type='video',
            order='relevance',
            maxResults=max_results
        ).execute()
        
        video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
        if not video_ids:
            return []

        video_details_response = youtube_service.videos().list(
            part='snippet,statistics',
            id=','.join(video_ids)
        ).execute()
        
        all_video_details = []
        for item in video_details_response.get('items', []):
            all_video_details.append({
                "title": item['snippet']['title'],
                "url": f"https://www.youtube.com/watch?v={item['id']}",
                "view_count": int(item['statistics'].get('viewCount', 0)),
                "video_id": item['id']
            })
            
        if not transcripts_only:
            return all_video_details

        final_videos = []
        for video in all_video_details:
            try:
                transcript_list = YouTubeTranscriptApi.list_transcripts(video['video_id'])
                transcript_list.find_transcript([lang_code])
                final_videos.append(video)
            except Exception:
                continue
                
        return final_videos

    except Exception as e:
        print(f"-> ìœ íŠœë¸Œ ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


@tool
def find_youtube_videos(topic: str, language: str, time_filter: str = "3 years") -> str:
    """
    íŠ¹ì • ì£¼ì œ(topic)ì˜ YouTube ì˜ìƒì„ ì§€ì •ëœ ì–¸ì–´(language)ì™€ ê¸°ê°„(time_filter) ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì˜ˆ: topic="ë°ì´í„° ë¶„ì„", language="korean", time_filter="1 year"
    """
    # ê²€ìƒ‰ì–´ë¥¼ ì¡°í•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ ë„í•©ë‹ˆë‹¤.
    query = f"{topic} {language} within {time_filter}"
    print(f"Executing search with query: '{query}'") # ì–´ë–¤ ê²€ìƒ‰ì–´ë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸

    # max_resultsë¥¼ 2ë¡œ ê³ ì •í•˜ì—¬ 2ê°œì”© ì°¾ë„ë¡ í•©ë‹ˆë‹¤.
    tool_instance = YouTubeSearchTool(max_results=2)
    return tool_instance.run(query)

# ë„êµ¬ 2: [FINAL & CORRECTED] GenericLoaderì™€ Parserë¥¼ ì‚¬ìš©í•œ ì˜ìƒ ë‚´ìš© ë¶„ì„ ë„êµ¬
@tool
def analyze_video_content(video_url: str, question: str) -> str:
    """
    íŠ¹ì • YouTube ì˜ìƒì˜ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    ì˜ìƒ URLê³¼ ë¶„ì„í•  ì§ˆë¬¸, ë‘ ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    save_dir = "./temp_audio"
    try:
        # 1. [ìˆ˜ì •] GenericLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë”ì™€ íŒŒì„œë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
        # 1-1. ì¬ë£Œ ì¤€ë¹„ ë‹´ë‹¹: ìœ íŠœë¸Œ ì˜¤ë””ì˜¤ë¥¼ ê°€ì ¸ì˜¬ ë¡œë” ì„¤ì •
        blob_loader = YoutubeAudioLoader([video_url], save_dir)

        # 1-2. ìš”ë¦¬ì‚¬: ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  íŒŒì„œ ì„¤ì •
        parser = OpenAIWhisperParser()

        # 1-3. ì£¼ë°© ì‹œìŠ¤í…œ: ë¡œë”ì™€ íŒŒì„œë¥¼ GenericLoaderë¡œ ì—°ê²°
        loader = GenericLoader(blob_loader, parser)

        # 2. [ìˆ˜ì •] GenericLoaderë¥¼ ì‹¤í–‰í•˜ì—¬ ë¬¸ì„œ(í…ìŠ¤íŠ¸)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        docs = loader.load()

        if not docs:
            return "ì˜ìƒ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜¤ë””ì˜¤ ì¶”ì¶œ ë˜ëŠ” ë³€í™˜ ì‹¤íŒ¨)"

        transcript_text = docs[0].page_content

        # 3. ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        prompt = f"""
        ì•„ë˜ëŠ” YouTube ì˜ìƒì˜ ì „ì²´ ìŒì„± ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ê²ƒì…ë‹ˆë‹¤.
        ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        --- ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë‚´ìš© ---
        {transcript_text[:4000]}

        --- ì§ˆë¬¸ ---
        {question}
        """
        response = llm.invoke(prompt)
        return response.content

    except Exception as e:
        return f"ì˜ìƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    finally:
        # 4. ì‘ì—…ì´ ëë‚˜ë©´ ë‹¤ìš´ë¡œë“œí–ˆë˜ ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
        if os.path.exists(save_dir):
            files = glob.glob(os.path.join(save_dir, '*'))
            for f in files:
                os.remove(f)

"""ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„° í¬ë¡¤ë§"""

@tool
def search_naver_news(company_name: str, display_count: int = 5) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ íŠ¹ì • íšŒì‚¬(company_name)ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ê²€ìƒ‰í•  ê²°ê³¼ ê°œìˆ˜(display_count)ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    # 2. API ìš”ì²­ ì •ë³´ ì„¤ì •
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": os.environ['NAVER_CLIENT_ID'],
        "X-Naver-Client-Secret": os.environ['NAVER_CLIENT_SECRET']
    }
    params = {
        "query": company_name,
        "display": display_count,
        "sort": "date"  # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
    }

    # 3. API í˜¸ì¶œ
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()  # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´

        # 4. ê²°ê³¼ íŒŒì‹± ë° ì •ë¦¬
        data = response.json()
        items = data.get("items", [])

        if not items:
            return f"'{company_name}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê²°ê³¼ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ê°€ê³µ
        formatted_results = []
        for item in items:
            title = item.get("title", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"')
            description = item.get("description", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"')
            link = item.get("link", "")
            pub_date = item.get("pubDate", "")

            formatted_results.append(
                f"- ì œëª©: {title}\n"
                f"  - ë‚ ì§œ: {pub_date}\n"
                f"  - ìš”ì•½: {description}\n"
                f"  - ë§í¬: {link}"
            )

        return "\n\n".join(formatted_results)

    except requests.exceptions.RequestException as e:
        return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

"""í•´ì™¸ ë‰´ìŠ¤ ë°ì´í„° í¬ë¡¤ë§"""

@tool
def search_global_news(query: str, days_ago: int = 30) -> str:
    """
    í•´ì™¸ ì£¼ìš” ì–¸ë¡ ì‚¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ íŠ¹ì • ì£¼ì œ(query)ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ìµœê·¼ ë©°ì¹ (days_ago) ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ì§€ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ê¸°ë³¸ê°’: 30ì¼)
    ì™¸êµ­ê³„ ê¸°ì—…ì„ ì¡°ì‚¬í•˜ê±°ë‚˜, êµ­ë‚´ ê¸°ì—…ì˜ í•´ì™¸ ë°˜ì‘ì„ ë³¼ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.
    """

    # 2. API ìš”ì²­ ì •ë³´ ì„¤ì •
    url = "https://newsapi.org/v2/everything"

    # ê²€ìƒ‰ ì‹œì‘ ë‚ ì§œ ê³„ì‚°
    from_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')

    params = {
        "q": query,
        "apiKey": os.environ['NEWS_API_KEY'],
        "from": from_date,
        "sortBy": "relevancy",  # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        "language": "en",       # ì˜ì–´ ë‰´ìŠ¤ ìš°ì„  ê²€ìƒ‰
        "pageSize": 5           # ìµœëŒ€ 5ê°œì˜ ê²°ê³¼ë§Œ ê°€ì ¸ì˜´
    }

    # 3. API í˜¸ì¶œ
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()

        # 4. ê²°ê³¼ íŒŒì‹± ë° ì •ë¦¬
        data = response.json()
        articles = data.get("articles", [])

        if not articles:
            return f"'{query}'ì— ëŒ€í•œ í•´ì™¸ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        formatted_results = []
        for article in articles:
            title = article.get("title", "")
            source = article.get("source", {}).get("name", "")
            description = article.get("description", "")
            url = article.get("url", "")
            pub_date = article.get("publishedAt", "").split("T")[0] # ë‚ ì§œë§Œ í‘œì‹œ

            formatted_results.append(
                f"- ì œëª©: {title}\n"
                f"  - ì¶œì²˜: {source} ({pub_date})\n"
                f"  - ìš”ì•½: {description}\n"
                f"  - ë§í¬: {url}"
            )

        return "\n\n".join(formatted_results)

    except requests.exceptions.RequestException as e:
        return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

"""Arxiv ë…¼ë¬¸ í¬ë¡¤ë§"""

@tool
def search_arxiv_papers(query: str, load_max_docs: int = 2) -> str:
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬(query)ë¡œ arXivì—ì„œ ìµœì‹  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , ê° ë…¼ë¬¸ ì´ˆë¡ì˜ ê¸¸ì´ë¥¼
    ì•ˆì „í•˜ê²Œ ì˜ë¼ì„œ(2500ì) ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì •ë¦¬í•œ í›„ ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"--> (ArXiv ë„êµ¬) ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰: {query}")
    try:
        loader = ArxivLoader(
            query=query,
            load_max_docs=load_max_docs,
            sort_by="submittedDate"
        )
        docs = loader.load()

        if not docs:
            return "ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        summaries = []
        for doc in docs:
            # [â­í•µì‹¬â­] ê° ë…¼ë¬¸ ì´ˆë¡ì˜ ë‚´ìš©ì„ ìµœëŒ€ 2500ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
            truncated_abstract = doc.page_content[:2500]
            summary = f"ë…¼ë¬¸ ì œëª©: {doc.metadata['Title']}\nì´ˆë¡: {truncated_abstract}"
            summaries.append(summary)

        final_summary = "\n\n---\n\n".join(summaries)
        print(f"-> ArXiv ê²€ìƒ‰ ì™„ë£Œ. (ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_summary)})")
        return final_summary

    except Exception as e:
        print(f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

"""ì›¹ ì„œì¹­ í¬ë¡¤ë§"""

@tool
def tavily_web_search(query: str) -> str:
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬(query)ë¡œ ì›¹ì„ ê²€ìƒ‰í•˜ê³ , ê° ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš©ì„
    ì¼ì •í•œ ê¸¸ì´(3000ì)ë¡œ ì˜ë¼ì„œ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì •ë¦¬í•œ í›„ ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"--> (ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ë„êµ¬) ì‹¤í–‰: {query[:50]}...")

    # 1. k=3: ìµœëŒ€ 3ê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì›¹í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    retriever = TavilySearchAPIRetriever(k=3)

    # 2. retrieverë¥¼ ì‹¤í–‰í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìŒ
    try:
        docs = retriever.invoke(query)
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # 3. ê° Documentì˜ ë‚´ìš©ì„ ì˜ë¼ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
    summaries = []
    for doc in docs:
        # [â­í•µì‹¬â­] ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ ìµœëŒ€ 3000ìë¡œ ì œí•œí•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ LLMì˜ ì»¨í…ìŠ¤íŠ¸ ì°½ì„ ë„˜ì„ ê°€ëŠ¥ì„±ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.
        truncated_content = doc.page_content[:3000]
        summary = f"--- ê²€ìƒ‰ ê²°ê³¼ ì¶œì²˜: {doc.metadata.get('source', 'N/A')} ---\n{truncated_content}"
        summaries.append(summary)

    final_summary = "\n\n".join(summaries)
    print(f"-> ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì™„ë£Œ. (ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_summary)})")

    return final_summary
